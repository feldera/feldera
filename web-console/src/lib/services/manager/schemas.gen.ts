// This file is auto-generated by @hey-api/openapi-ts

export const $AdHocInputConfig = {
  type: 'object',
  description: `Configuration for inserting data with ad-hoc queries

An ad-hoc input adapters cannot be usefully configured as part of pipeline
configuration.  Instead, use ad-hoc queries through the UI, the REST API, or
the \`fda\` command-line tool.`,
  required: ['name'],
  properties: {
    name: {
      type: 'string',
      description: 'Autogenerated name.'
    }
  }
} as const

export const $AdHocResultFormat = {
  type: 'string',
  description: 'URL-encoded `format` argument to the `/query` endpoint.',
  enum: ['text', 'json', 'parquet', 'arrow_ipc', 'hash']
} as const

export const $AdhocQueryArgs = {
  type: 'object',
  description: `Arguments to the \`/query\` endpoint.

The arguments can be provided in two ways:

- In case a normal HTTP connection is established to the endpoint,
these arguments are passed as URL-encoded parameters.
Note: this mode is deprecated and will be removed in the future.

- If a Websocket connection is opened to \`/query\`, the arguments are passed
to the server over the websocket as a JSON encoded string.`,
  properties: {
    format: {
      $ref: '#/components/schemas/AdHocResultFormat'
    },
    sql: {
      type: 'string',
      description: 'The SQL query to run.'
    }
  }
} as const

export const $ApiKeyDescr = {
  type: 'object',
  description: 'API key descriptor.',
  required: ['id', 'name', 'scopes'],
  properties: {
    id: {
      $ref: '#/components/schemas/ApiKeyId'
    },
    name: {
      type: 'string'
    },
    scopes: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/ApiPermission'
      }
    }
  }
} as const

export const $ApiKeyId = {
  type: 'string',
  format: 'uuid',
  description: 'API key identifier.'
} as const

export const $ApiPermission = {
  type: 'string',
  description: 'Permission types for invoking API endpoints.',
  enum: ['Read', 'Write']
} as const

export const $AuthProvider = {
  oneOf: [
    {
      type: 'object',
      required: ['AwsCognito'],
      properties: {
        AwsCognito: {
          $ref: '#/components/schemas/ProviderAwsCognito'
        }
      }
    },
    {
      type: 'object',
      required: ['GoogleIdentity'],
      properties: {
        GoogleIdentity: {
          $ref: '#/components/schemas/ProviderGoogleIdentity'
        }
      }
    }
  ]
} as const

export const $BuildInformation = {
  type: 'object',
  description: 'Information about the build of the platform.',
  required: [
    'build_timestamp',
    'build_cpu',
    'build_os',
    'cargo_dependencies',
    'cargo_features',
    'cargo_debug',
    'cargo_opt_level',
    'cargo_target_triple',
    'rustc_version'
  ],
  properties: {
    build_cpu: {
      type: 'string',
      description: 'CPU of build machine.'
    },
    build_os: {
      type: 'string',
      description: 'OS of build machine.'
    },
    build_timestamp: {
      type: 'string',
      description: 'Timestamp of the build.'
    },
    cargo_debug: {
      type: 'string',
      description: 'Whether the build is optimized for performance.'
    },
    cargo_dependencies: {
      type: 'string',
      description: 'Dependencies used during the build.'
    },
    cargo_features: {
      type: 'string',
      description: 'Features enabled during the build.'
    },
    cargo_opt_level: {
      type: 'string',
      description: 'Optimization level of the build.'
    },
    cargo_target_triple: {
      type: 'string',
      description: 'Target triple of the build.'
    },
    rustc_version: {
      type: 'string',
      description: 'Rust version of the build used.'
    }
  }
} as const

export const $CheckpointFailure = {
  type: 'object',
  description: 'Information about a failed checkpoint.',
  required: ['sequence_number', 'error'],
  properties: {
    error: {
      type: 'string',
      description: 'Error message associated with the failure.'
    },
    sequence_number: {
      type: 'integer',
      format: 'int64',
      description: 'Sequence number of the failed checkpoint.',
      minimum: 0
    }
  }
} as const

export const $CheckpointResponse = {
  type: 'object',
  description: 'Response to a checkpoint request.',
  required: ['checkpoint_sequence_number'],
  properties: {
    checkpoint_sequence_number: {
      type: 'integer',
      format: 'int64',
      minimum: 0
    }
  }
} as const

export const $CheckpointStatus = {
  type: 'object',
  description: 'Checkpoint status returned by the `/checkpoint_status` endpoint.',
  properties: {
    failure: {
      allOf: [
        {
          $ref: '#/components/schemas/CheckpointFailure'
        }
      ],
      nullable: true
    },
    success: {
      type: 'integer',
      format: 'int64',
      description: 'Most recently successful checkpoint.',
      nullable: true,
      minimum: 0
    }
  }
} as const

export const $Chunk = {
  type: 'object',
  description: `A set of updates to a SQL table or view.

The \`sequence_number\` field stores the offset of the chunk relative to the
start of the stream and can be used to implement reliable delivery.
The payload is stored in the \`bin_data\`, \`text_data\`, or \`json_data\` field
depending on the data format used.`,
  required: ['sequence_number'],
  properties: {
    bin_data: {
      type: 'string',
      format: 'binary',
      description: 'Base64 encoded binary payload, e.g., bincode.',
      nullable: true
    },
    json_data: {
      type: 'object',
      description: 'JSON payload.',
      nullable: true
    },
    sequence_number: {
      type: 'integer',
      format: 'int64',
      minimum: 0
    },
    text_data: {
      type: 'string',
      description: 'Text payload, e.g., CSV.',
      nullable: true
    }
  }
} as const

export const $ClockConfig = {
  type: 'object',
  required: ['clock_resolution_usecs'],
  properties: {
    clock_resolution_usecs: {
      type: 'integer',
      format: 'int64',
      minimum: 0
    }
  }
} as const

export const $ColumnType = {
  type: 'object',
  description: `A SQL column type description.

Matches the Calcite JSON format.`,
  required: ['nullable'],
  properties: {
    component: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    },
    fields: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Field'
      },
      description: `The fields of the type (if available).

For example this would specify the fields of a \`CREATE TYPE\` construct.

\`\`\`sql
CREATE TYPE person_typ AS (
firstname       VARCHAR(30),
lastname        VARCHAR(30),
address         ADDRESS_TYP
);
\`\`\`

Would lead to the following \`fields\` value:

\`\`\`sql
[
ColumnType { name: "firstname, ... },
ColumnType { name: "lastname", ... },
ColumnType { name: "address", fields: [ ... ] }
]
\`\`\``,
      nullable: true
    },
    key: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    },
    nullable: {
      type: 'boolean',
      description: 'Does the type accept NULL values?'
    },
    precision: {
      type: 'integer',
      format: 'int64',
      description: `Precision of the type.

# Examples
- \`VARCHAR\` sets precision to \`-1\`.
- \`VARCHAR(255)\` sets precision to \`255\`.
- \`BIGINT\`, \`DATE\`, \`FLOAT\`, \`DOUBLE\`, \`GEOMETRY\`, etc. sets precision
to None
- \`TIME\`, \`TIMESTAMP\` set precision to \`0\`.`,
      nullable: true
    },
    scale: {
      type: 'integer',
      format: 'int64',
      description: `The scale of the type.

# Example
- \`DECIMAL(1,2)\` sets scale to \`2\`.`,
      nullable: true
    },
    type: {
      $ref: '#/components/schemas/SqlType'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    }
  }
} as const

export const $CombinedDesiredStatus = {
  type: 'string',
  enum: ['Stopped', 'Unavailable', 'Standby', 'Paused', 'Running', 'Suspended']
} as const

export const $CombinedStatus = {
  type: 'string',
  enum: [
    'Stopped',
    'Provisioning',
    'Unavailable',
    'Standby',
    'Initializing',
    'Bootstrapping',
    'Replaying',
    'Paused',
    'Running',
    'Suspended',
    'Stopping'
  ]
} as const

export const $CompilationProfile = {
  type: 'string',
  description: `Enumeration of possible compilation profiles that can be passed to the Rust compiler
as an argument via \`cargo build --profile <>\`. A compilation profile affects among
other things the compilation speed (how long till the program is ready to be run)
and runtime speed (the performance while running).`,
  enum: ['dev', 'unoptimized', 'optimized']
} as const

export const $CompletionStatus = {
  type: 'string',
  description: 'Completion token status returned by the `/completion_status` endpoint.',
  enum: ['complete', 'inprogress']
} as const

export const $CompletionStatusArgs = {
  type: 'object',
  description: 'URL-encoded arguments to the `/completion_status` endpoint.',
  required: ['token'],
  properties: {
    token: {
      type: 'string',
      description: `Completion token returned by the \`/completion_token\` or \`/ingress\`
endpoint.`
    }
  }
} as const

export const $CompletionStatusResponse = {
  type: 'object',
  description: 'Response to a completion token status request.',
  required: ['status'],
  properties: {
    status: {
      $ref: '#/components/schemas/CompletionStatus'
    }
  }
} as const

export const $CompletionTokenResponse = {
  type: 'object',
  description: 'Response to a completion token creation request.',
  required: ['token'],
  properties: {
    token: {
      type: 'string',
      description: `Completion token.

An opaque string associated with the current position in the input stream
generated by an input connector.
Pass this string to the \`/completion_status\` endpoint to check whether all
inputs associated with the token have been fully processed by the pipeline.`
    }
  }
} as const

export const $Configuration = {
  type: 'object',
  required: [
    'telemetry',
    'edition',
    'version',
    'revision',
    'runtime_revision',
    'changelog_url',
    'build_info'
  ],
  properties: {
    build_info: {
      $ref: '#/components/schemas/BuildInformation'
    },
    changelog_url: {
      type: 'string',
      description: 'URL that navigates to the changelog of the current version'
    },
    edition: {
      type: 'string',
      description: 'Feldera edition: "Open source" or "Enterprise"'
    },
    license_validity: {
      allOf: [
        {
          $ref: '#/components/schemas/LicenseValidity'
        }
      ],
      nullable: true
    },
    revision: {
      type: 'string',
      description:
        'Specific revision corresponding to the edition `version` (e.g., git commit hash).'
    },
    runtime_revision: {
      type: 'string',
      description:
        'Specific revision corresponding to the default runtime version of the platform (e.g., git commit hash).'
    },
    telemetry: {
      type: 'string',
      description: 'Telemetry key.'
    },
    unstable_features: {
      type: 'string',
      description: 'List of unstable features that are enabled.',
      nullable: true
    },
    update_info: {
      allOf: [
        {
          $ref: '#/components/schemas/UpdateInformation'
        }
      ],
      nullable: true
    },
    version: {
      type: 'string',
      description: `The version corresponding to the type of \`edition\`.
Format is \`x.y.z\`.`
    }
  }
} as const

export const $ConnectorConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/OutputBufferConfig'
    },
    {
      type: 'object',
      required: ['transport'],
      properties: {
        format: {
          allOf: [
            {
              $ref: '#/components/schemas/FormatConfig'
            }
          ],
          nullable: true
        },
        index: {
          type: 'string',
          description: `Name of the index that the connector is attached to.

This property is valid for output connectors only.  It is used with data
transports and formats that expect output updates in the form of key/value
pairs, where the key typically represents a unique id associated with the
table or view.

To support such output formats, an output connector can be attached to an
index created using the SQL CREATE INDEX statement.  An index of a table
or view contains the same updates as the table or view itself, indexed by
one or more key columns.

See individual connector documentation for details on how they work
with indexes.`,
          nullable: true
        },
        labels: {
          type: 'array',
          items: {
            type: 'string'
          },
          description: `Arbitrary user-defined text labels associated with the connector.

These labels can be used in conjunction with the \`start_after\` property
to control the start order of connectors.`
        },
        max_batch_size: {
          type: 'integer',
          format: 'int64',
          description: `Maximum batch size, in records.

This is the maximum number of records to process in one batch through
the circuit.  The time and space cost of processing a batch is
asymptotically superlinear in the size of the batch, but very small
batches are less efficient due to constant factors.

This should usually be less than \`max_queued_records\`, to give the
connector a round-trip time to restart and refill the buffer while
batches are being processed.

Some input adapters might not honor this setting.

The default is 10,000.`,
          minimum: 0
        },
        max_queued_records: {
          type: 'integer',
          format: 'int64',
          description: `Backpressure threshold.

Maximal number of records queued by the endpoint before the endpoint
is paused by the backpressure mechanism.

For input endpoints, this setting bounds the number of records that have
been received from the input transport but haven't yet been consumed by
the circuit since the circuit, since the circuit is still busy processing
previous inputs.

For output endpoints, this setting bounds the number of records that have
been produced by the circuit but not yet sent via the output transport endpoint
nor stored in the output buffer (see \`enable_output_buffer\`).

Note that this is not a hard bound: there can be a small delay between
the backpressure mechanism is triggered and the endpoint is paused, during
which more data may be queued.

The default is 1 million.`,
          minimum: 0
        },
        paused: {
          type: 'boolean',
          description: `Create connector in paused state.

The default is \`false\`.`
        },
        start_after: {
          type: 'array',
          items: {
            type: 'string'
          },
          description: `Start the connector after all connectors with specified labels.

This property is used to control the start order of connectors.
The connector will not start until all connectors with the specified
labels have finished processing all inputs.`,
          nullable: true
        },
        transport: {
          $ref: '#/components/schemas/TransportConfig'
        }
      }
    }
  ],
  description: "A data connector's configuration"
} as const

export const $DatagenInputConfig = {
  type: 'object',
  description: 'Configuration for generating random data for a table.',
  properties: {
    plan: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/GenerationPlan'
      },
      description: `The sequence of generations to perform.

If not set, the generator will produce a single sequence with default settings.
If set, the generator will produce the specified sequences in sequential order.

Note that if one of the sequences before the last one generates an unlimited number of rows
the following sequences will not be executed.`,
      default: [
        {
          rate: null,
          limit: null,
          worker_chunk_size: null,
          fields: {}
        }
      ]
    },
    seed: {
      type: 'integer',
      format: 'int64',
      description: `Optional seed for the random generator.

Setting this to a fixed value will make the generator produce the same sequence of records
every time the pipeline is run.

# Notes
- To ensure the set of generated input records is deterministic across multiple runs,
apart from setting a seed, \`workers\` also needs to remain unchanged.
- The input will arrive in non-deterministic order if \`workers > 1\`.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    workers: {
      type: 'integer',
      description: 'Number of workers to use for generating data.',
      default: 1,
      minimum: 0
    }
  },
  additionalProperties: false
} as const

export const $DatagenStrategy = {
  type: 'string',
  description: 'Strategy used to generate values.',
  enum: [
    'increment',
    'uniform',
    'zipf',
    'word',
    'words',
    'sentence',
    'sentences',
    'paragraph',
    'paragraphs',
    'first_name',
    'last_name',
    'title',
    'suffix',
    'name',
    'name_with_title',
    'domain_suffix',
    'email',
    'username',
    'password',
    'field',
    'position',
    'seniority',
    'job_title',
    'ipv4',
    'ipv6',
    'ip',
    'mac_address',
    'user_agent',
    'rfc_status_code',
    'valid_status_code',
    'company_suffix',
    'company_name',
    'buzzword',
    'buzzword_middle',
    'buzzword_tail',
    'catch_phrase',
    'bs_verb',
    'bs_adj',
    'bs_noun',
    'bs',
    'profession',
    'industry',
    'currency_code',
    'currency_name',
    'currency_symbol',
    'credit_card_number',
    'city_prefix',
    'city_suffix',
    'city_name',
    'country_name',
    'country_code',
    'street_suffix',
    'street_name',
    'time_zone',
    'state_name',
    'state_abbr',
    'secondary_address_type',
    'secondary_address',
    'zip_code',
    'post_code',
    'building_number',
    'latitude',
    'longitude',
    'isbn',
    'isbn13',
    'isbn10',
    'phone_number',
    'cell_number',
    'file_path',
    'file_name',
    'file_extension',
    'dir_path'
  ]
} as const

export const $DeltaTableIngestMode = {
  type: 'string',
  description: `Delta table read mode.

Three options are available:

* \`snapshot\` - read a snapshot of the table and stop.

* \`follow\` - continuously ingest changes to the table, starting from a specified version
or timestamp.

* \`snapshot_and_follow\` - read a snapshot of the table before switching to continuous ingestion
mode.`,
  enum: ['snapshot', 'follow', 'snapshot_and_follow', 'cdc']
} as const

export const $DeltaTableReaderConfig = {
  type: 'object',
  description: 'Delta table input connector configuration.',
  required: ['uri', 'mode'],
  properties: {
    cdc_delete_filter: {
      type: 'string',
      description: `A predicate that determines whether the record represents a deletion.

This setting is only valid in the \`cdc\` mode. It specifies a predicate applied to
each row in the Delta table to determine whether the row represents a deletion event.
Its value must be a valid Boolean SQL expression that can be used in a query of the
form \`SELECT * from <table> WHERE <cdc_delete_filter>\`.`,
      nullable: true
    },
    cdc_order_by: {
      type: 'string',
      description: `An expression that determines the ordering of updates in the Delta table.

This setting is only valid in the \`cdc\` mode. It specifies a predicate applied to
each row in the Delta table to determine the order in which updates in the table should
be applied. Its value must be a valid SQL expression that can be used in a query of the
form \`SELECT * from <table> ORDER BY <cdc_order_by>\`.`,
      nullable: true
    },
    datetime: {
      type: 'string',
      description: `Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
"2024-12-09T16:09:53+00:00".

When this option is set, the connector finds and opens the version of the table as of the
specified point in time (based on the server time recorded in the transaction log, not the
event time encoded in the data).  In \`snapshot\` and \`snapshot_and_follow\` modes, it
retrieves the snapshot of this version of the table.  In \`follow\`, \`snapshot_and_follow\`, and
\`cdc\` modes, it follows transaction log records **after** this version.

Note: at most one of \`version\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
      nullable: true
    },
    end_version: {
      type: 'integer',
      format: 'int64',
      description: `Optional final table version.

Valid only when the connector is configured in \`follow\`, \`snapshot_and_follow\`, or \`cdc\` mode.

When set, the connector will stop scanning the table’s transaction log after reaching this version or any greater version.
This bound is inclusive: if the specified version appears in the log, it will be processed before signaling end-of-input.`,
      nullable: true
    },
    filter: {
      type: 'string',
      description: `Optional row filter.

When specified, only rows that satisfy the filter condition are read from the delta table.
The condition must be a valid SQL Boolean expression that can be used in
the \`where\` clause of the \`select * from my_table where ...\` query.`,
      nullable: true
    },
    max_concurrent_readers: {
      type: 'integer',
      format: 'int32',
      description: `Maximum number of concurrent object store reads performed by all Delta Lake connectors.

This setting is used to limit the number of concurrent reads of the object store in a
pipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously
reading from the object store, this can lead to transport timeouts.

When enabled, this setting limits the number of concurrent reads across all connectors.
This is a global setting that affects all Delta Lake connectors, and not just the connector
where it is specified. It should therefore be used at most once in a pipeline.  If multiple
connectors specify this setting, they must all use the same value.

The default value is 6.`,
      nullable: true,
      minimum: 0
    },
    mode: {
      $ref: '#/components/schemas/DeltaTableIngestMode'
    },
    num_parsers: {
      type: 'integer',
      format: 'int32',
      description: `The number of parallel parsing tasks the connector uses to process data read from the
table. Increasing this value can enhance performance by allowing more concurrent processing.
Recommended range: 1–10. The default is 4.`,
      minimum: 0
    },
    skip_unused_columns: {
      type: 'boolean',
      description: `Don't read unused columns from the Delta table.

When set to \`true\`, this option instructs the connector to avoid reading
columns from the Delta table that are not used in any view definitions.
To be skipped, the columns must be either nullable or have default
values. This can improve ingestion performance, especially for wide
tables.

Note: The simplest way to exclude unused columns is to omit them from the Feldera SQL table
declaration. The connector never reads columns that aren't declared in the SQL schema.
Additionally, the SQL compiler emits warnings for declared but unused columns—use these as
a guide to optimize your schema.`
    },
    snapshot_filter: {
      type: 'string',
      description: `Optional snapshot filter.

This option is only valid when \`mode\` is set to \`snapshot\` or \`snapshot_and_follow\`.

When specified, only rows that satisfy the filter condition are included in the
snapshot.  The condition must be a valid SQL Boolean expression that can be used in
the \`where\` clause of the \`select * from snapshot where ...\` query.

Unlike the \`filter\` option, which applies to all records retrieved from the table, this
filter only applies to rows in the initial snapshot of the table.
For instance, it can be used to specify the range of event times to include in the snapshot,
e.g.: \`ts BETWEEN TIMESTAMP '2005-01-01 00:00:00' AND TIMESTAMP '2010-12-31 23:59:59'\`.

This option can be used together with the \`filter\` option. During the initial snapshot,
only rows that satisfy both \`filter\` and \`snapshot_filter\` are retrieved from the Delta table.
When subsequently following changes in the the transaction log (\`mode = snapshot_and_follow\`),
all rows that meet the \`filter\` condition are ingested, regardless of \`snapshot_filter\`.`,
      nullable: true
    },
    timestamp_column: {
      type: 'string',
      description: `Table column that serves as an event timestamp.

When this option is specified, and \`mode\` is one of \`snapshot\` or \`snapshot_and_follow\`,
table rows are ingested in the timestamp order, respecting the
[\`LATENESS\`](https://docs.feldera.com/sql/streaming#lateness-expressions)
property of the column: each ingested row has a timestamp no more than \`LATENESS\`
time units earlier than the most recent timestamp of any previously ingested row.
The ingestion is performed by partitioning the table into timestamp ranges of width
\`LATENESS\`. Each range is processed sequentially, in increasing timestamp order.

# Example

Consider a table with timestamp column of type \`TIMESTAMP\` and lateness attribute
\`INTERVAL 1 DAY\`. Assuming that the oldest timestamp in the table is
\`2024-01-01T00:00:00\`\`, the connector will fetch all records with timestamps
from \`2024-01-01\`, then all records for \`2024-01-02\`, \`2024-01-03\`, etc., until all records
in the table have been ingested.

# Requirements

* The timestamp column must be of a supported type: integer, \`DATE\`, or \`TIMESTAMP\`.
* The timestamp column must be declared with non-zero \`LATENESS\`.
* For efficient ingest, the table must be optimized for timestamp-based
queries using partitioning, Z-ordering, or liquid clustering.`,
      nullable: true
    },
    uri: {
      type: 'string',
      description: `Table URI.

Example: "s3://feldera-fraud-detection-data/demographics_train"`
    },
    version: {
      type: 'integer',
      format: 'int64',
      description: `Optional table version.

When this option is set, the connector finds and opens the specified version of the table.
In \`snapshot\` and \`snapshot_and_follow\` modes, it retrieves the snapshot of this version of
the table.  In \`follow\`, \`snapshot_and_follow\`, and \`cdc\` modes, it follows transaction log records
**after** this version.

Note: at most one of \`version\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
      nullable: true
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Storage options for configuring backend object store.

For specific options available for different storage backends, see:
* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)`
  }
} as const

export const $DeltaTableWriteMode = {
  type: 'string',
  description: `Delta table write mode.

Determines how the Delta table connector handles an existing table at the target location.`,
  enum: ['append', 'truncate', 'error_if_exists']
} as const

export const $DeltaTableWriterConfig = {
  type: 'object',
  description: 'Delta table output connector configuration.',
  required: ['uri'],
  properties: {
    mode: {
      $ref: '#/components/schemas/DeltaTableWriteMode'
    },
    uri: {
      type: 'string',
      description: 'Table URI.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Storage options for configuring backend object store.

For specific options available for different storage backends, see:
* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)`
  }
} as const

export const $Demo = {
  type: 'object',
  required: ['name', 'title', 'description', 'program_code', 'udf_rust', 'udf_toml'],
  properties: {
    description: {
      type: 'string',
      description: 'Description of the demo (parsed from SQL preamble).'
    },
    name: {
      type: 'string',
      description: 'Name of the demo (parsed from SQL preamble).'
    },
    program_code: {
      type: 'string',
      description: 'Program SQL code.'
    },
    title: {
      type: 'string',
      description: 'Title of the demo (parsed from SQL preamble).'
    },
    udf_rust: {
      type: 'string',
      description: 'User defined function (UDF) Rust code.'
    },
    udf_toml: {
      type: 'string',
      description: 'User defined function (UDF) TOML dependencies.'
    }
  }
} as const

export const $DisplaySchedule = {
  oneOf: [
    {
      type: 'string',
      description: 'Display it only once: after dismissal do not show it again',
      enum: ['Once']
    },
    {
      type: 'string',
      description: 'Display it again the next session if it is dismissed',
      enum: ['Session']
    },
    {
      type: 'object',
      required: ['Every'],
      properties: {
        Every: {
          type: 'object',
          description: 'Display it again after a certain period of time after it is dismissed',
          required: ['seconds'],
          properties: {
            seconds: {
              type: 'integer',
              format: 'int64',
              minimum: 0
            }
          }
        }
      }
    },
    {
      type: 'string',
      description: 'Always display it, do not allow it to be dismissed',
      enum: ['Always']
    }
  ]
} as const

export const $ErrorResponse = {
  type: 'object',
  description: 'Information returned by REST API endpoints on error.',
  required: ['message', 'error_code', 'details'],
  properties: {
    details: {
      type: 'object',
      description: `Detailed error metadata.
The contents of this field is determined by \`error_code\`.`
    },
    error_code: {
      type: 'string',
      description: 'Error code is a string that specifies this error type.',
      example: 'CodeSpecifyingErrorType'
    },
    message: {
      type: 'string',
      description: 'Human-readable error message.',
      example: 'Explanation of the error that occurred.'
    }
  }
} as const

export const $Field = {
  allOf: [
    {
      $ref: '#/components/schemas/SqlIdentifier'
    },
    {
      type: 'object',
      required: ['columntype', 'unused'],
      properties: {
        columntype: {
          $ref: '#/components/schemas/ColumnType'
        },
        default: {
          type: 'string',
          nullable: true
        },
        lateness: {
          type: 'string',
          nullable: true
        },
        unused: {
          type: 'boolean'
        },
        watermark: {
          type: 'string',
          nullable: true
        }
      }
    }
  ],
  description: `A SQL field.

Matches the SQL compiler JSON format.`
} as const

export const $FileBackendConfig = {
  type: 'object',
  description: 'Configuration for local file system access.',
  properties: {
    async_threads: {
      type: 'boolean',
      description: `Whether to use background threads for file I/O.

Background threads should improve performance, but they can reduce
performance if too few cores are available. This is provided for
debugging and fine-tuning and should ordinarily be left unset.`,
      default: null,
      nullable: true
    },
    ioop_delay: {
      type: 'integer',
      format: 'int64',
      description: `Per-I/O operation sleep duration, in milliseconds.

This is for simulating slow storage devices.  Do not use this in
production.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    sync: {
      allOf: [
        {
          $ref: '#/components/schemas/SyncConfig'
        }
      ],
      default: null,
      nullable: true
    }
  }
} as const

export const $FileInputConfig = {
  type: 'object',
  description: 'Configuration for reading data from a file with `FileInputTransport`',
  required: ['path'],
  properties: {
    buffer_size_bytes: {
      type: 'integer',
      description: `Read buffer size.

Default: when this parameter is not specified, a platform-specific
default is used.`,
      nullable: true,
      minimum: 0
    },
    follow: {
      type: 'boolean',
      description: `Enable file following.

When \`false\`, the endpoint outputs an \`InputConsumer::eoi\`
message and stops upon reaching the end of file.  When \`true\`, the
endpoint will keep watching the file and outputting any new content
appended to it.`
    },
    path: {
      type: 'string',
      description: `File path.

This may be a file name or a \`file://\` URL with an absolute path.`
    }
  }
} as const

export const $FileOutputConfig = {
  type: 'object',
  description: 'Configuration for writing data to a file with `FileOutputTransport`.',
  required: ['path'],
  properties: {
    path: {
      type: 'string',
      description: 'File path.'
    }
  }
} as const

export const $FormatConfig = {
  type: 'object',
  description: `Data format specification used to parse raw data received from the
endpoint or to encode data sent to the endpoint.`,
  required: ['name'],
  properties: {
    config: {
      type: 'object',
      description: 'Format-specific parser or encoder configuration.'
    },
    name: {
      type: 'string',
      description: 'Format name, e.g., "csv", "json", "bincode", etc.'
    }
  }
} as const

export const $FtConfig = {
  type: 'object',
  description: `Fault-tolerance configuration.

The default [FtConfig] (via [FtConfig::default]) disables fault tolerance,
which is the configuration that one gets if [RuntimeConfig] omits fault
tolerance configuration.

The default value for [FtConfig::model] enables fault tolerance, as
\`Some(FtModel::default())\`.  This is the configuration that one gets if
[RuntimeConfig] includes a fault tolerance configuration but does not
specify a particular model.`,
  properties: {
    checkpoint_interval_secs: {
      type: 'integer',
      format: 'int64',
      description: `Interval between automatic checkpoints, in seconds.

The default is 60 seconds.  Values less than 1 or greater than 3600 will
be forced into that range.`,
      nullable: true,
      minimum: 0
    },
    model: {
      oneOf: [
        {
          $ref: '#/components/schemas/FtModel'
        },
        {
          type: 'string',
          enum: ['none']
        }
      ],
      default: 'exactly_once'
    }
  }
} as const

export const $FtModel = {
  type: 'string',
  description: `Fault tolerance model.

The ordering is significant: we consider [Self::ExactlyOnce] to be a "higher
level" of fault tolerance than [Self::AtLeastOnce].`,
  enum: ['at_least_once', 'exactly_once']
} as const

export const $GenerationPlan = {
  type: 'object',
  description:
    'A random generation plan for a table that generates either a limited amount of rows or runs continuously.',
  properties: {
    fields: {
      type: 'object',
      description: 'Specifies the values that the generator should produce.',
      default: {},
      additionalProperties: {
        $ref: '#/components/schemas/RngFieldSettings'
      }
    },
    limit: {
      type: 'integer',
      description: `Total number of new rows to generate.

If not set, the generator will produce new/unique records as long as the pipeline is running.
If set to 0, the table will always remain empty.
If set, the generator will produce new records until the specified limit is reached.

Note that if the table has one or more primary keys that don't use the \`increment\` strategy to
generate the key there is a potential that an update is generated instead of an insert. In
this case it's possible the total number of records is less than the specified limit.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    rate: {
      type: 'integer',
      format: 'int32',
      description: `Non-zero number of rows to generate per second.

If not set, the generator will produce rows as fast as possible.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    worker_chunk_size: {
      type: 'integer',
      description: `When multiple workers are used, each worker will pick a consecutive "chunk" of
records to generate.

By default, if not specified, the generator will use the formula \`min(rate, 10_000)\`
to determine it. This works well in most situations. However, if you're
running tests with lateness and many workers you can e.g., reduce the
chunk size to make sure a smaller range of records is being ingested in parallel.

This also controls the sizes of input batches.  If, for example, \`rate\`
and \`worker_chunk_size\` are both 1000, with a single worker, the
generator will output 1000 records once a second.  But if we reduce
\`worker_chunk_size\` to 100 without changing \`rate\`, the generator will
instead output 100 records 10 times per second.

# Example
Assume you generate a total of 125 records with 4 workers and a chunk size of 25.
In this case, worker A will generate records 0..25, worker B will generate records 25..50,
etc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk
will pick up the last chunk of records (100..125) to generate.`,
      default: null,
      nullable: true,
      minimum: 0
    }
  },
  additionalProperties: false
} as const

export const $GetPipelineParameters = {
  type: 'object',
  description: 'Query parameters to GET a pipeline or a list of pipelines.',
  properties: {
    selector: {
      $ref: '#/components/schemas/PipelineFieldSelector'
    }
  }
} as const

export const $GlueCatalogConfig = {
  type: 'object',
  description: 'AWS Glue catalog config.',
  properties: {
    'glue.access-key-id': {
      type: 'string',
      description: 'Access key id used to access the Glue catalog.',
      nullable: true
    },
    'glue.endpoint': {
      type: 'string',
      description: `Configure an alternative endpoint of the Glue service for Glue catalog to access.

Example: \`"https://glue.us-east-1.amazonaws.com"\``,
      nullable: true
    },
    'glue.id': {
      type: 'string',
      description: 'The 12-digit ID of the Glue catalog.',
      nullable: true
    },
    'glue.profile-name': {
      type: 'string',
      description: 'Profile used to access the Glue catalog.',
      nullable: true
    },
    'glue.region': {
      type: 'string',
      description: 'Region of the Glue catalog.',
      nullable: true
    },
    'glue.secret-access-key': {
      type: 'string',
      description: 'Secret access key used to access the Glue catalog.',
      nullable: true
    },
    'glue.session-token': {
      type: 'string',
      nullable: true
    },
    'glue.warehouse': {
      type: 'string',
      description: `Location for table metadata.

Example: \`"s3://my-data-warehouse/tables/"\``,
      nullable: true
    }
  }
} as const

export const $HealthStatus = {
  type: 'object',
  required: ['runner', 'compiler'],
  properties: {
    compiler: {
      $ref: '#/components/schemas/ServiceStatus'
    },
    runner: {
      $ref: '#/components/schemas/ServiceStatus'
    }
  }
} as const

export const $HttpInputConfig = {
  type: 'object',
  description: `Configuration for reading data via HTTP.

HTTP input adapters cannot be usefully configured as part of pipeline
configuration.  Instead, instantiate them through the REST API as
\`/pipelines/{pipeline_name}/ingress/{table_name}\`.`,
  required: ['name'],
  properties: {
    name: {
      type: 'string',
      description: 'Autogenerated name.'
    }
  }
} as const

export const $IcebergCatalogType = {
  type: 'string',
  enum: ['rest', 'glue']
} as const

export const $IcebergIngestMode = {
  type: 'string',
  description: `Iceberg table read mode.

Three options are available:

* \`snapshot\` - read a snapshot of the table and stop.

* \`follow\` - continuously ingest changes to the table, starting from a specified snapshot
or timestamp.

* \`snapshot_and_follow\` - read a snapshot of the table before switching to continuous ingestion
mode.`,
  enum: ['snapshot', 'follow', 'snapshot_and_follow']
} as const

export const $IcebergReaderConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/GlueCatalogConfig'
    },
    {
      $ref: '#/components/schemas/RestCatalogConfig'
    },
    {
      type: 'object',
      required: ['mode'],
      properties: {
        catalog_type: {
          allOf: [
            {
              $ref: '#/components/schemas/IcebergCatalogType'
            }
          ],
          nullable: true
        },
        datetime: {
          type: 'string',
          description: `Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
"2024-12-09T16:09:53+00:00".

When this option is set, the connector finds and opens the snapshot of the table as of the
specified point in time (based on the server time recorded in the transaction
log, not the event time encoded in the data).  In \`snapshot\` and \`snapshot_and_follow\`
modes, it retrieves this snapshot.  In \`follow\` and \`snapshot_and_follow\` modes, it
follows transaction log records **after** this snapshot.

Note: at most one of \`snapshot_id\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
          nullable: true
        },
        metadata_location: {
          type: 'string',
          description: `Location of the table metadata JSON file.

This propery is used to access an Iceberg table without a catalog. It is mutually
exclusive with the \`catalog_type\` property.`,
          nullable: true
        },
        mode: {
          $ref: '#/components/schemas/IcebergIngestMode'
        },
        snapshot_filter: {
          type: 'string',
          description: `Optional row filter.

This option is only valid when \`mode\` is set to \`snapshot\` or \`snapshot_and_follow\`.

When specified, only rows that satisfy the filter condition are included in the
snapshot.  The condition must be a valid SQL Boolean expression that can be used in
the \`where\` clause of the \`select * from snapshot where ...\` query.

This option can be used to specify the range of event times to include in the snapshot,
e.g.: \`ts BETWEEN '2005-01-01 00:00:00' AND '2010-12-31 23:59:59'\`.`,
          nullable: true
        },
        snapshot_id: {
          type: 'integer',
          format: 'int64',
          description: `Optional snapshot id.

When this option is set, the connector finds the specified snapshot of the table.
In \`snapshot\` and \`snapshot_and_follow\` modes, it loads this snapshot.
In \`follow\` and \`snapshot_and_follow\` modes, it follows table updates
**after** this snapshot.

Note: at most one of \`snapshot_id\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
          nullable: true
        },
        table_name: {
          type: 'string',
          description: `Specifies the Iceberg table name in the "namespace.table" format.

This option is applicable when an Iceberg catalog is configured using the \`catalog_type\` property.`,
          nullable: true
        },
        timestamp_column: {
          type: 'string',
          description: `Table column that serves as an event timestamp.

When this option is specified, and \`mode\` is one of \`snapshot\` or \`snapshot_and_follow\`,
table rows are ingested in the timestamp order, respecting the
[\`LATENESS\`](https://docs.feldera.com/sql/streaming#lateness-expressions)
property of the column: each ingested row has a timestamp no more than \`LATENESS\`
time units earlier than the most recent timestamp of any previously ingested row.
The ingestion is performed by partitioning the table into timestamp ranges of width
\`LATENESS\`. Each range is processed sequentially, in increasing timestamp order.

# Example

Consider a table with timestamp column of type \`TIMESTAMP\` and lateness attribute
\`INTERVAL 1 DAY\`. Assuming that the oldest timestamp in the table is
\`2024-01-01T00:00:00\`\`, the connector will fetch all records with timestamps
from \`2024-01-01\`, then all records for \`2024-01-02\`, \`2024-01-03\`, etc., until all records
in the table have been ingested.

# Requirements

* The timestamp column must be of a supported type: integer, \`DATE\`, or \`TIMESTAMP\`.
* The timestamp column must be declared with non-zero \`LATENESS\`.
* For efficient ingest, the table must be optimized for timestamp-based
queries using partitioning, Z-ordering, or liquid clustering.`,
          nullable: true
        }
      },
      additionalProperties: {
        type: 'string',
        description: `Storage options for configuring backend object store.

See the [list of available options in PyIceberg documentation](https://py.iceberg.apache.org/configuration/#fileio).`
      }
    }
  ],
  description: 'Iceberg input connector configuration.'
} as const

export const $InputEndpointConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    {
      type: 'object',
      required: ['stream'],
      properties: {
        stream: {
          type: 'string',
          description: `The name of the input stream of the circuit that this endpoint is
connected to.`
        }
      }
    }
  ],
  description: 'Describes an input connector configuration'
} as const

export const $IntervalUnit = {
  type: 'string',
  description: `The specified units for SQL Interval types.

\`INTERVAL 1 DAY\`, \`INTERVAL 1 DAY TO HOUR\`, \`INTERVAL 1 DAY TO MINUTE\`,
would yield \`Day\`, \`DayToHour\`, \`DayToMinute\`, as the \`IntervalUnit\` respectively.`,
  enum: [
    'Day',
    'DayToHour',
    'DayToMinute',
    'DayToSecond',
    'Hour',
    'HourToMinute',
    'HourToSecond',
    'Minute',
    'MinuteToSecond',
    'Month',
    'Second',
    'Year',
    'YearToMonth'
  ]
} as const

export const $JsonLines = {
  type: 'string',
  description: 'Whether JSON values can span multiple lines.',
  enum: ['multiple', 'single']
} as const

export const $JsonUpdateFormat = {
  type: 'string',
  description: `Supported JSON data change event formats.

Each element in a JSON-formatted input stream specifies
an update to one or more records in an input table.  We support
several different ways to represent such updates.

### \`InsertDelete\`

Each element in the input stream consists of an "insert" or "delete"
command and a record to be inserted to or deleted from the input table.

\`\`\`json
{"insert": {"column1": "hello, world!", "column2": 100}}
\`\`\`

### \`Weighted\`

Each element in the input stream consists of a record and a weight
which indicates how many times the row appears.

\`\`\`json
{"weight": 2, "data": {"column1": "hello, world!", "column2": 100}}
\`\`\`

Note that the line above would be equivalent to the following input in the \`InsertDelete\` format:

\`\`\`json
{"insert": {"column1": "hello, world!", "column2": 100}}
{"insert": {"column1": "hello, world!", "column2": 100}}
\`\`\`

Similarly, negative weights are equivalent to deletions:

\`\`\`json
{"weight": -1, "data": {"column1": "hello, world!", "column2": 100}}
\`\`\`

is equivalent to in the \`InsertDelete\` format:

\`\`\`json
{"delete": {"column1": "hello, world!", "column2": 100}}
\`\`\`

### \`Debezium\`

Debezium CDC format.  Refer to [Debezium input connector documentation](https://docs.feldera.com/connectors/sources/debezium) for details.

### \`Snowflake\`

Uses flat structure so that fields can get parsed directly into SQL
columns.  Defines three metadata fields:

* \`__action\` - "insert" or "delete"
* \`__stream_id\` - unique 64-bit ID of the output stream (records within
a stream are totally ordered)
* \`__seq_number\` - monotonically increasing sequence number relative to
the start of the stream.

\`\`\`json
{"PART":1,"VENDOR":2,"EFFECTIVE_SINCE":"2019-05-21","PRICE":"10000","__action":"insert","__stream_id":4523666124030717756,"__seq_number":1}
\`\`\`

### \`Raw\`

This format is suitable for insert-only streams (no deletions).
Each element in the input stream contains a record without any
additional envelope that gets inserted in the input table.`,
  enum: ['insert_delete', 'weighted', 'debezium', 'snowflake', 'raw', 'redis']
} as const

export const $KafkaHeader = {
  type: 'object',
  description: 'Kafka message header.',
  required: ['key'],
  properties: {
    key: {
      type: 'string'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaHeaderValue'
        }
      ],
      nullable: true
    }
  }
} as const

export const $KafkaHeaderValue = {
  type: 'string',
  format: 'binary',
  description: 'Kafka header value encoded as a UTF-8 string or a byte array.'
} as const

export const $KafkaInputConfig = {
  type: 'object',
  description: 'Configuration for reading data from Kafka topics with `InputTransport`.',
  required: ['topic'],
  properties: {
    group_join_timeout_secs: {
      type: 'integer',
      format: 'int32',
      description: `Maximum timeout in seconds to wait for the endpoint to join the Kafka
consumer group during initialization.`,
      minimum: 0
    },
    log_level: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaLogLevel'
        }
      ],
      nullable: true
    },
    partitions: {
      type: 'array',
      items: {
        type: 'integer',
        format: 'int32'
      },
      description: `The list of Kafka partitions to read from.

Only the specified partitions will be consumed. If this field is not set,
the connector will consume from all available partitions.

If \`start_from\` is set to \`offsets\` and this field is provided, the
number of partitions must exactly match the number of offsets, and the
order of partitions must correspond to the order of offsets.

If offsets are provided for all partitions, this field can be omitted.`,
      nullable: true
    },
    poller_threads: {
      type: 'integer',
      description: `Set to 1 or more to fix the number of threads used to poll
\`rdkafka\`. Multiple threads can increase performance with small Kafka
messages; for large messages, one thread is enough. In either case, too
many threads can harm performance. If unset, the default is 3, which
helps with small messages but will not harm performance with large
messagee`,
      nullable: true,
      minimum: 0
    },
    region: {
      type: 'string',
      description:
        'The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).',
      nullable: true
    },
    start_from: {
      $ref: '#/components/schemas/KafkaStartFromConfig'
    },
    topic: {
      type: 'string',
      description: 'Topic to subscribe to.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Options passed directly to \`rdkafka\`.

[\`librdkafka\` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
used to configure the Kafka consumer.

This input connector does not use consumer groups, so options related to
consumer groups are rejected, including:

* \`group.id\`, if present, is ignored.
* \`auto.offset.reset\` (use \`start_from\` instead).
* "enable.auto.commit", if present, must be set to "false".
* "enable.auto.offset.store", if present, must be set to "false".`
  }
} as const

export const $KafkaLogLevel = {
  type: 'string',
  description: 'Kafka logging levels.',
  enum: ['emerg', 'alert', 'critical', 'error', 'warning', 'notice', 'info', 'debug']
} as const

export const $KafkaOutputConfig = {
  type: 'object',
  description: 'Configuration for writing data to a Kafka topic with `OutputTransport`.',
  required: ['topic'],
  properties: {
    fault_tolerance: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaOutputFtConfig'
        }
      ],
      nullable: true
    },
    headers: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/KafkaHeader'
      },
      description: 'Kafka headers to be added to each message produced by this connector.'
    },
    initialization_timeout_secs: {
      type: 'integer',
      format: 'int32',
      description: `Maximum timeout in seconds to wait for the endpoint to connect to
a Kafka broker.

Defaults to 60.`,
      minimum: 0
    },
    kafka_service: {
      type: 'string',
      description: 'If specified, this service is used to provide defaults for the Kafka options.',
      nullable: true
    },
    log_level: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaLogLevel'
        }
      ],
      nullable: true
    },
    region: {
      type: 'string',
      description:
        'The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).',
      nullable: true
    },
    topic: {
      type: 'string',
      description: 'Topic to write to.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Options passed directly to \`rdkafka\`.

See [\`librdkafka\` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
used to configure the Kafka producer.`
  }
} as const

export const $KafkaOutputFtConfig = {
  type: 'object',
  description: 'Fault tolerance configuration for Kafka output connector.',
  properties: {
    consumer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for consumers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for consumers, and may be empty.`,
      default: {},
      additionalProperties: {
        type: 'string'
      }
    },
    producer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for producers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for producers, and may be empty.`,
      default: {},
      additionalProperties: {
        type: 'string'
      }
    }
  }
} as const

export const $KafkaStartFromConfig = {
  oneOf: [
    {
      type: 'string',
      description: 'Start from the beginning of the topic.',
      enum: ['earliest']
    },
    {
      type: 'string',
      description: `Start from the current end of the topic.

This will only read any data that is added to the topic after the
connector initializes.`,
      enum: ['latest']
    },
    {
      type: 'object',
      required: ['offsets'],
      properties: {
        offsets: {
          type: 'array',
          items: {
            type: 'integer',
            format: 'int64'
          },
          description: `Start from particular offsets in the topic.

The number of offsets must match the number of partitions in the topic.`
        }
      }
    }
  ],
  description: 'Where to begin reading a Kafka topic.'
} as const

export const $LicenseInformation = {
  type: 'object',
  required: ['current', 'is_trial', 'description_html', 'remind_schedule'],
  properties: {
    current: {
      type: 'string',
      format: 'date-time',
      description: 'Timestamp when the server responded.'
    },
    description_html: {
      type: 'string',
      description:
        'Optional description of the advantages of extending the license / upgrading from a trial'
    },
    extension_url: {
      type: 'string',
      description: 'URL that navigates the user to extend / upgrade their license',
      nullable: true
    },
    is_trial: {
      type: 'boolean',
      description: 'Whether the license is a trial'
    },
    remind_schedule: {
      $ref: '#/components/schemas/DisplaySchedule'
    },
    remind_starting_at: {
      type: 'string',
      format: 'date-time',
      description: 'Timestamp from which the user should be reminded of the license expiring soon',
      nullable: true
    },
    valid_until: {
      type: 'string',
      format: 'date-time',
      description: 'Timestamp at which point the license expires',
      nullable: true
    }
  }
} as const

export const $LicenseValidity = {
  oneOf: [
    {
      type: 'object',
      required: ['Exists'],
      properties: {
        Exists: {
          $ref: '#/components/schemas/LicenseInformation'
        }
      }
    },
    {
      type: 'object',
      required: ['DoesNotExistOrNotConfirmed'],
      properties: {
        DoesNotExistOrNotConfirmed: {
          type: 'string',
          description: `Either the license key is invalid according to the server, or the request that checks with
the server failed (e.g., if it could not reach the server).`
        }
      }
    }
  ]
} as const

export const $MetricsFormat = {
  type: 'string',
  description: `Circuit metrics output format.
- \`prometheus\`: [format](https://github.com/prometheus/docs/blob/4b1b80f5f660a2f8dc25a54f52a65a502f31879a/docs/instrumenting/exposition_formats.md) expected by Prometheus
- \`json\`: JSON format`,
  enum: ['prometheus', 'json']
} as const

export const $MetricsParameters = {
  type: 'object',
  description: 'Query parameters to retrieve pipeline circuit metrics.',
  properties: {
    format: {
      $ref: '#/components/schemas/MetricsFormat'
    }
  }
} as const

export const $NewApiKeyRequest = {
  type: 'object',
  description: 'Request to create a new API key.',
  required: ['name'],
  properties: {
    name: {
      type: 'string',
      description: 'Key name.',
      example: 'my-api-key'
    }
  }
} as const

export const $NewApiKeyResponse = {
  type: 'object',
  description: 'Response to a successful API key creation.',
  required: ['id', 'name', 'api_key'],
  properties: {
    api_key: {
      type: 'string',
      description: `Generated secret API key. There is no way to retrieve this
key again through the API, so store it securely.`,
      example:
        'apikey:v5y5QNtlPNVMwkmNjKwFU8bbIu5lMge3yHbyddxAOdXlEo84SEoNn32DUhQaf1KLeI9aOOfnJjhQ1pYzMrU4wQXON6pm6BS7Zgzj46U2b8pwz1280vYBEtx41hiDBRP'
    },
    id: {
      $ref: '#/components/schemas/ApiKeyId'
    },
    name: {
      type: 'string',
      description: 'API key name provided by the user.',
      example: 'my-api-key'
    }
  }
} as const

export const $NexmarkInputConfig = {
  type: 'object',
  description: `Configuration for generating Nexmark input data.

This connector must be used exactly three times in a pipeline if it is used
at all, once for each [\`NexmarkTable\`].`,
  required: ['table'],
  properties: {
    options: {
      allOf: [
        {
          $ref: '#/components/schemas/NexmarkInputOptions'
        }
      ],
      nullable: true
    },
    table: {
      $ref: '#/components/schemas/NexmarkTable'
    }
  }
} as const

export const $NexmarkInputOptions = {
  type: 'object',
  description: 'Configuration for generating Nexmark input data.',
  properties: {
    batch_size_per_thread: {
      type: 'integer',
      format: 'int64',
      description: `Number of events to generate and submit together, per thread.

Each thread generates this many records, which are then combined with
the records generated by the other threads, to form combined input
batches of size \`threads × batch_size_per_thread\`.`,
      default: 1000,
      minimum: 0
    },
    events: {
      type: 'integer',
      format: 'int64',
      description: 'Number of events to generate.',
      default: 100000000,
      minimum: 0
    },
    max_step_size_per_thread: {
      type: 'integer',
      format: 'int64',
      description: `Maximum number of events to submit in a single step, per thread.

This should really be per worker thread, not per generator thread, but
the connector does not know how many worker threads there are.

This stands in for \`max_batch_size\` from the connector configuration
because it must be a constant across all three of the nexmark tables.`,
      default: 10000,
      minimum: 0
    },
    threads: {
      type: 'integer',
      description: `Number of event generator threads.

It's reasonable to choose the same number of generator threads as worker
threads.`,
      default: 4,
      minimum: 0
    }
  }
} as const

export const $NexmarkTable = {
  type: 'string',
  description: 'Table in Nexmark.',
  enum: ['bid', 'auction', 'person']
} as const

export const $ObjectStorageConfig = {
  type: 'object',
  required: ['url'],
  properties: {
    url: {
      type: 'string',
      description: `URL.

The following URL schemes are supported:

* S3:
- \`s3://<bucket>/<path>\`
- \`s3a://<bucket>/<path>\`
- \`https://s3.<region>.amazonaws.com/<bucket>\`
- \`https://<bucket>.s3.<region>.amazonaws.com\`
- \`https://ACCOUNT_ID.r2.cloudflarestorage.com/bucket\`
* Google Cloud Storage:
- \`gs://<bucket>/<path>\`
* Microsoft Azure Blob Storage:
- \`abfs[s]://<container>/<path>\` (according to [fsspec](https://github.com/fsspec/adlfs))
- \`abfs[s]://<file_system>@<account_name>.dfs.core.windows.net/<path>\`
- \`abfs[s]://<file_system>@<account_name>.dfs.fabric.microsoft.com/<path>\`
- \`az://<container>/<path>\` (according to [fsspec](https://github.com/fsspec/adlfs))
- \`adl://<container>/<path>\` (according to [fsspec](https://github.com/fsspec/adlfs))
- \`azure://<container>/<path>\` (custom)
- \`https://<account>.dfs.core.windows.net\`
- \`https://<account>.blob.core.windows.net\`
- \`https://<account>.blob.core.windows.net/<container>\`
- \`https://<account>.dfs.fabric.microsoft.com\`
- \`https://<account>.dfs.fabric.microsoft.com/<container>\`
- \`https://<account>.blob.fabric.microsoft.com\`
- \`https://<account>.blob.fabric.microsoft.com/<container>\`

Settings derived from the URL will override other settings.`
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Additional options as key-value pairs.

The following keys are supported:

* S3:
- \`access_key_id\`: AWS Access Key.
- \`secret_access_key\`: AWS Secret Access Key.
- \`region\`: Region.
- \`default_region\`: Default region.
- \`endpoint\`: Custom endpoint for communicating with S3,
e.g. \`https://localhost:4566\` for testing against a localstack
instance.
- \`token\`: Token to use for requests (passed to underlying provider).
- [Other keys](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html#variants).
* Google Cloud Storage:
- \`service_account\`: Path to the service account file.
- \`service_account_key\`: The serialized service account key.
- \`google_application_credentials\`: Application credentials path.
- [Other keys](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html).
* Microsoft Azure Blob Storage:
- \`access_key\`: Azure Access Key.
- \`container_name\`: Azure Container Name.
- \`account\`: Azure Account.
- \`bearer_token_authorization\`: Static bearer token for authorizing requests.
- \`client_id\`: Client ID for use in client secret or Kubernetes federated credential flow.
- \`client_secret\`: Client secret for use in client secret flow.
- \`tenant_id\`: Tenant ID for use in client secret or Kubernetes federated credential flow.
- \`endpoint\`: Override the endpoint for communicating with blob storage.
- [Other keys](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html#variants).

Options set through the URL take precedence over those set with these
options.`
  }
} as const

export const $OutputBufferConfig = {
  type: 'object',
  properties: {
    enable_output_buffer: {
      type: 'boolean',
      description: `Enable output buffering.

The output buffering mechanism allows decoupling the rate at which the pipeline
pushes changes to the output transport from the rate of input changes.

By default, output updates produced by the pipeline are pushed directly to
the output transport. Some destinations may prefer to receive updates in fewer
bigger batches. For instance, when writing Parquet files, producing
one bigger file every few minutes is usually better than creating
small files every few milliseconds.

To achieve such input/output decoupling, users can enable output buffering by
setting the \`enable_output_buffer\` flag to \`true\`.  When buffering is enabled, output
updates produced by the pipeline are consolidated in an internal buffer and are
pushed to the output transport when one of several conditions is satisfied:

* data has been accumulated in the buffer for more than \`max_output_buffer_time_millis\`
milliseconds.
* buffer size exceeds \`max_output_buffer_size_records\` records.

This flag is \`false\` by default.`,
      default: false
    },
    max_output_buffer_size_records: {
      type: 'integer',
      description: `Maximum number of updates to be kept in the output buffer.

This parameter bounds the maximal size of the buffer.
Note that the size of the buffer is not always equal to the
total number of updates output by the pipeline. Updates to the
same record can overwrite or cancel previous updates.

By default, the buffer can grow indefinitely until one of
the other output conditions is satisfied.

NOTE: this configuration option requires the \`enable_output_buffer\` flag
to be set.`,
      default: 18446744073709552000,
      minimum: 0
    },
    max_output_buffer_time_millis: {
      type: 'integer',
      description: `Maximum time in milliseconds data is kept in the output buffer.

By default, data is kept in the buffer indefinitely until one of
the other output conditions is satisfied.  When this option is
set the buffer will be flushed at most every
\`max_output_buffer_time_millis\` milliseconds.

NOTE: this configuration option requires the \`enable_output_buffer\` flag
to be set.`,
      default: 18446744073709552000,
      minimum: 0
    }
  }
} as const

export const $OutputEndpointConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    {
      type: 'object',
      required: ['stream'],
      properties: {
        stream: {
          type: 'string',
          description: `The name of the output stream of the circuit that this endpoint is
connected to.`
        }
      }
    }
  ],
  description: 'Describes an output connector configuration'
} as const

export const $PartialProgramInfo = {
  type: 'object',
  description: 'Program information is the result of the SQL compilation.',
  required: ['schema', 'udf_stubs', 'input_connectors', 'output_connectors'],
  properties: {
    input_connectors: {
      type: 'object',
      description: 'Input connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/InputEndpointConfig'
      }
    },
    output_connectors: {
      type: 'object',
      description: 'Output connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/OutputEndpointConfig'
      }
    },
    schema: {
      $ref: '#/components/schemas/ProgramSchema'
    },
    udf_stubs: {
      type: 'string',
      description: 'Generated user defined function (UDF) stubs Rust code: stubs.rs'
    }
  }
} as const

export const $PatchPipeline = {
  type: 'object',
  description: `Partially update the pipeline (PATCH).

Note that the patching only applies to the main fields, not subfields.
For instance, it is not possible to update only the number of workers;
it is required to again pass the whole runtime configuration with the
change.`,
  properties: {
    description: {
      type: 'string',
      nullable: true
    },
    name: {
      type: 'string',
      nullable: true
    },
    program_code: {
      type: 'string',
      nullable: true
    },
    program_config: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramConfig'
        }
      ],
      nullable: true
    },
    runtime_config: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeConfig'
        }
      ],
      nullable: true
    },
    udf_rust: {
      type: 'string',
      nullable: true
    },
    udf_toml: {
      type: 'string',
      nullable: true
    }
  }
} as const

export const $PipelineConfig = {
  allOf: [
    {
      type: 'object',
      description: `Global pipeline configuration settings. This is the publicly
exposed type for users to configure pipelines.`,
      properties: {
        checkpoint_during_suspend: {
          type: 'boolean',
          description: 'Deprecated: setting this true or false does not have an effect anymore.',
          default: true
        },
        clock_resolution_usecs: {
          type: 'integer',
          format: 'int64',
          description: `Real-time clock resolution in microseconds.

This parameter controls the execution of queries that use the \`NOW()\` function.  The output of such
queries depends on the real-time clock and can change over time without any external
inputs.  If the query uses \`NOW()\`, the pipeline will update the clock value and trigger incremental
recomputation at most each \`clock_resolution_usecs\` microseconds.  If the query does not use
\`NOW()\`, then clock value updates are suppressed and the pipeline ignores this setting.

It is set to 1 second (1,000,000 microseconds) by default.`,
          default: 1000000,
          nullable: true,
          minimum: 0
        },
        cpu_profiler: {
          type: 'boolean',
          description: `Enable CPU profiler.

The default value is \`true\`.`,
          default: true
        },
        dev_tweaks: {
          type: 'object',
          description: `Optional settings for tweaking Feldera internals.

The available key-value pairs change from one version of Feldera to
another, so users should not depend on particular settings being
available, or on their behavior.`,
          default: {},
          additionalProperties: {}
        },
        fault_tolerance: {
          allOf: [
            {
              $ref: '#/components/schemas/FtConfig'
            }
          ],
          default: {
            model: 'none',
            checkpoint_interval_secs: 60
          }
        },
        http_workers: {
          type: 'integer',
          format: 'int64',
          description: `Sets the number of available runtime threads for the http server.

In most cases, this does not need to be set explicitly and
the default is sufficient. Can be increased in case the
pipeline HTTP API operations are a bottleneck.

If not specified, the default is set to \`workers\`.`,
          default: null,
          nullable: true,
          minimum: 0
        },
        init_containers: {
          description: 'Specification of additional (sidecar) containers.',
          nullable: true
        },
        io_workers: {
          type: 'integer',
          format: 'int64',
          description: `Sets the number of available runtime threads for async IO tasks.

This affects some networking and file I/O operations
especially adapters and ad-hoc queries.

In most cases, this does not need to be set explicitly and
the default is sufficient. Can be increased in case
ingress, egress or ad-hoc queries are a bottleneck.

If not specified, the default is set to \`workers\`.`,
          default: null,
          nullable: true,
          minimum: 0
        },
        logging: {
          type: 'string',
          description: `Log filtering directives.

If set to a valid [tracing-subscriber] filter, this controls the log
messages emitted by the pipeline process.  Otherwise, or if the filter
has invalid syntax, messages at "info" severity and higher are written
to the log and all others are discarded.

[tracing-subscriber]: https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#directives`,
          default: null,
          nullable: true
        },
        max_buffering_delay_usecs: {
          type: 'integer',
          format: 'int64',
          description: `Maximal delay in microseconds to wait for \`min_batch_size_records\` to
get buffered by the controller, defaults to 0.`,
          default: 0,
          minimum: 0
        },
        max_parallel_connector_init: {
          type: 'integer',
          format: 'int64',
          description: `The maximum number of connectors initialized in parallel during pipeline
startup.

At startup, the pipeline must initialize all of its input and output connectors.
Depending on the number and types of connectors, this can take a long time.
To accelerate the process, multiple connectors are initialized concurrently.
This option controls the maximum number of connectors that can be initialized
in parallel.

The default is 10.`,
          default: null,
          nullable: true,
          minimum: 0
        },
        min_batch_size_records: {
          type: 'integer',
          format: 'int64',
          description: `Minimal input batch size.

The controller delays pushing input records to the circuit until at
least \`min_batch_size_records\` records have been received (total
across all endpoints) or \`max_buffering_delay_usecs\` microseconds
have passed since at least one input records has been buffered.
Defaults to 0.`,
          default: 0,
          minimum: 0
        },
        pin_cpus: {
          type: 'array',
          items: {
            type: 'integer',
            minimum: 0
          },
          description: `Optionally, a list of CPU numbers for CPUs to which the pipeline may pin
its worker threads.  Specify at least twice as many CPU numbers as
workers.  CPUs are generally numbered starting from 0.  The pipeline
might not be able to honor CPU pinning requests.

CPU pinning can make pipelines run faster and perform more consistently,
as long as different pipelines running on the same machine are pinned to
different CPUs.`,
          default: []
        },
        provisioning_timeout_secs: {
          type: 'integer',
          format: 'int64',
          description: `Timeout in seconds for the \`Provisioning\` phase of the pipeline.
Setting this value will override the default of the runner.`,
          default: null,
          nullable: true,
          minimum: 0
        },
        resources: {
          allOf: [
            {
              $ref: '#/components/schemas/ResourceConfig'
            }
          ],
          default: {
            cpu_cores_min: null,
            cpu_cores_max: null,
            memory_mb_min: null,
            memory_mb_max: null,
            storage_mb_max: null,
            storage_class: null
          }
        },
        storage: {
          allOf: [
            {
              $ref: '#/components/schemas/StorageOptions'
            }
          ],
          default: {
            backend: {
              name: 'default'
            },
            min_storage_bytes: null,
            min_step_storage_bytes: null,
            compression: 'default',
            cache_mib: null
          },
          nullable: true
        },
        tracing: {
          type: 'boolean',
          description: 'Enable pipeline tracing.',
          default: false
        },
        tracing_endpoint_jaeger: {
          type: 'string',
          description: 'Jaeger tracing endpoint to send tracing information to.',
          default: '127.0.0.1:6831'
        },
        workers: {
          type: 'integer',
          format: 'int32',
          description: `Number of DBSP worker threads.

Each DBSP "foreground" worker thread is paired with a "background"
thread for LSM merging, making the total number of threads twice the
specified number.

The typical sweet spot for the number of workers is between 4 and 16.
Each worker increases overall memory consumption for data structures
used during a step.`,
          default: 8,
          minimum: 0
        }
      }
    },
    {
      type: 'object',
      required: ['inputs'],
      properties: {
        inputs: {
          type: 'object',
          description: 'Input endpoint configuration.',
          additionalProperties: {
            $ref: '#/components/schemas/InputEndpointConfig'
          }
        },
        name: {
          type: 'string',
          description: 'Pipeline name.',
          nullable: true
        },
        outputs: {
          type: 'object',
          description: 'Output endpoint configuration.',
          additionalProperties: {
            $ref: '#/components/schemas/OutputEndpointConfig'
          }
        },
        secrets_dir: {
          type: 'string',
          description: `Directory containing values of secrets.

If this is not set, a default directory is used.`,
          nullable: true
        },
        storage_config: {
          allOf: [
            {
              $ref: '#/components/schemas/StorageConfig'
            }
          ],
          nullable: true
        }
      }
    }
  ],
  description: `Pipeline deployment configuration.
It represents configuration entries directly provided by the user
(e.g., runtime configuration) and entries derived from the schema
of the compiled program (e.g., connectors). Storage configuration,
if applicable, is set by the runner.`
} as const

export const $PipelineFieldSelector = {
  type: 'string',
  enum: ['all', 'status']
} as const

export const $PipelineId = {
  type: 'string',
  format: 'uuid',
  description: 'Pipeline identifier.'
} as const

export const $PipelineInfo = {
  type: 'object',
  description: `Pipeline information.
It both includes fields which are user-provided and system-generated.`,
  required: [
    'id',
    'name',
    'description',
    'created_at',
    'version',
    'platform_version',
    'runtime_config',
    'program_code',
    'udf_rust',
    'udf_toml',
    'program_config',
    'program_version',
    'program_status',
    'program_status_since',
    'program_error',
    'refresh_version',
    'storage_status',
    'deployment_status',
    'deployment_status_since',
    'deployment_desired_status',
    'deployment_desired_status_since',
    'deployment_resources_status',
    'deployment_resources_status_since',
    'deployment_resources_desired_status',
    'deployment_resources_desired_status_since'
  ],
  properties: {
    created_at: {
      type: 'string',
      format: 'date-time'
    },
    deployment_desired_status: {
      $ref: '#/components/schemas/CombinedDesiredStatus'
    },
    deployment_desired_status_since: {
      type: 'string',
      format: 'date-time'
    },
    deployment_error: {
      allOf: [
        {
          $ref: '#/components/schemas/ErrorResponse'
        }
      ],
      nullable: true
    },
    deployment_id: {
      type: 'string',
      format: 'uuid',
      nullable: true
    },
    deployment_initial: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeDesiredStatus'
        }
      ],
      nullable: true
    },
    deployment_resources_desired_status: {
      $ref: '#/components/schemas/ResourcesDesiredStatus'
    },
    deployment_resources_desired_status_since: {
      type: 'string',
      format: 'date-time'
    },
    deployment_resources_status: {
      $ref: '#/components/schemas/ResourcesStatus'
    },
    deployment_resources_status_since: {
      type: 'string',
      format: 'date-time'
    },
    deployment_runtime_desired_status: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeDesiredStatus'
        }
      ],
      nullable: true
    },
    deployment_runtime_desired_status_since: {
      type: 'string',
      format: 'date-time',
      nullable: true
    },
    deployment_runtime_status: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeStatus'
        }
      ],
      nullable: true
    },
    deployment_runtime_status_since: {
      type: 'string',
      format: 'date-time',
      nullable: true
    },
    deployment_status: {
      $ref: '#/components/schemas/CombinedStatus'
    },
    deployment_status_since: {
      type: 'string',
      format: 'date-time'
    },
    description: {
      type: 'string'
    },
    id: {
      $ref: '#/components/schemas/PipelineId'
    },
    name: {
      type: 'string'
    },
    platform_version: {
      type: 'string'
    },
    program_code: {
      type: 'string'
    },
    program_config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    program_error: {
      $ref: '#/components/schemas/ProgramError'
    },
    program_info: {
      allOf: [
        {
          $ref: '#/components/schemas/PartialProgramInfo'
        }
      ],
      nullable: true
    },
    program_status: {
      $ref: '#/components/schemas/ProgramStatus'
    },
    program_status_since: {
      type: 'string',
      format: 'date-time'
    },
    program_version: {
      $ref: '#/components/schemas/Version'
    },
    refresh_version: {
      $ref: '#/components/schemas/Version'
    },
    runtime_config: {
      $ref: '#/components/schemas/RuntimeConfig'
    },
    storage_status: {
      $ref: '#/components/schemas/StorageStatus'
    },
    udf_rust: {
      type: 'string'
    },
    udf_toml: {
      type: 'string'
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $PipelineSelectedInfo = {
  type: 'object',
  description: `Pipeline information which has a selected subset of optional fields.
It both includes fields which are user-provided and system-generated.
If an optional field is not selected (i.e., is \`None\`), it will not be serialized.`,
  required: [
    'id',
    'name',
    'description',
    'created_at',
    'version',
    'platform_version',
    'program_version',
    'program_status',
    'program_status_since',
    'refresh_version',
    'storage_status',
    'deployment_status',
    'deployment_status_since',
    'deployment_desired_status',
    'deployment_desired_status_since',
    'deployment_resources_status',
    'deployment_resources_status_since',
    'deployment_resources_desired_status',
    'deployment_resources_desired_status_since'
  ],
  properties: {
    created_at: {
      type: 'string',
      format: 'date-time'
    },
    deployment_desired_status: {
      $ref: '#/components/schemas/CombinedDesiredStatus'
    },
    deployment_desired_status_since: {
      type: 'string',
      format: 'date-time'
    },
    deployment_error: {
      allOf: [
        {
          $ref: '#/components/schemas/ErrorResponse'
        }
      ],
      nullable: true
    },
    deployment_id: {
      type: 'string',
      format: 'uuid',
      nullable: true
    },
    deployment_initial: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeDesiredStatus'
        }
      ],
      nullable: true
    },
    deployment_resources_desired_status: {
      $ref: '#/components/schemas/ResourcesDesiredStatus'
    },
    deployment_resources_desired_status_since: {
      type: 'string',
      format: 'date-time'
    },
    deployment_resources_status: {
      $ref: '#/components/schemas/ResourcesStatus'
    },
    deployment_resources_status_since: {
      type: 'string',
      format: 'date-time'
    },
    deployment_runtime_desired_status: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeDesiredStatus'
        }
      ],
      nullable: true
    },
    deployment_runtime_desired_status_since: {
      type: 'string',
      format: 'date-time',
      nullable: true
    },
    deployment_runtime_status: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeStatus'
        }
      ],
      nullable: true
    },
    deployment_runtime_status_since: {
      type: 'string',
      format: 'date-time',
      nullable: true
    },
    deployment_status: {
      $ref: '#/components/schemas/CombinedStatus'
    },
    deployment_status_since: {
      type: 'string',
      format: 'date-time'
    },
    description: {
      type: 'string'
    },
    id: {
      $ref: '#/components/schemas/PipelineId'
    },
    name: {
      type: 'string'
    },
    platform_version: {
      type: 'string'
    },
    program_code: {
      type: 'string',
      nullable: true
    },
    program_config: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramConfig'
        }
      ],
      nullable: true
    },
    program_error: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramError'
        }
      ],
      nullable: true
    },
    program_info: {
      allOf: [
        {
          $ref: '#/components/schemas/PartialProgramInfo'
        }
      ],
      nullable: true
    },
    program_status: {
      $ref: '#/components/schemas/ProgramStatus'
    },
    program_status_since: {
      type: 'string',
      format: 'date-time'
    },
    program_version: {
      $ref: '#/components/schemas/Version'
    },
    refresh_version: {
      $ref: '#/components/schemas/Version'
    },
    runtime_config: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeConfig'
        }
      ],
      nullable: true
    },
    storage_status: {
      $ref: '#/components/schemas/StorageStatus'
    },
    udf_rust: {
      type: 'string',
      nullable: true
    },
    udf_toml: {
      type: 'string',
      nullable: true
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $PostPutPipeline = {
  type: 'object',
  description: `Create a new pipeline (POST), or fully update an existing pipeline (PUT).
Fields which are optional and not provided will be set to their empty type value
(for strings: an empty string \`""\`, for objects: an empty dictionary \`{}\`).`,
  required: ['name', 'program_code'],
  properties: {
    description: {
      type: 'string',
      nullable: true
    },
    name: {
      type: 'string'
    },
    program_code: {
      type: 'string'
    },
    program_config: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramConfig'
        }
      ],
      nullable: true
    },
    runtime_config: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeConfig'
        }
      ],
      nullable: true
    },
    udf_rust: {
      type: 'string',
      nullable: true
    },
    udf_toml: {
      type: 'string',
      nullable: true
    }
  }
} as const

export const $PostStopPipelineParameters = {
  type: 'object',
  description: 'Query parameters to POST a pipeline stop.',
  properties: {
    force: {
      type: 'boolean',
      description: `The \`force\` parameter determines whether to immediately deprovision the pipeline compute
resources (\`force=true\`) or first attempt to atomically checkpoint before doing so
(\`force=false\`, which is the default).`
    }
  }
} as const

export const $PostgresReaderConfig = {
  type: 'object',
  description: 'Postgres input connector configuration.',
  required: ['uri', 'query'],
  properties: {
    query: {
      type: 'string',
      description: 'Query that specifies what data to fetch from postgres.'
    },
    uri: {
      type: 'string',
      description: `Postgres URI.
See: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>`
    }
  }
} as const

export const $PostgresWriterConfig = {
  type: 'object',
  description: 'Postgres output connector configuration.',
  required: ['uri', 'table'],
  properties: {
    max_buffer_size_bytes: {
      type: 'integer',
      description: `The maximum buffer size in for a single operation.
Note that the buffers of \`INSERT\`, \`UPDATE\` and \`DELETE\` queries are
separate.
Default: 1 MiB`,
      default: 1048576,
      minimum: 0
    },
    max_records_in_buffer: {
      type: 'integer',
      description: 'The maximum number of records in a single buffer.',
      nullable: true,
      minimum: 0
    },
    ssl_ca_pem: {
      type: 'string',
      description: 'The CA certificate in PEM format.',
      nullable: true
    },
    ssl_client_key: {
      type: 'string',
      description: 'The client certificate key in PEM format.',
      nullable: true
    },
    ssl_client_pem: {
      type: 'string',
      description: 'The client certificate in PEM format.',
      nullable: true
    },
    table: {
      type: 'string',
      description: 'The table to write the output to.'
    },
    uri: {
      type: 'string',
      description: `Postgres URI.
See: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>`
    },
    verify_hostname: {
      type: 'boolean',
      description: 'True to enable hostname verification when using TLS. True by default.',
      nullable: true
    }
  }
} as const

export const $ProgramConfig = {
  type: 'object',
  description: 'Program configuration.',
  properties: {
    cache: {
      type: 'boolean',
      description: `If \`true\` (default), when a prior compilation with the same checksum
already exists, the output of that (i.e., binary) is used.
Set \`false\` to always trigger a new compilation, which might take longer
and as well can result in overriding an existing binary.`,
      default: true
    },
    profile: {
      allOf: [
        {
          $ref: '#/components/schemas/CompilationProfile'
        }
      ],
      default: null,
      nullable: true
    },
    runtime_version: {
      type: 'string',
      description: `Override runtime version of the pipeline being executed.

Warning: This setting is experimental and may change in the future.
Requires the platform to run with the unstable feature \`runtime_version\`
enabled. Should only be used for testing purposes, and requires
network access.

A runtime version can be specified in the form of a version
or SHA taken from the \`feldera/feldera\` repository main branch.

Examples: \`v0.96.0\` or \`f4dcac0989ca0fda7d2eb93602a49d007cb3b0ae\`

A platform of version \`0.x.y\` may be capable of running future and past
runtimes with versions \`>=0.x.y\` and \`<=0.x.y\` until breaking API changes happen,
the exact bounds for each platform version are unspecified until we reach a
stable version. Compatibility is only guaranteed if platform and runtime version
are exact matches.

Note that any enterprise features are currently considered to be part of
the platform.

If not set (null), the runtime version will be the same as the platform version.`,
      default: null,
      nullable: true
    }
  }
} as const

export const $ProgramError = {
  type: 'object',
  description: 'Log, warning and error information about the program compilation.',
  properties: {
    rust_compilation: {
      allOf: [
        {
          $ref: '#/components/schemas/RustCompilationInfo'
        }
      ],
      nullable: true
    },
    sql_compilation: {
      allOf: [
        {
          $ref: '#/components/schemas/SqlCompilationInfo'
        }
      ],
      nullable: true
    },
    system_error: {
      type: 'string',
      description: `System error that occurred.
- Set \`Some(...)\` upon transition to \`SystemError\`
- Set \`None\` upon transition to \`Pending\``,
      nullable: true
    }
  }
} as const

export const $ProgramInfo = {
  type: 'object',
  description: `Program information is the output of the SQL compiler.

It includes information needed for Rust compilation (e.g., generated Rust code)
as well as only for runtime (e.g., schema, input/output connectors).`,
  required: ['schema', 'input_connectors', 'output_connectors'],
  properties: {
    dataflow: {
      description: 'Dataflow graph of the program.'
    },
    input_connectors: {
      type: 'object',
      description: 'Input connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/InputEndpointConfig'
      }
    },
    main_rust: {
      type: 'string',
      description: 'Generated main program Rust code: main.rs'
    },
    output_connectors: {
      type: 'object',
      description: 'Output connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/OutputEndpointConfig'
      }
    },
    schema: {
      $ref: '#/components/schemas/ProgramSchema'
    },
    udf_stubs: {
      type: 'string',
      description: 'Generated user defined function (UDF) stubs Rust code: stubs.rs'
    }
  }
} as const

export const $ProgramSchema = {
  type: 'object',
  description: `A struct containing the tables (inputs) and views for a program.

Parse from the JSON data-type of the DDL generated by the SQL compiler.`,
  required: ['inputs', 'outputs'],
  properties: {
    inputs: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Relation'
      }
    },
    outputs: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Relation'
      }
    }
  }
} as const

export const $ProgramStatus = {
  type: 'string',
  description: 'Program compilation status.',
  enum: [
    'Pending',
    'CompilingSql',
    'SqlCompiled',
    'CompilingRust',
    'Success',
    'SqlError',
    'RustError',
    'SystemError'
  ]
} as const

export const $PropertyValue = {
  type: 'object',
  required: ['value', 'key_position', 'value_position'],
  properties: {
    key_position: {
      $ref: '#/components/schemas/SourcePosition'
    },
    value: {
      type: 'string'
    },
    value_position: {
      $ref: '#/components/schemas/SourcePosition'
    }
  }
} as const

export const $ProviderAwsCognito = {
  type: 'object',
  required: ['jwk_uri', 'login_url', 'logout_url'],
  properties: {
    jwk_uri: {
      type: 'string'
    },
    login_url: {
      type: 'string'
    },
    logout_url: {
      type: 'string'
    }
  }
} as const

export const $ProviderGoogleIdentity = {
  type: 'object',
  required: ['jwk_uri', 'client_id'],
  properties: {
    client_id: {
      type: 'string'
    },
    jwk_uri: {
      type: 'string'
    }
  }
} as const

export const $PubSubInputConfig = {
  type: 'object',
  description: 'Google Pub/Sub input connector configuration.',
  required: ['subscription'],
  properties: {
    connect_timeout_seconds: {
      type: 'integer',
      format: 'int32',
      description: 'gRPC connection timeout.',
      nullable: true,
      minimum: 0
    },
    credentials: {
      type: 'string',
      description: `The content of a Google Cloud credentials JSON file.

When this option is specified, the connector will use the provided credentials for
authentication.  Otherwise, it will use Application Default Credentials (ADC) configured
in the environment where the Feldera service is running.  See
[Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)
for information on configuring application default credentials.

When running Feldera in an environment where ADC are not configured,
e.g., a Docker container, use this option to ship Google Cloud credentials from another environment.
For example, if you use the
[\`gcloud auth application-default login\`](https://cloud.google.com/pubsub/docs/authentication#client-libs)
command for authentication in your local development environment, ADC are stored in the
\`.config/gcloud/application_default_credentials.json\` file in your home directory.`,
      nullable: true
    },
    emulator: {
      type: 'string',
      description: `Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)
instead of the production service, e.g., 'localhost:8681'.`,
      nullable: true
    },
    endpoint: {
      type: 'string',
      description: "Override the default service endpoint 'pubsub.googleapis.com'",
      nullable: true
    },
    pool_size: {
      type: 'integer',
      format: 'int32',
      description: 'gRPC channel pool size.',
      nullable: true,
      minimum: 0
    },
    project_id: {
      type: 'string',
      description: `Google Cloud project_id.

When not specified, the connector will use the project id associated
with the authenticated account.`,
      nullable: true
    },
    snapshot: {
      type: 'string',
      description: `Reset subscription's backlog to a given snapshot on startup,
using the Pub/Sub \`Seek\` API.

This option is mutually exclusive with the \`timestamp\` option.`,
      nullable: true
    },
    subscription: {
      type: 'string',
      description: 'Subscription name.'
    },
    timeout_seconds: {
      type: 'integer',
      format: 'int32',
      description: 'gRPC request timeout.',
      nullable: true,
      minimum: 0
    },
    timestamp: {
      type: 'string',
      description: `Reset subscription's backlog to a given timestamp on startup,
using the Pub/Sub \`Seek\` API.

The value of this option is an ISO 8601-encoded UTC time, e.g., "2024-08-17T16:39:57-08:00".

This option is mutually exclusive with the \`snapshot\` option.`,
      nullable: true
    }
  }
} as const

export const $RedisOutputConfig = {
  type: 'object',
  description: 'Redis output connector configuration.',
  required: ['connection_string'],
  properties: {
    connection_string: {
      type: 'string',
      description: `The URL format: \`redis://[<username>][:<password>@]<hostname>[:port][/[<db>][?protocol=<protocol>]]\`
This is parsed by the [redis](https://docs.rs/redis/latest/redis/#connection-parameters) crate.`
    },
    key_separator: {
      type: 'string',
      description: `Separator used to join multiple components into a single key.
":" by default.`
    }
  }
} as const

export const $Relation = {
  allOf: [
    {
      $ref: '#/components/schemas/SqlIdentifier'
    },
    {
      type: 'object',
      required: ['fields'],
      properties: {
        fields: {
          type: 'array',
          items: {
            $ref: '#/components/schemas/Field'
          }
        },
        materialized: {
          type: 'boolean'
        },
        properties: {
          type: 'object',
          additionalProperties: {
            $ref: '#/components/schemas/PropertyValue'
          }
        }
      }
    }
  ],
  description: `A SQL table or view. It has a name and a list of fields.

Matches the Calcite JSON format.`
} as const

export const $ResourceConfig = {
  type: 'object',
  properties: {
    cpu_cores_max: {
      type: 'number',
      format: 'double',
      description: `The maximum number of CPU cores to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true
    },
    cpu_cores_min: {
      type: 'number',
      format: 'double',
      description: `The minimum number of CPU cores to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true
    },
    memory_mb_max: {
      type: 'integer',
      format: 'int64',
      description: `The maximum memory in Megabytes to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    },
    memory_mb_min: {
      type: 'integer',
      format: 'int64',
      description: `The minimum memory in Megabytes to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    },
    storage_class: {
      type: 'string',
      description: `Storage class to use for an instance of this pipeline.
The class determines storage performance such as IOPS and throughput.`,
      default: null,
      nullable: true
    },
    storage_mb_max: {
      type: 'integer',
      format: 'int64',
      description: `The total storage in Megabytes to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    }
  }
} as const

export const $ResourcesDesiredStatus = {
  type: 'string',
  enum: ['Stopped', 'Provisioned']
} as const

export const $ResourcesStatus = {
  type: 'string',
  description: `Pipeline resources status.

\`\`\`text
/start (early start failed)
┌───────────────────┐
│                   ▼
Stopped ◄────────── Stopping
/start │                   ▲
│                   │ /stop?force=true
│                   │ OR: timeout (from Provisioning)
▼                   │ OR: fatal runtime or resource error
⌛Provisioning ────────────│ OR: runtime status is Suspended
│                   │
│                   │
▼                   │
Provisioned ─────────────┘
\`\`\`

### Desired and actual status

We use the desired state model to manage the lifecycle of a pipeline. In this model, the
pipeline has two status attributes associated with it: the **desired** status, which represents
what the user would like the pipeline to do, and the **current** status, which represents the
actual (last observed) status of the pipeline. The pipeline runner service continuously monitors
the desired status field to decide where to steer the pipeline towards.

There are two desired statuses:
- \`Provisioned\` (set by invoking \`/start\`)
- \`Stopped\` (set by invoking \`/stop?force=true\`)

The user can monitor the current status of the pipeline via the \`GET /v0/pipelines/{name}\`
endpoint. In a typical scenario, the user first sets the desired status, e.g., by invoking the
\`/start\` endpoint, and then polls the \`GET /v0/pipelines/{name}\` endpoint to monitor the actual
status of the pipeline until its \`deployment_resources_status\` attribute changes to
\`Provisioned\` indicating that the pipeline has been successfully provisioned, or \`Stopped\` with
\`deployment_error\` being set.`,
  enum: ['Stopped', 'Provisioning', 'Provisioned', 'Stopping']
} as const

export const $RestCatalogConfig = {
  type: 'object',
  description: 'Iceberg REST catalog config.',
  properties: {
    'rest.audience': {
      type: 'string',
      description: 'Logical name of target resource or service.',
      nullable: true
    },
    'rest.credential': {
      type: 'string',
      description: `Credential to use for OAuth2 credential flow when initializing the catalog.

A key and secret pair separated by ":" (key is optional).`,
      nullable: true
    },
    'rest.headers': {
      type: 'array',
      items: {
        type: 'array',
        items: {
          allOf: [
            {
              type: 'string'
            },
            {
              type: 'string'
            }
          ]
        }
      },
      description: 'Additional HTTP request headers added to each catalog REST API call.',
      nullable: true
    },
    'rest.oauth2-server-uri': {
      type: 'string',
      description:
        "Authentication URL to use for client credentials authentication (default: uri + 'v1/oauth/tokens')",
      nullable: true
    },
    'rest.prefix': {
      type: 'string',
      description: `Customize table storage paths.

When combined with the \`warehouse\` property, the prefix determines
how table data is organized within the storage.`,
      nullable: true
    },
    'rest.resource': {
      type: 'string',
      description: 'URI for the target resource or service.',
      nullable: true
    },
    'rest.scope': {
      type: 'string',
      nullable: true
    },
    'rest.token': {
      type: 'string',
      description: 'Bearer token value to use for `Authorization` header.',
      nullable: true
    },
    'rest.uri': {
      type: 'string',
      description: 'URI identifying the REST catalog server.',
      nullable: true
    },
    'rest.warehouse': {
      type: 'string',
      description: 'The default location for managed tables created by the catalog.',
      nullable: true
    }
  }
} as const

export const $RngFieldSettings = {
  type: 'object',
  description: 'Configuration for generating random data for a field of a table.',
  properties: {
    e: {
      type: 'integer',
      format: 'int64',
      description: `The frequency rank exponent for the Zipf distribution.

- This value is only used if the strategy is set to \`Zipf\`.
- The default value is 1.0.`,
      default: 1
    },
    fields: {
      type: 'object',
      description:
        'Specifies the values that the generator should produce in case the field is a struct type.',
      default: null,
      additionalProperties: {
        $ref: '#/components/schemas/RngFieldSettings'
      },
      nullable: true
    },
    key: {
      allOf: [
        {
          $ref: '#/components/schemas/RngFieldSettings'
        }
      ],
      default: null,
      nullable: true
    },
    null_percentage: {
      type: 'integer',
      description: `Percentage of records where this field should be set to NULL.

If not set, the generator will produce only records with non-NULL values.
If set to \`1..=100\`, the generator will produce records with NULL values with the specified percentage.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    range: {
      type: 'object',
      description: `An optional, exclusive range [a, b) to limit the range of values the generator should produce.

- For integer/floating point types specifies min/max values as an integer.
If not set, the generator will produce values for the entire range of the type for number types.
- For string/binary types specifies min/max length as an integer, values are required to be >=0.
If not set, a range of [0, 25) is used by default.
- For timestamp types specifies the min/max as two strings in the RFC 3339 format
(e.g., ["2021-01-01T00:00:00Z", "2022-01-02T00:00:00Z"]).
Alternatively, the range values can be specified as a number of non-leap
milliseconds since January 1, 1970 0:00:00.000 UTC (aka “UNIX timestamp”).
If not set, a range of ["1970-01-01T00:00:00Z", "2100-01-01T00:00:00Z") or [0, 4102444800000)
is used by default.
- For time types specifies the min/max as two strings in the "HH:MM:SS" format.
Alternatively, the range values can be specified in milliseconds as two positive integers.
If not set, the range is 24h.
- For date types, the min/max range is specified as two strings in the "YYYY-MM-DD" format.
Alternatively, two integers that represent number of days since January 1, 1970 can be used.
If not set, a range of ["1970-01-01", "2100-01-01") or [0, 54787) is used by default.
- For array types specifies the min/max number of elements as an integer.
If not set, a range of [0, 5) is used by default. Range values are required to be >=0.
- For map types specifies the min/max number of key-value pairs as an integer.
If not set, a range of [0, 5) is used by default.
- For struct/boolean/null types \`range\` is ignored.`
    },
    scale: {
      type: 'integer',
      format: 'int64',
      description: `A scale factor to apply a multiplier to the generated value.

- For integer/floating point types, the value is multiplied by the scale factor.
- For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.
- For time types, the generated value (milliseconds) is multiplied by the scale factor.
- For date types, the generated value (days) is multiplied by the scale factor.
- For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.

- If \`values\` is specified, the scale factor is ignored.
- If \`range\` is specified and the range is required to be positive (struct, map, array etc.)
the scale factor is required to be positive too.

The default scale factor is 1.`,
      default: 1
    },
    strategy: {
      allOf: [
        {
          $ref: '#/components/schemas/DatagenStrategy'
        }
      ],
      default: 'increment'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/RngFieldSettings'
        }
      ],
      default: null,
      nullable: true
    },
    values: {
      type: 'array',
      items: {
        type: 'object'
      },
      description: `An optional set of values the generator will pick from.

If set, the generator will pick values from the specified set.
If not set, the generator will produce values according to the specified range.
If set to an empty set, the generator will produce NULL values.
If set to a single value, the generator will produce only that value.

Note that \`range\` is ignored if \`values\` is set.`,
      default: null,
      nullable: true
    }
  },
  additionalProperties: false
} as const

export const $RuntimeConfig = {
  type: 'object',
  description: `Global pipeline configuration settings. This is the publicly
exposed type for users to configure pipelines.`,
  properties: {
    checkpoint_during_suspend: {
      type: 'boolean',
      description: 'Deprecated: setting this true or false does not have an effect anymore.',
      default: true
    },
    clock_resolution_usecs: {
      type: 'integer',
      format: 'int64',
      description: `Real-time clock resolution in microseconds.

This parameter controls the execution of queries that use the \`NOW()\` function.  The output of such
queries depends on the real-time clock and can change over time without any external
inputs.  If the query uses \`NOW()\`, the pipeline will update the clock value and trigger incremental
recomputation at most each \`clock_resolution_usecs\` microseconds.  If the query does not use
\`NOW()\`, then clock value updates are suppressed and the pipeline ignores this setting.

It is set to 1 second (1,000,000 microseconds) by default.`,
      default: 1000000,
      nullable: true,
      minimum: 0
    },
    cpu_profiler: {
      type: 'boolean',
      description: `Enable CPU profiler.

The default value is \`true\`.`,
      default: true
    },
    dev_tweaks: {
      type: 'object',
      description: `Optional settings for tweaking Feldera internals.

The available key-value pairs change from one version of Feldera to
another, so users should not depend on particular settings being
available, or on their behavior.`,
      default: {},
      additionalProperties: {}
    },
    fault_tolerance: {
      allOf: [
        {
          $ref: '#/components/schemas/FtConfig'
        }
      ],
      default: {
        model: 'none',
        checkpoint_interval_secs: 60
      }
    },
    http_workers: {
      type: 'integer',
      format: 'int64',
      description: `Sets the number of available runtime threads for the http server.

In most cases, this does not need to be set explicitly and
the default is sufficient. Can be increased in case the
pipeline HTTP API operations are a bottleneck.

If not specified, the default is set to \`workers\`.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    init_containers: {
      description: 'Specification of additional (sidecar) containers.',
      nullable: true
    },
    io_workers: {
      type: 'integer',
      format: 'int64',
      description: `Sets the number of available runtime threads for async IO tasks.

This affects some networking and file I/O operations
especially adapters and ad-hoc queries.

In most cases, this does not need to be set explicitly and
the default is sufficient. Can be increased in case
ingress, egress or ad-hoc queries are a bottleneck.

If not specified, the default is set to \`workers\`.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    logging: {
      type: 'string',
      description: `Log filtering directives.

If set to a valid [tracing-subscriber] filter, this controls the log
messages emitted by the pipeline process.  Otherwise, or if the filter
has invalid syntax, messages at "info" severity and higher are written
to the log and all others are discarded.

[tracing-subscriber]: https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#directives`,
      default: null,
      nullable: true
    },
    max_buffering_delay_usecs: {
      type: 'integer',
      format: 'int64',
      description: `Maximal delay in microseconds to wait for \`min_batch_size_records\` to
get buffered by the controller, defaults to 0.`,
      default: 0,
      minimum: 0
    },
    max_parallel_connector_init: {
      type: 'integer',
      format: 'int64',
      description: `The maximum number of connectors initialized in parallel during pipeline
startup.

At startup, the pipeline must initialize all of its input and output connectors.
Depending on the number and types of connectors, this can take a long time.
To accelerate the process, multiple connectors are initialized concurrently.
This option controls the maximum number of connectors that can be initialized
in parallel.

The default is 10.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    min_batch_size_records: {
      type: 'integer',
      format: 'int64',
      description: `Minimal input batch size.

The controller delays pushing input records to the circuit until at
least \`min_batch_size_records\` records have been received (total
across all endpoints) or \`max_buffering_delay_usecs\` microseconds
have passed since at least one input records has been buffered.
Defaults to 0.`,
      default: 0,
      minimum: 0
    },
    pin_cpus: {
      type: 'array',
      items: {
        type: 'integer',
        minimum: 0
      },
      description: `Optionally, a list of CPU numbers for CPUs to which the pipeline may pin
its worker threads.  Specify at least twice as many CPU numbers as
workers.  CPUs are generally numbered starting from 0.  The pipeline
might not be able to honor CPU pinning requests.

CPU pinning can make pipelines run faster and perform more consistently,
as long as different pipelines running on the same machine are pinned to
different CPUs.`,
      default: []
    },
    provisioning_timeout_secs: {
      type: 'integer',
      format: 'int64',
      description: `Timeout in seconds for the \`Provisioning\` phase of the pipeline.
Setting this value will override the default of the runner.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    resources: {
      allOf: [
        {
          $ref: '#/components/schemas/ResourceConfig'
        }
      ],
      default: {
        cpu_cores_min: null,
        cpu_cores_max: null,
        memory_mb_min: null,
        memory_mb_max: null,
        storage_mb_max: null,
        storage_class: null
      }
    },
    storage: {
      allOf: [
        {
          $ref: '#/components/schemas/StorageOptions'
        }
      ],
      default: {
        backend: {
          name: 'default'
        },
        min_storage_bytes: null,
        min_step_storage_bytes: null,
        compression: 'default',
        cache_mib: null
      },
      nullable: true
    },
    tracing: {
      type: 'boolean',
      description: 'Enable pipeline tracing.',
      default: false
    },
    tracing_endpoint_jaeger: {
      type: 'string',
      description: 'Jaeger tracing endpoint to send tracing information to.',
      default: '127.0.0.1:6831'
    },
    workers: {
      type: 'integer',
      format: 'int32',
      description: `Number of DBSP worker threads.

Each DBSP "foreground" worker thread is paired with a "background"
thread for LSM merging, making the total number of threads twice the
specified number.

The typical sweet spot for the number of workers is between 4 and 16.
Each worker increases overall memory consumption for data structures
used during a step.`,
      default: 8,
      minimum: 0
    }
  }
} as const

export const $RuntimeDesiredStatus = {
  type: 'string',
  enum: ['Unavailable', 'Standby', 'Paused', 'Running', 'Suspended']
} as const

export const $RuntimeStatus = {
  type: 'string',
  description: `Runtime status of the pipeline.

Of the statuses, only \`Unavailable\` is determined by the runner. All other statuses are
determined by the pipeline and taken over by the runner.`,
  enum: [
    'Unavailable',
    'Standby',
    'Initializing',
    'Bootstrapping',
    'Replaying',
    'Paused',
    'Running',
    'Suspended'
  ]
} as const

export const $RustCompilationInfo = {
  type: 'object',
  description: 'Rust compilation information.',
  required: ['exit_code', 'stdout', 'stderr'],
  properties: {
    exit_code: {
      type: 'integer',
      format: 'int32',
      description: 'Exit code of the `cargo` compilation command.'
    },
    stderr: {
      type: 'string',
      description: 'Output printed to stderr by the `cargo` compilation command.'
    },
    stdout: {
      type: 'string',
      description: 'Output printed to stdout by the `cargo` compilation command.'
    }
  }
} as const

export const $S3InputConfig = {
  type: 'object',
  description: 'Configuration for reading data from AWS S3.',
  required: ['region', 'bucket_name'],
  properties: {
    aws_access_key_id: {
      type: 'string',
      description:
        'AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.',
      nullable: true
    },
    aws_secret_access_key: {
      type: 'string',
      description:
        'Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.',
      nullable: true
    },
    bucket_name: {
      type: 'string',
      description: 'S3 bucket name to access.'
    },
    endpoint_url: {
      type: 'string',
      description: `The endpoint URL used to communicate with this service. Can be used to make this connector
talk to non-AWS services with an S3 API.`,
      nullable: true
    },
    key: {
      type: 'string',
      description: 'Read a single object specified by a key.',
      nullable: true
    },
    max_concurrent_fetches: {
      type: 'integer',
      format: 'int32',
      description: `Controls the number of S3 objects fetched in parallel.

Increasing this value can improve throughput by enabling greater concurrency.
However, higher concurrency may lead to timeouts or increased memory usage due to in-memory buffering.

Recommended range: 1–10. Default: 8.`,
      minimum: 0
    },
    no_sign_request: {
      type: 'boolean',
      description:
        'Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI.'
    },
    prefix: {
      type: 'string',
      description:
        'Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.',
      nullable: true
    },
    region: {
      type: 'string',
      description: 'AWS region.'
    }
  }
} as const

export const $SampleStatistics = {
  type: 'object',
  description: 'One sample of time-series data.',
  required: ['t', 'r', 'm', 's'],
  properties: {
    m: {
      type: 'integer',
      format: 'int64',
      description: 'Memory usage in bytes.',
      minimum: 0
    },
    r: {
      type: 'integer',
      format: 'int64',
      description: 'Records processed.',
      minimum: 0
    },
    s: {
      type: 'integer',
      format: 'int64',
      description: 'Storage usage in bytes.',
      minimum: 0
    },
    t: {
      type: 'string',
      format: 'date-time',
      description: 'Sample time.'
    }
  }
} as const

export const $ServiceStatus = {
  type: 'object',
  required: ['healthy', 'message', 'unchanged_since', 'checked_at'],
  properties: {
    checked_at: {
      type: 'string',
      format: 'date-time'
    },
    healthy: {
      type: 'boolean'
    },
    message: {
      type: 'string'
    },
    unchanged_since: {
      type: 'string',
      format: 'date-time'
    }
  }
} as const

export const $SourcePosition = {
  type: 'object',
  required: ['start_line_number', 'start_column', 'end_line_number', 'end_column'],
  properties: {
    end_column: {
      type: 'integer',
      minimum: 0
    },
    end_line_number: {
      type: 'integer',
      minimum: 0
    },
    start_column: {
      type: 'integer',
      minimum: 0
    },
    start_line_number: {
      type: 'integer',
      minimum: 0
    }
  }
} as const

export const $SqlCompilationInfo = {
  type: 'object',
  description: 'SQL compilation information.',
  required: ['exit_code', 'messages'],
  properties: {
    exit_code: {
      type: 'integer',
      format: 'int32',
      description: 'Exit code of the SQL compiler.'
    },
    messages: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/SqlCompilerMessage'
      },
      description: 'Messages (warnings and errors) generated by the SQL compiler.'
    }
  }
} as const

export const $SqlCompilerMessage = {
  type: 'object',
  description: `A SQL compiler error.

The SQL compiler returns a list of errors in the following JSON format if
it's invoked with the \`-je\` option.

\`\`\`ignore
[ {
"start_line_number" : 2,
"start_column" : 4,
"end_line_number" : 2,
"end_column" : 8,
"warning" : false,
"error_type" : "PRIMARY KEY cannot be nullable",
"message" : "PRIMARY KEY column 'C' has type INTEGER, which is nullable",
"snippet" : "    2|   c INT PRIMARY KEY\n         ^^^^^\n    3|);\n"
} ]
\`\`\``,
  required: [
    'start_line_number',
    'start_column',
    'end_line_number',
    'end_column',
    'warning',
    'error_type',
    'message'
  ],
  properties: {
    end_column: {
      type: 'integer',
      minimum: 0
    },
    end_line_number: {
      type: 'integer',
      minimum: 0
    },
    error_type: {
      type: 'string'
    },
    message: {
      type: 'string'
    },
    snippet: {
      type: 'string',
      nullable: true
    },
    start_column: {
      type: 'integer',
      minimum: 0
    },
    start_line_number: {
      type: 'integer',
      minimum: 0
    },
    warning: {
      type: 'boolean'
    }
  }
} as const

export const $SqlIdentifier = {
  type: 'object',
  description: `An SQL identifier.

This struct is used to represent SQL identifiers in a canonical form.
We store table names or field names as identifiers in the schema.`,
  required: ['name', 'case_sensitive'],
  properties: {
    case_sensitive: {
      type: 'boolean'
    },
    name: {
      type: 'string'
    }
  }
} as const

export const $SqlType = {
  oneOf: [
    {
      type: 'string',
      description: 'SQL `BOOLEAN` type.',
      enum: ['Boolean']
    },
    {
      type: 'string',
      description: 'SQL `TINYINT` type.',
      enum: ['TinyInt']
    },
    {
      type: 'string',
      description: 'SQL `SMALLINT` or `INT2` type.',
      enum: ['SmallInt']
    },
    {
      type: 'string',
      description: 'SQL `INTEGER`, `INT`, `SIGNED`, `INT4` type.',
      enum: ['Int']
    },
    {
      type: 'string',
      description: 'SQL `BIGINT` or `INT64` type.',
      enum: ['BigInt']
    },
    {
      type: 'string',
      description: 'SQL `TINYINT UNSIGNED` type.',
      enum: ['UTinyInt']
    },
    {
      type: 'string',
      description: 'SQL `SMALLINT UNSIGNED` type.',
      enum: ['USmallInt']
    },
    {
      type: 'string',
      description: 'SQL `UNSIGNED`, `INTEGER UNSIGNED`, `INT UNSIGNED` type.',
      enum: ['UInt']
    },
    {
      type: 'string',
      description: 'SQL `BIGINT UNSIGNED` type.',
      enum: ['UBigInt']
    },
    {
      type: 'string',
      description: 'SQL `REAL` or `FLOAT4` or `FLOAT32` type.',
      enum: ['Real']
    },
    {
      type: 'string',
      description: 'SQL `DOUBLE` or `FLOAT8` or `FLOAT64` type.',
      enum: ['Double']
    },
    {
      type: 'string',
      description: 'SQL `DECIMAL` or `DEC` or `NUMERIC` type.',
      enum: ['Decimal']
    },
    {
      type: 'string',
      description: 'SQL `CHAR(n)` or `CHARACTER(n)` type.',
      enum: ['Char']
    },
    {
      type: 'string',
      description: 'SQL `VARCHAR`, `CHARACTER VARYING`, `TEXT`, or `STRING` type.',
      enum: ['Varchar']
    },
    {
      type: 'string',
      description: 'SQL `BINARY(n)` type.',
      enum: ['Binary']
    },
    {
      type: 'string',
      description: 'SQL `VARBINARY` or `BYTEA` type.',
      enum: ['Varbinary']
    },
    {
      type: 'string',
      description: 'SQL `TIME` type.',
      enum: ['Time']
    },
    {
      type: 'string',
      description: 'SQL `DATE` type.',
      enum: ['Date']
    },
    {
      type: 'string',
      description: 'SQL `TIMESTAMP` type.',
      enum: ['Timestamp']
    },
    {
      type: 'object',
      required: ['Interval'],
      properties: {
        Interval: {
          $ref: '#/components/schemas/IntervalUnit'
        }
      }
    },
    {
      type: 'string',
      description: 'SQL `ARRAY` type.',
      enum: ['Array']
    },
    {
      type: 'string',
      description: 'A complex SQL struct type (`CREATE TYPE x ...`).',
      enum: ['Struct']
    },
    {
      type: 'string',
      description: 'SQL `MAP` type.',
      enum: ['Map']
    },
    {
      type: 'string',
      description: 'SQL `NULL` type.',
      enum: ['Null']
    },
    {
      type: 'string',
      description: 'SQL `UUID` type.',
      enum: ['Uuid']
    },
    {
      type: 'string',
      description: 'SQL `VARIANT` type.',
      enum: ['Variant']
    }
  ],
  description: 'The available SQL types as specified in `CREATE` statements.'
} as const

export const $StartFromCheckpoint = {
  oneOf: [
    {
      type: 'string',
      enum: ['latest']
    },
    {
      type: 'string',
      format: 'uuid'
    }
  ],
  nullable: true
} as const

export const $StartTransactionResponse = {
  type: 'object',
  description: 'Response to a `/start_transaction` request.',
  required: ['transaction_id'],
  properties: {
    transaction_id: {
      type: 'integer',
      format: 'int64'
    }
  }
} as const

export const $StorageBackendConfig = {
  oneOf: [
    {
      type: 'object',
      required: ['name'],
      properties: {
        name: {
          type: 'string',
          enum: ['default']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileBackendConfig'
        },
        name: {
          type: 'string',
          enum: ['file']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/ObjectStorageConfig'
        },
        name: {
          type: 'string',
          enum: ['object']
        }
      }
    }
  ],
  description: 'Backend storage configuration.',
  discriminator: {
    propertyName: 'name'
  }
} as const

export const $StorageCacheConfig = {
  type: 'string',
  description: 'How to cache access to storage within a Feldera pipeline.',
  enum: ['page_cache', 'feldera_cache']
} as const

export const $StorageCompression = {
  type: 'string',
  description: 'Storage compression algorithm.',
  enum: ['default', 'none', 'snappy']
} as const

export const $StorageConfig = {
  type: 'object',
  description: 'Configuration for persistent storage in a [`PipelineConfig`].',
  required: ['path'],
  properties: {
    cache: {
      $ref: '#/components/schemas/StorageCacheConfig'
    },
    path: {
      type: 'string',
      description: `A directory to keep pipeline state, as a path on the filesystem of the
machine or container where the pipeline will run.

When storage is enabled, this directory stores the data for
[StorageBackendConfig::Default].

When fault tolerance is enabled, this directory stores checkpoints and
the log.`
    }
  }
} as const

export const $StorageOptions = {
  type: 'object',
  description: 'Storage configuration for a pipeline.',
  properties: {
    backend: {
      allOf: [
        {
          $ref: '#/components/schemas/StorageBackendConfig'
        }
      ],
      default: {
        name: 'default'
      }
    },
    cache_mib: {
      type: 'integer',
      description: `The maximum size of the in-memory storage cache, in MiB.

If set, the specified cache size is spread across all the foreground and
background threads. If unset, each foreground or background thread cache
is limited to 256 MiB.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    compression: {
      allOf: [
        {
          $ref: '#/components/schemas/StorageCompression'
        }
      ],
      default: 'default'
    },
    min_step_storage_bytes: {
      type: 'integer',
      description: `For a batch of data passed through the pipeline during a single step,
the minimum estimated number of bytes to write it to storage.

This is provided for debugging and fine-tuning and should ordinarily be
left unset.  A value of 0 will write even empty batches to storage, and
nonzero values provide a threshold.  \`usize::MAX\`, the default,
effectively disables storage for such batches.  If it is set to another
value, it should ordinarily be greater than or equal to
\`min_storage_bytes\`.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    min_storage_bytes: {
      type: 'integer',
      description: `For a batch of data maintained as part of a persistent index during a
pipeline run, the minimum estimated number of bytes to write it to
storage.

This is provided for debugging and fine-tuning and should ordinarily be
left unset.

A value of 0 will write even empty batches to storage, and nonzero
values provide a threshold.  \`usize::MAX\` would effectively disable
storage for such batches.  The default is 1,048,576 (1 MiB).`,
      default: null,
      nullable: true,
      minimum: 0
    }
  }
} as const

export const $StorageStatus = {
  type: 'string',
  description: `Storage status.

The storage status can only transition when the resources status is \`Stopped\`.

\`\`\`text
Cleared ───┐
▲       │
/clear │       │
│       │
Clearing   │
▲       │
│       │
InUse ◄───┘
\`\`\``,
  enum: ['Cleared', 'InUse', 'Clearing']
} as const

export const $SyncConfig = {
  type: 'object',
  required: ['bucket'],
  properties: {
    access_key: {
      type: 'string',
      description: `The access key used to authenticate with the storage provider.

If not provided, rclone will fall back to environment-based credentials, such as
\`RCLONE_S3_ACCESS_KEY_ID\`. In Kubernetes environments using IRSA (IAM Roles for Service Accounts),
this can be left empty to allow automatic authentication via the pod's service account.`,
      nullable: true
    },
    bucket: {
      type: 'string',
      description: `The name of the storage bucket.

This may include a path to a folder inside the bucket (e.g., \`my-bucket/data\`).`
    },
    checkers: {
      type: 'integer',
      format: 'int32',
      description: `The number of checkers to run in parallel.
Default: 20`,
      nullable: true,
      minimum: 0
    },
    endpoint: {
      type: 'string',
      description: `The endpoint URL for the storage service.

This is typically required for custom or local S3-compatible storage providers like MinIO.
Example: \`http://localhost:9000\`

Relevant rclone config key: [\`endpoint\`](https://rclone.org/s3/#s3-endpoint)`,
      nullable: true
    },
    fail_if_no_checkpoint: {
      type: 'boolean',
      description: `When true, the pipeline will fail to initialize if fetching the
specified checkpoint fails (missing, download error).
When false, the pipeline will start from scratch instead.

False by default.`,
      default: false
    },
    flags: {
      type: 'array',
      items: {
        type: 'string'
      },
      description: `Extra flags to pass to \`rclone\`.

WARNING: Supplying incorrect or conflicting flags can break \`rclone\`.
Use with caution.

Refer to the docs to see the supported flags:
- [Global flags](https://rclone.org/flags/)
- [S3 specific flags](https://rclone.org/s3/)`,
      nullable: true
    },
    ignore_checksum: {
      type: 'boolean',
      description: `Set to skip post copy check of checksums, and only check the file sizes.
This can significantly improve the throughput.
Defualt: false`,
      nullable: true
    },
    multi_thread_cutoff: {
      type: 'string',
      description: `Use multi-thread download for files above this size.
Format: \`[size][Suffix]\` (Example: 1G, 500M)
Supported suffixes: k|M|G|T
Default: 100M`,
      nullable: true
    },
    multi_thread_streams: {
      type: 'integer',
      format: 'int32',
      description: `Number of streams to use for multi-thread downloads.
Default: 10`,
      nullable: true,
      minimum: 0
    },
    provider: {
      type: 'string',
      description: `The name of the cloud storage provider (e.g., \`"AWS"\`, \`"Minio"\`).

Used for provider-specific behavior in rclone.
If omitted, defaults to \`"Other"\`.

See [rclone S3 provider documentation](https://rclone.org/s3/#s3-provider)`,
      nullable: true
    },
    pull_interval: {
      type: 'integer',
      format: 'int64',
      description: `The interval (in seconds) between each attempt to fetch the latest
checkpoint from object store while in standby mode.

Applies only when \`start_from_checkpoint\` is set to \`latest\`.

Default: 10 seconds`,
      default: 10,
      minimum: 0
    },
    region: {
      type: 'string',
      description: `The region that this bucket is in.

Leave empty for Minio or the default region (\`us-east-1\` for AWS).`,
      nullable: true
    },
    retention_min_age: {
      type: 'integer',
      format: 'int32',
      description: `The minimum age (in days) a checkpoint must reach before it becomes
eligible for deletion. All younger checkpoints will be preserved.

Default: 30`,
      default: 30,
      minimum: 0
    },
    retention_min_count: {
      type: 'integer',
      format: 'int32',
      description: `The minimum number of checkpoints to retain in object store.
No checkpoints will be deleted if the total count is below this threshold.

Default: 10`,
      default: 10,
      minimum: 0
    },
    secret_key: {
      type: 'string',
      description: `The secret key used together with the access key for authentication.

If not provided, rclone will fall back to environment-based credentials, such as
\`RCLONE_S3_SECRET_ACCESS_KEY\`. In Kubernetes environments using IRSA (IAM Roles for Service Accounts),
this can be left empty to allow automatic authentication via the pod's service account.`,
      nullable: true
    },
    standby: {
      type: 'boolean',
      description: `When \`true\`, the pipeline starts in **standby** mode; processing doesn't
start until activation (\`POST /activate\`).
If this pipeline was previously activated and the storage has not been
cleared, the pipeline will auto activate, no newer checkpoints will be
fetched.

Standby behavior depends on \`start_from_checkpoint\`:
- If \`latest\`, pipeline continuously fetches the latest available
checkpoint until activated.
- If checkpoint UUID, pipeline fetches this checkpoint once and waits
in standby until activated.

Default: \`false\``,
      default: false
    },
    start_from_checkpoint: {
      allOf: [
        {
          $ref: '#/components/schemas/StartFromCheckpoint'
        }
      ],
      nullable: true
    },
    transfers: {
      type: 'integer',
      format: 'int32',
      description: `The number of file transfers to run in parallel.
Default: 20`,
      nullable: true,
      minimum: 0
    },
    upload_concurrency: {
      type: 'integer',
      format: 'int32',
      description: `The number of chunks of the same file that are uploaded for multipart uploads.
Default: 10`,
      nullable: true,
      minimum: 0
    }
  }
} as const

export const $TimeSeries = {
  type: 'object',
  description: 'Time series to make graphs in the web console easier.',
  required: ['now', 'samples'],
  properties: {
    now: {
      type: 'string',
      format: 'date-time',
      description: 'Current time as of the creation of the structure.'
    },
    samples: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/SampleStatistics'
      },
      description: `Time series.

These report 60 seconds of samples, one per second.`
    }
  }
} as const

export const $TransportConfig = {
  oneOf: [
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileInputConfig'
        },
        name: {
          type: 'string',
          enum: ['file_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['file_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/KafkaInputConfig'
        },
        name: {
          type: 'string',
          enum: ['kafka_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/KafkaOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['kafka_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/PubSubInputConfig'
        },
        name: {
          type: 'string',
          enum: ['pub_sub_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/UrlInputConfig'
        },
        name: {
          type: 'string',
          enum: ['url_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/S3InputConfig'
        },
        name: {
          type: 'string',
          enum: ['s3_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DeltaTableReaderConfig'
        },
        name: {
          type: 'string',
          enum: ['delta_table_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DeltaTableWriterConfig'
        },
        name: {
          type: 'string',
          enum: ['delta_table_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/RedisOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['redis_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/IcebergReaderConfig'
        },
        name: {
          type: 'string',
          enum: ['iceberg_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/PostgresReaderConfig'
        },
        name: {
          type: 'string',
          enum: ['postgres_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/PostgresWriterConfig'
        },
        name: {
          type: 'string',
          enum: ['postgres_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DatagenInputConfig'
        },
        name: {
          type: 'string',
          enum: ['datagen']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/NexmarkInputConfig'
        },
        name: {
          type: 'string',
          enum: ['nexmark']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/HttpInputConfig'
        },
        name: {
          type: 'string',
          enum: ['http_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name'],
      properties: {
        name: {
          type: 'string',
          enum: ['http_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/AdHocInputConfig'
        },
        name: {
          type: 'string',
          enum: ['ad_hoc_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/ClockConfig'
        },
        name: {
          type: 'string',
          enum: ['clock_input']
        }
      }
    }
  ],
  description: `Transport-specific endpoint configuration passed to
\`crate::OutputTransport::new_endpoint\`
and \`crate::InputTransport::new_endpoint\`.`,
  discriminator: {
    propertyName: 'name'
  }
} as const

export const $UpdateInformation = {
  type: 'object',
  required: ['latest_version', 'is_latest_version', 'instructions_url', 'remind_schedule'],
  properties: {
    instructions_url: {
      type: 'string',
      description:
        "URL that navigates the user to instructions on how to update their deployment's version"
    },
    is_latest_version: {
      type: 'boolean',
      description: 'Whether the current version matches the latest version'
    },
    latest_version: {
      type: 'string',
      description: 'Latest version corresponding to the edition'
    },
    remind_schedule: {
      $ref: '#/components/schemas/DisplaySchedule'
    }
  }
} as const

export const $UrlInputConfig = {
  type: 'object',
  description: `Configuration for reading data from an HTTP or HTTPS URL with
\`UrlInputTransport\`.`,
  required: ['path'],
  properties: {
    path: {
      type: 'string',
      description: 'URL.'
    },
    pause_timeout: {
      type: 'integer',
      format: 'int32',
      description: `Timeout before disconnection when paused, in seconds.

If the pipeline is paused, or if the input adapter reads data faster
than the pipeline can process it, then the controller will pause the
input adapter. If the input adapter stays paused longer than this
timeout, it will drop the network connection to the server. It will
automatically reconnect when the input adapter starts running again.`,
      minimum: 0
    }
  }
} as const

export const $Version = {
  type: 'integer',
  format: 'int64',
  description: 'Version number.'
} as const
