// This file is auto-generated by @hey-api/openapi-ts

export const $AdHocResultFormat = {
  type: 'string',
  description: 'URL-encoded `format` argument to the `/query` endpoint.',
  enum: ['text', 'json', 'parquet']
} as const

export const $AdhocQueryArgs = {
  type: 'object',
  description: 'URL-encoded arguments to the `/query` endpoint.',
  required: ['sql'],
  properties: {
    format: {
      $ref: '#/components/schemas/AdHocResultFormat'
    },
    sql: {
      type: 'string',
      description: 'The SQL query to run.'
    }
  }
} as const

export const $ApiKeyDescr = {
  type: 'object',
  description: 'API key descriptor.',
  required: ['id', 'name', 'scopes'],
  properties: {
    id: {
      $ref: '#/components/schemas/ApiKeyId'
    },
    name: {
      type: 'string'
    },
    scopes: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/ApiPermission'
      }
    }
  }
} as const

export const $ApiKeyId = {
  type: 'string',
  format: 'uuid',
  description: 'API key identifier.'
} as const

export const $ApiPermission = {
  type: 'string',
  description: 'Permission types for invoking API endpoints.',
  enum: ['Read', 'Write']
} as const

export const $AuthProvider = {
  oneOf: [
    {
      type: 'object',
      required: ['AwsCognito'],
      properties: {
        AwsCognito: {
          $ref: '#/components/schemas/ProviderAwsCognito'
        }
      }
    },
    {
      type: 'object',
      required: ['GoogleIdentity'],
      properties: {
        GoogleIdentity: {
          $ref: '#/components/schemas/ProviderGoogleIdentity'
        }
      }
    }
  ]
} as const

export const $Chunk = {
  type: 'object',
  description: `A set of updates to a SQL table or view.

The \`sequence_number\` field stores the offset of the chunk relative to the
start of the stream and can be used to implement reliable delivery.
The payload is stored in the \`bin_data\`, \`text_data\`, or \`json_data\` field
depending on the data format used.`,
  required: ['sequence_number'],
  properties: {
    bin_data: {
      type: 'string',
      format: 'binary',
      description: 'Base64 encoded binary payload, e.g., bincode.',
      nullable: true
    },
    json_data: {
      type: 'object',
      description: 'JSON payload.',
      nullable: true
    },
    sequence_number: {
      type: 'integer',
      format: 'int64',
      minimum: 0
    },
    text_data: {
      type: 'string',
      description: 'Text payload, e.g., CSV.',
      nullable: true
    }
  }
} as const

export const $ColumnType = {
  type: 'object',
  description: `A SQL column type description.

Matches the Calcite JSON format.`,
  required: ['nullable'],
  properties: {
    component: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    },
    fields: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Field'
      },
      description: `The fields of the type (if available).

For example this would specify the fields of a \`CREATE TYPE\` construct.

\`\`\`sql
CREATE TYPE person_typ AS (
firstname       VARCHAR(30),
lastname        VARCHAR(30),
address         ADDRESS_TYP
);
\`\`\`

Would lead to the following \`fields\` value:

\`\`\`sql
[
ColumnType { name: "firstname, ... },
ColumnType { name: "lastname", ... },
ColumnType { name: "address", fields: [ ... ] }
]
\`\`\``,
      nullable: true
    },
    key: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    },
    nullable: {
      type: 'boolean',
      description: 'Does the type accept NULL values?'
    },
    precision: {
      type: 'integer',
      format: 'int64',
      description: `Precision of the type.

# Examples
- \`VARCHAR\` sets precision to \`-1\`.
- \`VARCHAR(255)\` sets precision to \`255\`.
- \`BIGINT\`, \`DATE\`, \`FLOAT\`, \`DOUBLE\`, \`GEOMETRY\`, etc. sets precision
to None
- \`TIME\`, \`TIMESTAMP\` set precision to \`0\`.`,
      nullable: true
    },
    scale: {
      type: 'integer',
      format: 'int64',
      description: `The scale of the type.

# Example
- \`DECIMAL(1,2)\` sets scale to \`2\`.`,
      nullable: true
    },
    type: {
      $ref: '#/components/schemas/SqlType'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    }
  }
} as const

export const $CompilationProfile = {
  type: 'string',
  description: `Enumeration of possible compilation profiles that can be passed to the Rust compiler
as an argument via \`cargo build --profile <>\`. A compilation profile affects among
other things the compilation speed (how long till the program is ready to be run)
and runtime speed (the performance while running).`,
  enum: ['dev', 'unoptimized', 'optimized']
} as const

export const $Configuration = {
  type: 'object',
  required: ['telemetry', 'version'],
  properties: {
    telemetry: {
      type: 'string'
    },
    version: {
      type: 'string'
    }
  }
} as const

export const $ConnectorConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/OutputBufferConfig'
    },
    {
      type: 'object',
      required: ['transport'],
      properties: {
        format: {
          allOf: [
            {
              $ref: '#/components/schemas/FormatConfig'
            }
          ],
          nullable: true
        },
        max_batch_size: {
          type: 'integer',
          format: 'int64',
          description: `Maximum batch size, in records.

This is the maximum number of records to process in one batch through
the circuit.  The time and space cost of processing a batch is
asymptotically superlinear in the size of the batch, but very small
batches are less efficient due to constant factors.

This should usually be less than \`max_queued_records\`, to give the
connector a round-trip time to restart and refill the buffer while
batches are being processed.

Some input adapters might not honor this setting.

The default is 10,000.`,
          minimum: 0
        },
        max_queued_records: {
          type: 'integer',
          format: 'int64',
          description: `Backpressure threshold.

Maximal number of records queued by the endpoint before the endpoint
is paused by the backpressure mechanism.

For input endpoints, this setting bounds the number of records that have
been received from the input transport but haven't yet been consumed by
the circuit since the circuit, since the circuit is still busy processing
previous inputs.

For output endpoints, this setting bounds the number of records that have
been produced by the circuit but not yet sent via the output transport endpoint
nor stored in the output buffer (see \`enable_output_buffer\`).

Note that this is not a hard bound: there can be a small delay between
the backpressure mechanism is triggered and the endpoint is paused, during
which more data may be queued.

The default is 1 million.`,
          minimum: 0
        },
        paused: {
          type: 'boolean',
          description: `Create connector in paused state.

The default is \`false\`.`
        },
        transport: {
          $ref: '#/components/schemas/TransportConfig'
        }
      }
    }
  ],
  description: "A data connector's configuration"
} as const

export const $DatagenInputConfig = {
  type: 'object',
  description: 'Configuration for generating random data for a table.',
  properties: {
    plan: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/GenerationPlan'
      },
      description: `The sequence of generations to perform.

If not set, the generator will produce a single sequence with default settings.
If set, the generator will produce the specified sequences in sequential order.

Note that if one of the sequences before the last one generates an unlimited number of rows
the following sequences will not be executed.`,
      default: [
        {
          fields: {},
          limit: null,
          rate: null,
          worker_chunk_size: null
        }
      ]
    },
    seed: {
      type: 'integer',
      format: 'int64',
      description: `Optional seed for the random generator.

Setting this to a fixed value will make the generator produce the same sequence of records
every time the pipeline is run.

# Notes
- To ensure the set of generated input records is deterministic across multiple runs,
apart from setting a seed, \`workers\` also needs to remain unchanged.
- The input will arrive in non-deterministic order if \`workers > 1\`.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    workers: {
      type: 'integer',
      description: 'Number of workers to use for generating data.',
      default: 1,
      minimum: 0
    }
  },
  additionalProperties: false
} as const

export const $DatagenStrategy = {
  type: 'string',
  description: 'Strategy used to generate values.',
  enum: [
    'increment',
    'uniform',
    'zipf',
    'word',
    'words',
    'sentence',
    'sentences',
    'paragraph',
    'paragraphs',
    'first_name',
    'last_name',
    'title',
    'suffix',
    'name',
    'name_with_title',
    'domain_suffix',
    'email',
    'username',
    'password',
    'field',
    'position',
    'seniority',
    'job_title',
    'ipv4',
    'ipv6',
    'ip',
    'mac_address',
    'user_agent',
    'rfc_status_code',
    'valid_status_code',
    'company_suffix',
    'company_name',
    'buzzword',
    'buzzword_middle',
    'buzzword_tail',
    'catch_phrase',
    'bs_verb',
    'bs_adj',
    'bs_noun',
    'bs',
    'profession',
    'industry',
    'currency_code',
    'currency_name',
    'currency_symbol',
    'credit_card_number',
    'city_prefix',
    'city_suffix',
    'city_name',
    'country_name',
    'country_code',
    'street_suffix',
    'street_name',
    'time_zone',
    'state_name',
    'state_abbr',
    'secondary_address_type',
    'secondary_address',
    'zip_code',
    'post_code',
    'building_number',
    'latitude',
    'longitude',
    'isbn',
    'isbn13',
    'isbn10',
    'phone_number',
    'cell_number',
    'file_path',
    'file_name',
    'file_extension',
    'dir_path'
  ]
} as const

export const $DeltaTableIngestMode = {
  type: 'string',
  description: `Delta table read mode.

Three options are available:

* \`snapshot\` - read a snapshot of the table and stop.

* \`follow\` - continuously ingest changes to the table, starting from a specified version
or timestamp.

* \`snapshot_and_follow\` - read a snapshot of the table before switching to continuous ingestion
mode.`,
  enum: ['snapshot', 'follow', 'snapshot_and_follow']
} as const

export const $DeltaTableReaderConfig = {
  type: 'object',
  description: 'Delta table output connector configuration.',
  required: ['uri', 'mode'],
  properties: {
    datetime: {
      type: 'string',
      description: `Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
"2024-12-09T16:09:53+00:00.

When this option is set, the connector finds and opens the version of the table as of the
specified point in time.  In \`snapshot\` and \`snapshot_and_follow\` modes, it retrieves the
snapshot of this version of the table (based on the server time recorded in the transaction
log, not the event time encoded in the data).  In \`follow\` and \`snapshot_and_follow\` modes, it
follows transaction log records **after** this version.

Note: at most one of \`version\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
      nullable: true
    },
    mode: {
      $ref: '#/components/schemas/DeltaTableIngestMode'
    },
    snapshot_filter: {
      type: 'string',
      description: `Optional row filter.

This option is only valid when \`mode\` is set to \`snapshot\` or \`snapshot_and_follow\`.

When specified, only rows that satisfy the filter condition are included in the
snapshot.  The condition must be a valid SQL Boolean expression that can be used in
the \`where\` clause of the \`select * from snapshot where ...\` query.

This option can be used to specify the range of event times to include in the snapshot,
e.g.: \`ts BETWEEN '2005-01-01 00:00:00' AND '2010-12-31 23:59:59'\`.`,
      nullable: true
    },
    timestamp_column: {
      type: 'string',
      description: `Table column that serves as an event timestamp.


When this option is specified, and \`mode\` is one of \`snapshot\` or \`snapshot_and_follow\`,
the snapshot of the table is ingested in the timestamp order.  This setting is required
for tables declared with the
[\`LATENESS\`](https://docs.feldera.com/sql/streaming#lateness-expressions) attribute
in Feldera SQL. It impacts the performance of the connector, since data must be sorted
before pushing it to the pipeline; therefore it is not recommended to use this
settings for tables without \`LATENESS\`.`,
      nullable: true
    },
    uri: {
      type: 'string',
      description: `Table URI.

Example: "s3://feldera-fraud-detection-data/demographics_train"`
    },
    version: {
      type: 'integer',
      format: 'int64',
      description: `Optional table version.

When this option is set, the connector finds and opens the specified version of the table.
In \`snapshot\` and \`snapshot_and_follow\` modes, it retrieves the snapshot of this version of
the table.  In \`follow\` and \`snapshot_and_follow\` modes, it follows transaction log records
**after** this version.

Note: at most one of \`version\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
      nullable: true
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Storage options for configuring backend object store.

For specific options available for different storage backends, see:
* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)`
  }
} as const

export const $DeltaTableWriteMode = {
  type: 'string',
  description: `Delta table write mode.

Determines how the Delta table connector handles an existing table at the target location.`,
  enum: ['append', 'truncate', 'error_if_exists']
} as const

export const $DeltaTableWriterConfig = {
  type: 'object',
  description: 'Delta table output connector configuration.',
  required: ['uri'],
  properties: {
    mode: {
      $ref: '#/components/schemas/DeltaTableWriteMode'
    },
    uri: {
      type: 'string',
      description: 'Table URI.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Storage options for configuring backend object store.

For specific options available for different storage backends, see:
* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)`
  }
} as const

export const $Demo = {
  type: 'object',
  required: ['name', 'title', 'description', 'program_code', 'udf_rust', 'udf_toml'],
  properties: {
    description: {
      type: 'string',
      description: 'Description of the demo (parsed from SQL preamble).'
    },
    name: {
      type: 'string',
      description: 'Name of the demo (parsed from SQL preamble).'
    },
    program_code: {
      type: 'string',
      description: 'Program SQL code.'
    },
    title: {
      type: 'string',
      description: 'Title of the demo (parsed from SQL preamble).'
    },
    udf_rust: {
      type: 'string',
      description: 'User defined function (UDF) Rust code.'
    },
    udf_toml: {
      type: 'string',
      description: 'User defined function (UDF) TOML dependencies.'
    }
  }
} as const

export const $ErrorResponse = {
  type: 'object',
  description: 'Information returned by REST API endpoints on error.',
  required: ['message', 'error_code', 'details'],
  properties: {
    details: {
      type: 'object',
      description: `Detailed error metadata.
The contents of this field is determined by \`error_code\`.`
    },
    error_code: {
      type: 'string',
      description: 'Error code is a string that specifies this error type.',
      example: 'UnknownInputFormat'
    },
    message: {
      type: 'string',
      description: 'Human-readable error message.',
      example: "Unknown input format 'xml'."
    }
  }
} as const

export const $ExtendedPipelineDescr = {
  type: 'object',
  description: `Pipeline descriptor which besides the basic fields in direct regular control of the user
also has all additional fields generated and maintained by the back-end.`,
  required: [
    'id',
    'name',
    'description',
    'version',
    'created_at',
    'runtime_config',
    'program_code',
    'udf_rust',
    'udf_toml',
    'program_config',
    'program_version',
    'program_status',
    'program_status_since',
    'deployment_status',
    'deployment_status_since',
    'deployment_desired_status'
  ],
  properties: {
    created_at: {
      type: 'string',
      format: 'date-time',
      description: 'Timestamp when the pipeline was originally created.'
    },
    deployment_config: {
      allOf: [
        {
          $ref: '#/components/schemas/PipelineConfig'
        }
      ],
      nullable: true
    },
    deployment_desired_status: {
      $ref: '#/components/schemas/PipelineDesiredStatus'
    },
    deployment_error: {
      allOf: [
        {
          $ref: '#/components/schemas/ErrorResponse'
        }
      ],
      nullable: true
    },
    deployment_location: {
      type: 'string',
      description: `Location where the pipeline can be reached at runtime
(e.g., a TCP port number or a URI).`,
      nullable: true
    },
    deployment_status: {
      $ref: '#/components/schemas/PipelineStatus'
    },
    deployment_status_since: {
      type: 'string',
      format: 'date-time',
      description: `Time when the pipeline was assigned its current status
of the pipeline.`
    },
    description: {
      type: 'string',
      description: 'Pipeline description.'
    },
    id: {
      $ref: '#/components/schemas/PipelineId'
    },
    name: {
      type: 'string',
      description: 'Pipeline name.'
    },
    program_binary_url: {
      type: 'string',
      description: 'URL where to download the program binary from.',
      nullable: true
    },
    program_code: {
      type: 'string',
      description: 'Program SQL code.'
    },
    program_config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    program_info: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramInfo'
        }
      ],
      nullable: true
    },
    program_status: {
      $ref: '#/components/schemas/ProgramStatus'
    },
    program_status_since: {
      type: 'string',
      format: 'date-time',
      description: 'Timestamp when the current program status was set.'
    },
    program_version: {
      $ref: '#/components/schemas/Version'
    },
    runtime_config: {
      $ref: '#/components/schemas/RuntimeConfig'
    },
    udf_rust: {
      type: 'string',
      description: 'Rust code for UDFs.'
    },
    udf_toml: {
      type: 'string',
      description: 'Rust dependencies in the TOML format.'
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $ExtendedPipelineDescrOptionalCode = {
  type: 'object',
  description: 'Extended pipeline descriptor with code being optionally included.',
  required: [
    'id',
    'name',
    'description',
    'version',
    'created_at',
    'runtime_config',
    'program_config',
    'program_version',
    'program_status',
    'program_status_since',
    'deployment_status',
    'deployment_status_since',
    'deployment_desired_status'
  ],
  properties: {
    created_at: {
      type: 'string',
      format: 'date-time'
    },
    deployment_config: {
      allOf: [
        {
          $ref: '#/components/schemas/PipelineConfig'
        }
      ],
      nullable: true
    },
    deployment_desired_status: {
      $ref: '#/components/schemas/PipelineDesiredStatus'
    },
    deployment_error: {
      allOf: [
        {
          $ref: '#/components/schemas/ErrorResponse'
        }
      ],
      nullable: true
    },
    deployment_location: {
      type: 'string',
      nullable: true
    },
    deployment_status: {
      $ref: '#/components/schemas/PipelineStatus'
    },
    deployment_status_since: {
      type: 'string',
      format: 'date-time'
    },
    description: {
      type: 'string'
    },
    id: {
      $ref: '#/components/schemas/PipelineId'
    },
    name: {
      type: 'string'
    },
    program_binary_url: {
      type: 'string',
      nullable: true
    },
    program_code: {
      type: 'string',
      nullable: true
    },
    program_config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    program_info: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramInfo'
        }
      ],
      nullable: true
    },
    program_status: {
      $ref: '#/components/schemas/ProgramStatus'
    },
    program_status_since: {
      type: 'string',
      format: 'date-time'
    },
    program_version: {
      $ref: '#/components/schemas/Version'
    },
    runtime_config: {
      $ref: '#/components/schemas/RuntimeConfig'
    },
    udf_rust: {
      type: 'string',
      nullable: true
    },
    udf_toml: {
      type: 'string',
      nullable: true
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $Field = {
  allOf: [
    {
      $ref: '#/components/schemas/SqlIdentifier'
    },
    {
      type: 'object',
      required: ['columntype'],
      properties: {
        columntype: {
          $ref: '#/components/schemas/ColumnType'
        }
      }
    }
  ],
  description: `A SQL field.

Matches the SQL compiler JSON format.`
} as const

export const $FileInputConfig = {
  type: 'object',
  description: 'Configuration for reading data from a file with `FileInputTransport`',
  required: ['path'],
  properties: {
    buffer_size_bytes: {
      type: 'integer',
      description: `Read buffer size.

Default: when this parameter is not specified, a platform-specific
default is used.`,
      nullable: true,
      minimum: 0
    },
    follow: {
      type: 'boolean',
      description: `Enable file following.

When \`false\`, the endpoint outputs an \`InputConsumer::eoi\`
message and stops upon reaching the end of file.  When \`true\`, the
endpoint will keep watching the file and outputting any new content
appended to it.`
    },
    path: {
      type: 'string',
      description: 'File path.'
    }
  }
} as const

export const $FileOutputConfig = {
  type: 'object',
  description: 'Configuration for writing data to a file with `FileOutputTransport`.',
  required: ['path'],
  properties: {
    path: {
      type: 'string',
      description: 'File path.'
    }
  }
} as const

export const $FormatConfig = {
  type: 'object',
  description: `Data format specification used to parse raw data received from the
endpoint or to encode data sent to the endpoint.`,
  required: ['name'],
  properties: {
    config: {
      type: 'object',
      description: 'Format-specific parser or encoder configuration.'
    },
    name: {
      type: 'string',
      description: 'Format name, e.g., "csv", "json", "bincode", etc.'
    }
  }
} as const

export const $FtConfig = {
  type: 'string',
  description: 'Fault-tolerance configuration for runtime startup.',
  enum: ['initial_state', 'latest_checkpoint']
} as const

export const $GenerationPlan = {
  type: 'object',
  description:
    'A random generation plan for a table that generates either a limited amount of rows or runs continuously.',
  properties: {
    fields: {
      type: 'object',
      description: 'Specifies the values that the generator should produce.',
      default: {},
      additionalProperties: {
        $ref: '#/components/schemas/RngFieldSettings'
      }
    },
    limit: {
      type: 'integer',
      description: `Total number of new rows to generate.

If not set, the generator will produce new/unique records as long as the pipeline is running.
If set to 0, the table will always remain empty.
If set, the generator will produce new records until the specified limit is reached.

Note that if the table has one or more primary keys that don't use the \`increment\` strategy to
generate the key there is a potential that an update is generated instead of an insert. In
this case it's possible the total number of records is less than the specified limit.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    rate: {
      type: 'integer',
      format: 'int32',
      description: `Non-zero number of rows to generate per second.

If not set, the generator will produce rows as fast as possible.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    worker_chunk_size: {
      type: 'integer',
      description: `When multiple workers are used, each worker will pick a consecutive "chunk" of
records to generate.

By default, if not specified, the generator will use the formula \`min(rate, 10_000)\`
to determine it. This works well in most situations. However, if you're
running tests with lateness and many workers you can e.g., reduce the
chunk size to make sure a smaller range of records is being ingested in parallel.

# Example
Assume you generate a total of 125 records with 4 workers and a chunk size of 25.
In this case, worker A will generate records 0..25, worker B will generate records 25..50,
etc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk
will pick up the last chunk of records (100..125) to generate.`,
      default: null,
      nullable: true,
      minimum: 0
    }
  },
  additionalProperties: false
} as const

export const $InputEndpointConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    {
      type: 'object',
      required: ['stream'],
      properties: {
        stream: {
          type: 'string',
          description: `The name of the input stream of the circuit that this endpoint is
connected to.`
        }
      }
    }
  ],
  description: 'Describes an input connector configuration'
} as const

export const $IntervalUnit = {
  type: 'string',
  description: `The specified units for SQL Interval types.

\`INTERVAL 1 DAY\`, \`INTERVAL 1 DAY TO HOUR\`, \`INTERVAL 1 DAY TO MINUTE\`,
would yield \`Day\`, \`DayToHour\`, \`DayToMinute\`, as the \`IntervalUnit\` respectively.`,
  enum: [
    'DAY',
    'DAYTOHOUR',
    'DAYTOMINUTE',
    'DAYTOSECOND',
    'HOUR',
    'HOURTOMINUTE',
    'HOURTOSECOND',
    'MINUTE',
    'MINUTETOSECOND',
    'MONTH',
    'SECOND',
    'YEAR',
    'YEARTOMONTH'
  ]
} as const

export const $JsonUpdateFormat = {
  type: 'string',
  description: `Supported JSON data change event formats.

Each element in a JSON-formatted input stream specifies
an update to one or more records in an input table.  We support
several different ways to represent such updates.`,
  enum: ['insert_delete', 'weighted', 'debezium', 'snowflake', 'raw']
} as const

export const $KafkaHeader = {
  type: 'object',
  description: 'Kafka message header.',
  required: ['key'],
  properties: {
    key: {
      type: 'string'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaHeaderValue'
        }
      ],
      nullable: true
    }
  }
} as const

export const $KafkaHeaderValue = {
  type: 'string',
  format: 'binary',
  description: 'Kafka header value encoded as a UTF-8 string or a byte array.'
} as const

export const $KafkaInputConfig = {
  type: 'object',
  description: 'Configuration for reading data from Kafka topics with `InputTransport`.',
  required: ['topics'],
  properties: {
    fault_tolerance: {
      type: 'boolean',
      description: 'If true, this enables fault tolerance in the Kafka input connector.'
    },
    group_join_timeout_secs: {
      type: 'integer',
      format: 'int32',
      description: `Maximum timeout in seconds to wait for the endpoint to join the Kafka
consumer group during initialization.`,
      minimum: 0
    },
    kafka_service: {
      type: 'string',
      description: 'If specified, this service is used to provide defaults for the Kafka options.',
      nullable: true
    },
    log_level: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaLogLevel'
        }
      ],
      nullable: true
    },
    poller_threads: {
      type: 'integer',
      description: `Set to 1 or more to fix the number of threads used to poll
\`rdkafka\`. Multiple threads can increase performance with small Kafka
messages; for large messages, one thread is enough. In either case, too
many threads can harm performance. If unset, the default is 3, which
helps with small messages but will not harm performance with large
messagee`,
      nullable: true,
      minimum: 0
    },
    topics: {
      type: 'array',
      items: {
        type: 'string'
      },
      description: 'List of topics to subscribe to.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Options passed directly to \`rdkafka\`.

[\`librdkafka\` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
used to configure the Kafka consumer.  Not all options are valid with
this Kafka adapter:

* "enable.auto.commit", if present, must be set to "false",
* "enable.auto.offset.store", if present, must be set to "false"`
  }
} as const

export const $KafkaLogLevel = {
  type: 'string',
  description: 'Kafka logging levels.',
  enum: ['emerg', 'alert', 'critical', 'error', 'warning', 'notice', 'info', 'debug']
} as const

export const $KafkaOutputConfig = {
  type: 'object',
  description: 'Configuration for writing data to a Kafka topic with `OutputTransport`.',
  required: ['topic'],
  properties: {
    fault_tolerance: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaOutputFtConfig'
        }
      ],
      nullable: true
    },
    headers: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/KafkaHeader'
      },
      description: 'Kafka headers to be added to each message produced by this connector.'
    },
    initialization_timeout_secs: {
      type: 'integer',
      format: 'int32',
      description: `Maximum timeout in seconds to wait for the endpoint to connect to
a Kafka broker.

Defaults to 60.`,
      minimum: 0
    },
    kafka_service: {
      type: 'string',
      description: 'If specified, this service is used to provide defaults for the Kafka options.',
      nullable: true
    },
    log_level: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaLogLevel'
        }
      ],
      nullable: true
    },
    topic: {
      type: 'string',
      description: 'Topic to write to.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Options passed directly to \`rdkafka\`.

See [\`librdkafka\` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
used to configure the Kafka producer.`
  }
} as const

export const $KafkaOutputFtConfig = {
  type: 'object',
  description: 'Fault tolerance configuration for Kafka output connector.',
  properties: {
    consumer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for consumers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for consumers, and may be empty.`,
      default: {},
      additionalProperties: {
        type: 'string'
      }
    },
    producer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for producers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for producers, and may be empty.`,
      default: {},
      additionalProperties: {
        type: 'string'
      }
    }
  }
} as const

export const $ListPipelinesQueryParameters = {
  type: 'object',
  description: 'Query parameters for GET the list of pipelines.',
  properties: {
    code: {
      type: 'boolean',
      description: `Whether to include program code in the response (default: \`true\`).
Passing \`false\` reduces the response size, which is particularly handy
when frequently monitoring the endpoint over low bandwidth connections.`
    }
  }
} as const

export const $NewApiKeyRequest = {
  type: 'object',
  description: 'Request to create a new API key.',
  required: ['name'],
  properties: {
    name: {
      type: 'string',
      description: 'Key name.',
      example: 'my-api-key'
    }
  }
} as const

export const $NewApiKeyResponse = {
  type: 'object',
  description: 'Response to a successful API key creation.',
  required: ['id', 'name', 'api_key'],
  properties: {
    api_key: {
      type: 'string',
      description: `Generated secret API key. There is no way to retrieve this
key again through the API, so store it securely.`,
      example:
        'apikey:v5y5QNtlPNVMwkmNjKwFU8bbIu5lMge3yHbyddxAOdXlEo84SEoNn32DUhQaf1KLeI9aOOfnJjhQ1pYzMrU4wQXON6pm6BS7Zgzj46U2b8pwz1280vYBEtx41hiDBRP'
    },
    id: {
      $ref: '#/components/schemas/ApiKeyId'
    },
    name: {
      type: 'string',
      description: 'API key name provided by the user.',
      example: 'my-api-key'
    }
  }
} as const

export const $NexmarkInputConfig = {
  type: 'object',
  description: `Configuration for generating Nexmark input data.

This connector must be used exactly three times in a pipeline if it is used
at all, once for each [\`NexmarkTable\`].`,
  required: ['table'],
  properties: {
    options: {
      allOf: [
        {
          $ref: '#/components/schemas/NexmarkInputOptions'
        }
      ],
      nullable: true
    },
    table: {
      $ref: '#/components/schemas/NexmarkTable'
    }
  }
} as const

export const $NexmarkInputOptions = {
  type: 'object',
  description: 'Configuration for generating Nexmark input data.',
  properties: {
    batch_size_per_thread: {
      type: 'integer',
      format: 'int64',
      description: `Number of events to generate and submit together, per thread.

Each thread generates this many records, which are then combined with
the records generated by the other threads, to form combined input
batches of size \`threads × batch_size_per_thread\`.`,
      default: 1000,
      minimum: 0
    },
    events: {
      type: 'integer',
      format: 'int64',
      description: 'Number of events to generate.',
      default: 100000000,
      minimum: 0
    },
    max_step_size_per_thread: {
      type: 'integer',
      format: 'int64',
      description: `Maximum number of events to submit in a single step, per thread.

This should really be per worker thread, not per generator thread, but
the connector does not know how many worker threads there are.

This stands in for \`max_batch_size\` from the connector configuration
because it must be a constant across all three of the nexmark tables.`,
      default: 10000,
      minimum: 0
    },
    threads: {
      type: 'integer',
      description: `Number of event generator threads.

It's reasonable to choose the same number of generator threads as worker
threads.`,
      default: 4,
      minimum: 0
    }
  }
} as const

export const $NexmarkTable = {
  type: 'string',
  description: 'Table in Nexmark.',
  enum: ['bid', 'auction', 'person']
} as const

export const $OutputBufferConfig = {
  type: 'object',
  properties: {
    enable_output_buffer: {
      type: 'boolean',
      description: `Enable output buffering.

The output buffering mechanism allows decoupling the rate at which the pipeline
pushes changes to the output transport from the rate of input changes.

By default, output updates produced by the pipeline are pushed directly to
the output transport. Some destinations may prefer to receive updates in fewer
bigger batches. For instance, when writing Parquet files, producing
one bigger file every few minutes is usually better than creating
small files every few milliseconds.

To achieve such input/output decoupling, users can enable output buffering by
setting the \`enable_output_buffer\` flag to \`true\`.  When buffering is enabled, output
updates produced by the pipeline are consolidated in an internal buffer and are
pushed to the output transport when one of several conditions is satisfied:

* data has been accumulated in the buffer for more than \`max_output_buffer_time_millis\`
milliseconds.
* buffer size exceeds \`max_output_buffer_size_records\` records.

This flag is \`false\` by default.`,
      default: false
    },
    max_output_buffer_size_records: {
      type: 'integer',
      description: `Maximum number of updates to be kept in the output buffer.

This parameter bounds the maximal size of the buffer.
Note that the size of the buffer is not always equal to the
total number of updates output by the pipeline. Updates to the
same record can overwrite or cancel previous updates.

By default, the buffer can grow indefinitely until one of
the other output conditions is satisfied.

NOTE: this configuration option requires the \`enable_output_buffer\` flag
to be set.`,
      default: 18446744073709552000,
      minimum: 0
    },
    max_output_buffer_time_millis: {
      type: 'integer',
      description: `Maximum time in milliseconds data is kept in the output buffer.

By default, data is kept in the buffer indefinitely until one of
the other output conditions is satisfied.  When this option is
set the buffer will be flushed at most every
\`max_output_buffer_time_millis\` milliseconds.

NOTE: this configuration option requires the \`enable_output_buffer\` flag
to be set.`,
      default: 18446744073709552000,
      minimum: 0
    }
  }
} as const

export const $OutputEndpointConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    {
      type: 'object',
      required: ['stream'],
      properties: {
        stream: {
          type: 'string',
          description: `The name of the output stream of the circuit that this endpoint is
connected to.`
        }
      }
    }
  ],
  description: 'Describes an output connector configuration'
} as const

export const $PatchPipeline = {
  type: 'object',
  description: `Patch (partially) update the pipeline.

Note that the patching only applies to the main fields, not subfields.
For instance, it is not possible to update only the number of workers;
it is required to again pass the whole runtime configuration with the
change.`,
  properties: {
    description: {
      type: 'string',
      nullable: true
    },
    name: {
      type: 'string',
      nullable: true
    },
    program_code: {
      type: 'string',
      nullable: true
    },
    program_config: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramConfig'
        }
      ],
      nullable: true
    },
    runtime_config: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeConfig'
        }
      ],
      nullable: true
    },
    udf_rust: {
      type: 'string',
      nullable: true
    },
    udf_toml: {
      type: 'string',
      nullable: true
    }
  }
} as const

export const $PipelineConfig = {
  allOf: [
    {
      type: 'object',
      description: `Global pipeline configuration settings. This is the publicly
exposed type for users to configure pipelines.`,
      properties: {
        clock_resolution_usecs: {
          type: 'integer',
          format: 'int64',
          description: `Real-time clock resolution in microseconds.

This parameter controls the execution of queries that use the \`NOW()\` function.  The output of such
queries depends on the real-time clock and can change over time without any external
inputs.  The pipeline will update the clock value and trigger incremental recomputation
at most each \`clock_resolution_usecs\` microseconds.

It is set to 100 milliseconds (100,000 microseconds) by default.

Set to \`null\` to disable periodic clock updates.`,
          default: 100000,
          nullable: true,
          minimum: 0
        },
        cpu_profiler: {
          type: 'boolean',
          description: `Enable CPU profiler.

The default value is \`true\`.`,
          default: true
        },
        fault_tolerance: {
          allOf: [
            {
              $ref: '#/components/schemas/FtConfig'
            }
          ],
          default: null,
          nullable: true
        },
        max_buffering_delay_usecs: {
          type: 'integer',
          format: 'int64',
          description: `Maximal delay in microseconds to wait for \`min_batch_size_records\` to
get buffered by the controller, defaults to 0.`,
          default: 0,
          minimum: 0
        },
        min_batch_size_records: {
          type: 'integer',
          format: 'int64',
          description: `Minimal input batch size.

The controller delays pushing input records to the circuit until at
least \`min_batch_size_records\` records have been received (total
across all endpoints) or \`max_buffering_delay_usecs\` microseconds
have passed since at least one input records has been buffered.
Defaults to 0.`,
          default: 0,
          minimum: 0
        },
        min_storage_bytes: {
          type: 'integer',
          description: `The minimum estimated number of bytes in a batch of data to write it to
storage.  This is provided for debugging and fine-tuning and should
ordinarily be left unset. It only has an effect when \`storage\` is set to
true.

A value of 0 will write even empty batches to storage, and nonzero
values provide a threshold.  \`usize::MAX\` would effectively disable
storage.`,
          default: null,
          nullable: true,
          minimum: 0
        },
        resources: {
          allOf: [
            {
              $ref: '#/components/schemas/ResourceConfig'
            }
          ],
          default: {
            cpu_cores_max: null,
            cpu_cores_min: null,
            memory_mb_max: null,
            memory_mb_min: null,
            storage_class: null,
            storage_mb_max: null
          }
        },
        storage: {
          type: 'boolean',
          description: `Should storage be enabled for this pipeline?

- If \`false\` (default), the pipeline's state is kept in in-memory
data-structures.  This is useful if the pipeline's state will fit in
memory and if the pipeline is ephemeral and does not need to be
recovered after a restart. The pipeline will most likely run faster
since it does not need to access storage.

- If \`true\`, the pipeline's state is kept on storage.  This allows the
pipeline to work with state that will not fit into memory. It also
allows the state to be checkpointed and recovered across restarts.
This feature is currently experimental.`,
          default: false
        },
        tracing: {
          type: 'boolean',
          description: 'Enable pipeline tracing.',
          default: false
        },
        tracing_endpoint_jaeger: {
          type: 'string',
          description: 'Jaeger tracing endpoint to send tracing information to.',
          default: '127.0.0.1:6831'
        },
        workers: {
          type: 'integer',
          format: 'int32',
          description: 'Number of DBSP worker threads.',
          default: 8,
          minimum: 0
        }
      }
    },
    {
      type: 'object',
      required: ['inputs'],
      properties: {
        inputs: {
          type: 'object',
          description: 'Input endpoint configuration.',
          additionalProperties: {
            $ref: '#/components/schemas/InputEndpointConfig'
          }
        },
        name: {
          type: 'string',
          description: 'Pipeline name.',
          nullable: true
        },
        outputs: {
          type: 'object',
          description: 'Output endpoint configuration.',
          additionalProperties: {
            $ref: '#/components/schemas/OutputEndpointConfig'
          }
        },
        storage_config: {
          allOf: [
            {
              $ref: '#/components/schemas/StorageConfig'
            }
          ],
          nullable: true
        }
      }
    }
  ],
  description: `Pipeline deployment configuration.
It represents configuration entries directly provided by the user
(e.g., runtime configuration) and entries derived from the schema
of the compiled program (e.g., connectors). Storage configuration,
if applicable, is set by the runner.`
} as const

export const $PipelineDescr = {
  type: 'object',
  description: 'Pipeline descriptor.',
  required: ['name', 'description', 'runtime_config', 'program_code', 'program_config'],
  properties: {
    description: {
      type: 'string',
      description: 'Pipeline description.'
    },
    name: {
      type: 'string',
      description: 'Pipeline name.'
    },
    program_code: {
      type: 'string',
      description: 'Program SQL code.'
    },
    program_config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    runtime_config: {
      $ref: '#/components/schemas/RuntimeConfig'
    },
    udf_rust: {
      type: 'string',
      description: 'Rust code for UDFs.'
    },
    udf_toml: {
      type: 'string',
      description: 'Rust dependencies in the TOML format.'
    }
  }
} as const

export const $PipelineDesiredStatus = {
  type: 'string',
  enum: ['Shutdown', 'Paused', 'Running']
} as const

export const $PipelineId = {
  type: 'string',
  format: 'uuid',
  description: 'Pipeline identifier.'
} as const

export const $PipelineStatus = {
  type: 'string',
  description: `Pipeline status.

This type represents the state of the pipeline tracked by the pipeline
runner and observed by the API client via the \`GET /v0/pipelines/{name}\` endpoint.

### The lifecycle of a pipeline

The following automaton captures the lifecycle of the pipeline.
Individual states and transitions of the automaton are described below.

* States labeled with the hourglass symbol (⌛) are **timed** states. The
automaton stays in timed state until the corresponding operation completes
or until it transitions to become failed after the pre-defined timeout
period expires.

* State transitions labeled with API endpoint names (\`/start\`, \`/pause\`,
\`/shutdown\`) are triggered by invoking corresponding endpoint,
e.g., \`POST /v0/pipelines/{name}/start\`. Note that these only express
desired state, and are applied asynchronously by the automata.

\`\`\`text
Shutdown◄────────────────────┐
│                        │
/start or /pause│                    ShuttingDown ◄────── Failed
│                        ▲                  ▲
▼              /shutdown │                  │
⌛Provisioning ──────────────────┤        Shutdown, Provisioning,
│                        │        Initializing, Paused,
│                        │         Running, Unavailable
▼                        │    (all states except ShuttingDown
⌛Initializing ──────────────────┤      can transition to Failed)
│                        │
┌─────────┼────────────────────────┴─┐
│         ▼                          │
│       Paused  ◄──────► Unavailable │
│       │    ▲                ▲      │
│ /start│    │/pause          │      │
│       ▼    │                │      │
│      Running ◄──────────────┘      │
└────────────────────────────────────┘
\`\`\`

### Desired and actual status

We use the desired state model to manage the lifecycle of a pipeline.
In this model, the pipeline has two status attributes associated with
it at runtime: the **desired** status, which represents what the user
would like the pipeline to do, and the **current** status, which
represents the actual state of the pipeline.  The pipeline runner
service continuously monitors both fields and steers the pipeline
towards the desired state specified by the user.
Only three of the states in the pipeline automaton above can be
used as desired statuses: \`Paused\`, \`Running\`, and \`Shutdown\`.
These statuses are selected by invoking REST endpoints shown
in the diagram.

The user can monitor the current state of the pipeline via the
\`GET /v0/pipelines/{name}\` endpoint, which returns an object of type
\`ExtendedPipelineDescr\`. In a typical scenario, the user first sets
the desired state, e.g., by invoking the \`/start\` endpoint, and
then polls the \`GET /v0/pipelines/{name}\` endpoint to monitor the
actual status of the pipeline until its \`deployment_status\` attribute
changes to "running" indicating that the pipeline has been successfully
initialized and is processing data, or "failed", indicating an error.`,
  enum: [
    'Shutdown',
    'Provisioning',
    'Initializing',
    'Paused',
    'Running',
    'Unavailable',
    'Failed',
    'ShuttingDown'
  ]
} as const

export const $ProgramConfig = {
  type: 'object',
  description: 'Program configuration.',
  properties: {
    profile: {
      allOf: [
        {
          $ref: '#/components/schemas/CompilationProfile'
        }
      ],
      nullable: true
    }
  }
} as const

export const $ProgramInfo = {
  type: 'object',
  description:
    'Program information generated by the SQL compiler which includes schema, input connectors and output connectors.',
  required: ['schema', 'input_connectors', 'output_connectors'],
  properties: {
    input_connectors: {
      type: 'object',
      description: 'Input connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/InputEndpointConfig'
      }
    },
    output_connectors: {
      type: 'object',
      description: 'Output connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/OutputEndpointConfig'
      }
    },
    schema: {
      $ref: '#/components/schemas/ProgramSchema'
    },
    udf_stubs: {
      type: 'string',
      description: 'UDF stubs.'
    }
  }
} as const

export const $ProgramSchema = {
  type: 'object',
  description: `A struct containing the tables (inputs) and views for a program.

Parse from the JSON data-type of the DDL generated by the SQL compiler.`,
  required: ['inputs', 'outputs'],
  properties: {
    inputs: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Relation'
      }
    },
    outputs: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Relation'
      }
    }
  }
} as const

export const $ProgramStatus = {
  oneOf: [
    {
      type: 'string',
      description: `Compilation request received from the user; program has been placed
in the queue.`,
      enum: ['Pending']
    },
    {
      type: 'string',
      description: 'Compilation of SQL -> Rust in progress.',
      enum: ['CompilingSql']
    },
    {
      type: 'string',
      description: 'Compiling Rust -> executable in progress.',
      enum: ['CompilingRust']
    },
    {
      type: 'string',
      description: 'Compilation succeeded.',
      enum: ['Success']
    },
    {
      type: 'object',
      required: ['SqlError'],
      properties: {
        SqlError: {
          type: 'array',
          items: {
            $ref: '#/components/schemas/SqlCompilerMessage'
          },
          description: 'SQL compiler returned an error.'
        }
      }
    },
    {
      type: 'object',
      required: ['RustError'],
      properties: {
        RustError: {
          type: 'string',
          description: 'Rust compiler returned an error.'
        }
      }
    },
    {
      type: 'object',
      required: ['SystemError'],
      properties: {
        SystemError: {
          type: 'string',
          description: 'System/OS returned an error when trying to invoke commands.'
        }
      }
    }
  ],
  description: 'Program compilation status.'
} as const

export const $PropertyValue = {
  type: 'object',
  required: ['value', 'key_position', 'value_position'],
  properties: {
    key_position: {
      $ref: '#/components/schemas/SourcePosition'
    },
    value: {
      type: 'string'
    },
    value_position: {
      $ref: '#/components/schemas/SourcePosition'
    }
  }
} as const

export const $ProviderAwsCognito = {
  type: 'object',
  required: ['jwk_uri', 'login_url', 'logout_url'],
  properties: {
    jwk_uri: {
      type: 'string'
    },
    login_url: {
      type: 'string'
    },
    logout_url: {
      type: 'string'
    }
  }
} as const

export const $ProviderGoogleIdentity = {
  type: 'object',
  required: ['jwk_uri', 'client_id'],
  properties: {
    client_id: {
      type: 'string'
    },
    jwk_uri: {
      type: 'string'
    }
  }
} as const

export const $PubSubInputConfig = {
  type: 'object',
  description: 'Google Pub/Sub input connector configuration.',
  required: ['subscription'],
  properties: {
    connect_timeout_seconds: {
      type: 'integer',
      format: 'int32',
      description: 'gRPC connection timeout.',
      nullable: true,
      minimum: 0
    },
    credentials: {
      type: 'string',
      description: `The content of a Google Cloud credentials JSON file.

When this option is specified, the connector will use the provided credentials for
authentication.  Otherwise, it will use Application Default Credentials (ADC) configured
in the environment where the Feldera service is running.  See
[Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)
for information on configuring application default credentials.

When running Feldera in an environment where ADC are not configured,
e.g., a Docker container, use this option to ship Google Cloud credentials from another environment.
For example, if you use the
[\`gcloud auth application-default login\`](https://cloud.google.com/pubsub/docs/authentication#client-libs)
command for authentication in your local development environment, ADC are stored in the
\`.config/gcloud/application_default_credentials.json\` file in your home directory.`,
      nullable: true
    },
    emulator: {
      type: 'string',
      description: `Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)
instead of the production service, e.g., 'localhost:8681'.`,
      nullable: true
    },
    endpoint: {
      type: 'string',
      description: "Override the default service endpoint 'pubsub.googleapis.com'",
      nullable: true
    },
    pool_size: {
      type: 'integer',
      format: 'int32',
      description: 'gRPC channel pool size.',
      nullable: true,
      minimum: 0
    },
    project_id: {
      type: 'string',
      description: `Google Cloud project_id.

When not specified, the connector will use the project id associated
with the authenticated account.`,
      nullable: true
    },
    snapshot: {
      type: 'string',
      description: `Reset subscription's backlog to a given snapshot on startup,
using the Pub/Sub \`Seek\` API.

This option is mutually exclusive with the \`timestamp\` option.`,
      nullable: true
    },
    subscription: {
      type: 'string',
      description: 'Subscription name.'
    },
    timeout_seconds: {
      type: 'integer',
      format: 'int32',
      description: 'gRPC request timeout.',
      nullable: true,
      minimum: 0
    },
    timestamp: {
      type: 'string',
      description: `Reset subscription's backlog to a given timestamp on startup,
using the Pub/Sub \`Seek\` API.

The value of this option is an ISO 8601-encoded UTC time, e.g., "2024-08-17T16:39:57-08:00".

This option is mutually exclusive with the \`snapshot\` option.`,
      nullable: true
    }
  }
} as const

export const $Relation = {
  allOf: [
    {
      $ref: '#/components/schemas/SqlIdentifier'
    },
    {
      type: 'object',
      required: ['fields'],
      properties: {
        fields: {
          type: 'array',
          items: {
            $ref: '#/components/schemas/Field'
          }
        },
        materialized: {
          type: 'boolean'
        },
        properties: {
          type: 'object',
          additionalProperties: {
            $ref: '#/components/schemas/PropertyValue'
          }
        }
      }
    }
  ],
  description: `A SQL table or view. It has a name and a list of fields.

Matches the Calcite JSON format.`
} as const

export const $ResourceConfig = {
  type: 'object',
  properties: {
    cpu_cores_max: {
      type: 'integer',
      format: 'int64',
      description: `The maximum number of CPU cores to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    },
    cpu_cores_min: {
      type: 'integer',
      format: 'int64',
      description: `The minimum number of CPU cores to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    },
    memory_mb_max: {
      type: 'integer',
      format: 'int64',
      description: `The maximum memory in Megabytes to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    },
    memory_mb_min: {
      type: 'integer',
      format: 'int64',
      description: `The minimum memory in Megabytes to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    },
    storage_class: {
      type: 'string',
      description: `Storage class to use for an instance of this pipeline.
The class determines storage performance such as IOPS and throughput.`,
      default: null,
      nullable: true
    },
    storage_mb_max: {
      type: 'integer',
      format: 'int64',
      description: `The total storage in Megabytes to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    }
  }
} as const

export const $RngFieldSettings = {
  type: 'object',
  description: 'Configuration for generating random data for a field of a table.',
  properties: {
    e: {
      type: 'integer',
      format: 'int64',
      description: `The frequency rank exponent for the Zipf distribution.

- This value is only used if the strategy is set to \`Zipf\`.
- The default value is 1.0.`,
      default: 1
    },
    fields: {
      type: 'object',
      description:
        'Specifies the values that the generator should produce in case the field is a struct type.',
      default: null,
      additionalProperties: {
        $ref: '#/components/schemas/RngFieldSettings'
      },
      nullable: true
    },
    key: {
      allOf: [
        {
          $ref: '#/components/schemas/RngFieldSettings'
        }
      ],
      default: null,
      nullable: true
    },
    null_percentage: {
      type: 'integer',
      description: `Percentage of records where this field should be set to NULL.

If not set, the generator will produce only records with non-NULL values.
If set to \`1..=100\`, the generator will produce records with NULL values with the specified percentage.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    range: {
      type: 'object',
      description: `An optional, exclusive range [a, b) to limit the range of values the generator should produce.

- For integer/floating point types specifies min/max values as an integer.
If not set, the generator will produce values for the entire range of the type for number types.
- For string/binary types specifies min/max length as an integer, values are required to be >=0.
If not set, a range of [0, 25) is used by default.
- For timestamp types specifies the min/max as two strings in the RFC 3339 format
(e.g., ["2021-01-01T00:00:00Z", "2022-01-02T00:00:00Z"]).
Alternatively, the range values can be specified as a number of non-leap
milliseconds since January 1, 1970 0:00:00.000 UTC (aka “UNIX timestamp”).
If not set, a range of ["1970-01-01T00:00:00Z", "2100-01-01T00:00:00Z") or [0, 4102444800000)
is used by default.
- For time types specifies the min/max as two strings in the "HH:MM:SS" format.
Alternatively, the range values can be specified in milliseconds as two positive integers.
If not set, the range is 24h.
- For date types, the min/max range is specified as two strings in the "YYYY-MM-DD" format.
Alternatively, two integers that represent number of days since January 1, 1970 can be used.
If not set, a range of ["1970-01-01", "2100-01-01") or [0, 54787) is used by default.
- For array types specifies the min/max number of elements as an integer.
If not set, a range of [0, 5) is used by default. Range values are required to be >=0.
- For map types specifies the min/max number of key-value pairs as an integer.
If not set, a range of [0, 5) is used by default.
- For struct/boolean/null types \`range\` is ignored.`
    },
    scale: {
      type: 'integer',
      format: 'int64',
      description: `A scale factor to apply a multiplier to the generated value.

- For integer/floating point types, the value is multiplied by the scale factor.
- For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.
- For time types, the generated value (milliseconds) is multiplied by the scale factor.
- For date types, the generated value (days) is multiplied by the scale factor.
- For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.

- If \`values\` is specified, the scale factor is ignored.
- If \`range\` is specified and the range is required to be positive (struct, map, array etc.)
the scale factor is required to be positive too.

The default scale factor is 1.`,
      default: 1
    },
    strategy: {
      allOf: [
        {
          $ref: '#/components/schemas/DatagenStrategy'
        }
      ],
      default: 'increment'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/RngFieldSettings'
        }
      ],
      default: null,
      nullable: true
    },
    values: {
      type: 'array',
      items: {
        type: 'object'
      },
      description: `An optional set of values the generator will pick from.

If set, the generator will pick values from the specified set.
If not set, the generator will produce values according to the specified range.
If set to an empty set, the generator will produce NULL values.
If set to a single value, the generator will produce only that value.

Note that \`range\` is ignored if \`values\` is set.`,
      default: null,
      nullable: true
    }
  },
  additionalProperties: false
} as const

export const $RuntimeConfig = {
  type: 'object',
  description: `Global pipeline configuration settings. This is the publicly
exposed type for users to configure pipelines.`,
  properties: {
    clock_resolution_usecs: {
      type: 'integer',
      format: 'int64',
      description: `Real-time clock resolution in microseconds.

This parameter controls the execution of queries that use the \`NOW()\` function.  The output of such
queries depends on the real-time clock and can change over time without any external
inputs.  The pipeline will update the clock value and trigger incremental recomputation
at most each \`clock_resolution_usecs\` microseconds.

It is set to 100 milliseconds (100,000 microseconds) by default.

Set to \`null\` to disable periodic clock updates.`,
      default: 100000,
      nullable: true,
      minimum: 0
    },
    cpu_profiler: {
      type: 'boolean',
      description: `Enable CPU profiler.

The default value is \`true\`.`,
      default: true
    },
    fault_tolerance: {
      allOf: [
        {
          $ref: '#/components/schemas/FtConfig'
        }
      ],
      default: null,
      nullable: true
    },
    max_buffering_delay_usecs: {
      type: 'integer',
      format: 'int64',
      description: `Maximal delay in microseconds to wait for \`min_batch_size_records\` to
get buffered by the controller, defaults to 0.`,
      default: 0,
      minimum: 0
    },
    min_batch_size_records: {
      type: 'integer',
      format: 'int64',
      description: `Minimal input batch size.

The controller delays pushing input records to the circuit until at
least \`min_batch_size_records\` records have been received (total
across all endpoints) or \`max_buffering_delay_usecs\` microseconds
have passed since at least one input records has been buffered.
Defaults to 0.`,
      default: 0,
      minimum: 0
    },
    min_storage_bytes: {
      type: 'integer',
      description: `The minimum estimated number of bytes in a batch of data to write it to
storage.  This is provided for debugging and fine-tuning and should
ordinarily be left unset. It only has an effect when \`storage\` is set to
true.

A value of 0 will write even empty batches to storage, and nonzero
values provide a threshold.  \`usize::MAX\` would effectively disable
storage.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    resources: {
      allOf: [
        {
          $ref: '#/components/schemas/ResourceConfig'
        }
      ],
      default: {
        cpu_cores_max: null,
        cpu_cores_min: null,
        memory_mb_max: null,
        memory_mb_min: null,
        storage_class: null,
        storage_mb_max: null
      }
    },
    storage: {
      type: 'boolean',
      description: `Should storage be enabled for this pipeline?

- If \`false\` (default), the pipeline's state is kept in in-memory
data-structures.  This is useful if the pipeline's state will fit in
memory and if the pipeline is ephemeral and does not need to be
recovered after a restart. The pipeline will most likely run faster
since it does not need to access storage.

- If \`true\`, the pipeline's state is kept on storage.  This allows the
pipeline to work with state that will not fit into memory. It also
allows the state to be checkpointed and recovered across restarts.
This feature is currently experimental.`,
      default: false
    },
    tracing: {
      type: 'boolean',
      description: 'Enable pipeline tracing.',
      default: false
    },
    tracing_endpoint_jaeger: {
      type: 'string',
      description: 'Jaeger tracing endpoint to send tracing information to.',
      default: '127.0.0.1:6831'
    },
    workers: {
      type: 'integer',
      format: 'int32',
      description: 'Number of DBSP worker threads.',
      default: 8,
      minimum: 0
    }
  }
} as const

export const $S3InputConfig = {
  type: 'object',
  description: 'Configuration for reading data from AWS S3.',
  required: ['region', 'bucket_name'],
  properties: {
    aws_access_key_id: {
      type: 'string',
      description:
        'AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.',
      nullable: true
    },
    aws_secret_access_key: {
      type: 'string',
      description:
        'Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.',
      nullable: true
    },
    bucket_name: {
      type: 'string',
      description: 'S3 bucket name to access.'
    },
    key: {
      type: 'string',
      description: 'Read a single object specified by a key.',
      nullable: true
    },
    no_sign_request: {
      type: 'boolean',
      description:
        'Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI.'
    },
    prefix: {
      type: 'string',
      description:
        'Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.',
      nullable: true
    },
    region: {
      type: 'string',
      description: 'AWS region.'
    },
    streaming: {
      type: 'boolean',
      description: `Determines how the connector ingests an individual S3 object. When \`true\`,
the connector pushes the object to the pipeline chunk-by-chunk, so that the
pipeline can parse and process initial chunks of the object before the entire
object has been retrieved. This mode is suitable for streaming formats such as
newline-delimited JSON. When \`false\`, the connector buffers the entire object
in memory and pushes it to the pipeline as a single chunk.  Appropriate for
formats like Parquet that cannot be streamed.

The default value is \`false\`.`
    }
  }
} as const

export const $SourcePosition = {
  type: 'object',
  required: ['start_line_number', 'start_column', 'end_line_number', 'end_column'],
  properties: {
    end_column: {
      type: 'integer',
      minimum: 0
    },
    end_line_number: {
      type: 'integer',
      minimum: 0
    },
    start_column: {
      type: 'integer',
      minimum: 0
    },
    start_line_number: {
      type: 'integer',
      minimum: 0
    }
  }
} as const

export const $SqlCompilerMessage = {
  type: 'object',
  description: `A SQL compiler error.

The SQL compiler returns a list of errors in the following JSON format if
it's invoked with the \`-je\` option.

\`\`\`ignore
[ {
"start_line_number" : 2,
"start_column" : 4,
"end_line_number" : 2,
"end_column" : 8,
"warning" : false,
"error_type" : "PRIMARY KEY cannot be nullable",
"message" : "PRIMARY KEY column 'C' has type INTEGER, which is nullable",
"snippet" : "    2|   c INT PRIMARY KEY\n         ^^^^^\n    3|);\n"
} ]
\`\`\``,
  required: [
    'start_line_number',
    'start_column',
    'end_line_number',
    'end_column',
    'warning',
    'error_type',
    'message'
  ],
  properties: {
    end_column: {
      type: 'integer',
      minimum: 0
    },
    end_line_number: {
      type: 'integer',
      minimum: 0
    },
    error_type: {
      type: 'string'
    },
    message: {
      type: 'string'
    },
    snippet: {
      type: 'string',
      nullable: true
    },
    start_column: {
      type: 'integer',
      minimum: 0
    },
    start_line_number: {
      type: 'integer',
      minimum: 0
    },
    warning: {
      type: 'boolean'
    }
  }
} as const

export const $SqlIdentifier = {
  type: 'object',
  description: `An SQL identifier.

This struct is used to represent SQL identifiers in a canonical form.
We store table names or field names as identifiers in the schema.`,
  required: ['name', 'case_sensitive'],
  properties: {
    case_sensitive: {
      type: 'boolean'
    },
    name: {
      type: 'string'
    }
  }
} as const

export const $SqlType = {
  oneOf: [
    {
      type: 'string',
      description: 'SQL `BOOLEAN` type.',
      enum: ['BOOLEAN']
    },
    {
      type: 'string',
      description: 'SQL `TINYINT` type.',
      enum: ['TINYINT']
    },
    {
      type: 'string',
      description: 'SQL `SMALLINT` or `INT2` type.',
      enum: ['SMALLINT']
    },
    {
      type: 'string',
      description: 'SQL `INTEGER`, `INT`, `SIGNED`, `INT4` type.',
      enum: ['INTEGER']
    },
    {
      type: 'string',
      description: 'SQL `BIGINT` or `INT64` type.',
      enum: ['BIGINT']
    },
    {
      type: 'string',
      description: 'SQL `REAL` or `FLOAT4` or `FLOAT32` type.',
      enum: ['REAL']
    },
    {
      type: 'string',
      description: 'SQL `DOUBLE` or `FLOAT8` or `FLOAT64` type.',
      enum: ['DOUBLE']
    },
    {
      type: 'string',
      description: 'SQL `DECIMAL` or `DEC` or `NUMERIC` type.',
      enum: ['DECIMAL']
    },
    {
      type: 'string',
      description: 'SQL `CHAR(n)` or `CHARACTER(n)` type.',
      enum: ['CHAR']
    },
    {
      type: 'string',
      description: 'SQL `VARCHAR`, `CHARACTER VARYING`, `TEXT`, or `STRING` type.',
      enum: ['VARCHAR']
    },
    {
      type: 'string',
      description: 'SQL `BINARY(n)` type.',
      enum: ['BINARY']
    },
    {
      type: 'string',
      description: 'SQL `VARBINARY` or `BYTEA` type.',
      enum: ['VARBINARY']
    },
    {
      type: 'string',
      description: 'SQL `TIME` type.',
      enum: ['TIME']
    },
    {
      type: 'string',
      description: 'SQL `DATE` type.',
      enum: ['DATE']
    },
    {
      type: 'string',
      description: 'SQL `TIMESTAMP` type.',
      enum: ['TIMESTAMP']
    },
    {
      type: 'object',
      required: ['Interval'],
      properties: {
        Interval: {
          $ref: '#/components/schemas/IntervalUnit'
        }
      }
    },
    {
      type: 'string',
      description: 'SQL `ARRAY` type.',
      enum: ['ARRAY']
    },
    {
      type: 'string',
      description: 'A complex SQL struct type (`CREATE TYPE x ...`).',
      enum: ['STRUCT']
    },
    {
      type: 'string',
      description: 'SQL `MAP` type.',
      enum: ['MAP']
    },
    {
      type: 'string',
      description: 'SQL `NULL` type.',
      enum: ['NULL']
    },
    {
      type: 'string',
      description: 'SQL `VARIANT` type.',
      enum: ['VARIANT']
    }
  ],
  description: 'The available SQL types as specified in `CREATE` statements.'
} as const

export const $StorageCacheConfig = {
  type: 'string',
  description: 'How to cache access to storage within a Feldera pipeline.',
  enum: ['page_cache', 'feldera_cache']
} as const

export const $StorageConfig = {
  type: 'object',
  description: 'Configuration for persistent storage in a [`PipelineConfig`].',
  required: ['path'],
  properties: {
    cache: {
      $ref: '#/components/schemas/StorageCacheConfig'
    },
    path: {
      type: 'string',
      description: `The location where the pipeline state is stored or will be stored.

It should point to a path on the file-system of the machine/container
where the pipeline will run. If that path doesn't exist yet, or if it
does not contain any checkpoints, then the pipeline creates it and
starts from an initial state in which no data has yet been received. If
it does exist, then the pipeline starts from the most recent checkpoint
that already exists there. In either case, (further) checkpoints will be
written there.`
    }
  }
} as const

export const $TransportConfig = {
  oneOf: [
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileInputConfig'
        },
        name: {
          type: 'string',
          enum: ['file_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['file_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/KafkaInputConfig'
        },
        name: {
          type: 'string',
          enum: ['kafka_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/KafkaOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['kafka_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/PubSubInputConfig'
        },
        name: {
          type: 'string',
          enum: ['pub_sub_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/UrlInputConfig'
        },
        name: {
          type: 'string',
          enum: ['url_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/S3InputConfig'
        },
        name: {
          type: 'string',
          enum: ['s3_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DeltaTableReaderConfig'
        },
        name: {
          type: 'string',
          enum: ['delta_table_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DeltaTableWriterConfig'
        },
        name: {
          type: 'string',
          enum: ['delta_table_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DatagenInputConfig'
        },
        name: {
          type: 'string',
          enum: ['datagen']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/NexmarkInputConfig'
        },
        name: {
          type: 'string',
          enum: ['nexmark']
        }
      }
    },
    {
      type: 'object',
      required: ['name'],
      properties: {
        name: {
          type: 'string',
          enum: ['http_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name'],
      properties: {
        name: {
          type: 'string',
          enum: ['http_output']
        }
      }
    }
  ],
  description: `Transport-specific endpoint configuration passed to
\`crate::OutputTransport::new_endpoint\`
and \`crate::InputTransport::new_endpoint\`.`,
  discriminator: {
    propertyName: 'name'
  }
} as const

export const $UrlInputConfig = {
  type: 'object',
  description: `Configuration for reading data from an HTTP or HTTPS URL with
\`UrlInputTransport\`.`,
  required: ['path'],
  properties: {
    path: {
      type: 'string',
      description: 'URL.'
    },
    pause_timeout: {
      type: 'integer',
      format: 'int32',
      description: `Timeout before disconnection when paused, in seconds.

If the pipeline is paused, or if the input adapter reads data faster
than the pipeline can process it, then the controller will pause the
input adapter. If the input adapter stays paused longer than this
timeout, it will drop the network connection to the server. It will
automatically reconnect when the input adapter starts running again.`,
      minimum: 0
    }
  }
} as const

export const $Version = {
  type: 'integer',
  format: 'int64',
  description: 'Version number.'
} as const
