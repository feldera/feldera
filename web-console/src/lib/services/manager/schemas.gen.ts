// This file is auto-generated by @hey-api/openapi-ts

export const $AdHocInputConfig = {
  type: 'object',
  description: `Configuration for inserting data with ad-hoc queries

An ad-hoc input adapters cannot be usefully configured as part of pipeline
configuration.  Instead, use ad-hoc queries through the UI, the REST API, or
the \`fda\` command-line tool.`,
  required: ['name'],
  properties: {
    name: {
      type: 'string',
      description: 'Autogenerated name.'
    }
  }
} as const

export const $AdHocResultFormat = {
  type: 'string',
  description: 'URL-encoded `format` argument to the `/query` endpoint.',
  enum: ['text', 'json', 'parquet', 'arrow_ipc']
} as const

export const $AdhocQueryArgs = {
  type: 'object',
  description: 'URL-encoded arguments to the `/query` endpoint.',
  required: ['sql'],
  properties: {
    format: {
      $ref: '#/components/schemas/AdHocResultFormat'
    },
    sql: {
      type: 'string',
      description: 'The SQL query to run.'
    }
  }
} as const

export const $ApiKeyDescr = {
  type: 'object',
  description: 'API key descriptor.',
  required: ['id', 'name', 'scopes'],
  properties: {
    id: {
      $ref: '#/components/schemas/ApiKeyId'
    },
    name: {
      type: 'string'
    },
    scopes: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/ApiPermission'
      }
    }
  }
} as const

export const $ApiKeyId = {
  type: 'string',
  format: 'uuid',
  description: 'API key identifier.'
} as const

export const $ApiPermission = {
  type: 'string',
  description: 'Permission types for invoking API endpoints.',
  enum: ['Read', 'Write']
} as const

export const $AuthProvider = {
  oneOf: [
    {
      type: 'object',
      required: ['AwsCognito'],
      properties: {
        AwsCognito: {
          $ref: '#/components/schemas/ProviderAwsCognito'
        }
      }
    },
    {
      type: 'object',
      required: ['GoogleIdentity'],
      properties: {
        GoogleIdentity: {
          $ref: '#/components/schemas/ProviderGoogleIdentity'
        }
      }
    }
  ]
} as const

export const $Chunk = {
  type: 'object',
  description: `A set of updates to a SQL table or view.

The \`sequence_number\` field stores the offset of the chunk relative to the
start of the stream and can be used to implement reliable delivery.
The payload is stored in the \`bin_data\`, \`text_data\`, or \`json_data\` field
depending on the data format used.`,
  required: ['sequence_number'],
  properties: {
    bin_data: {
      type: 'string',
      format: 'binary',
      description: 'Base64 encoded binary payload, e.g., bincode.',
      nullable: true
    },
    json_data: {
      type: 'object',
      description: 'JSON payload.',
      nullable: true
    },
    sequence_number: {
      type: 'integer',
      format: 'int64',
      minimum: 0
    },
    text_data: {
      type: 'string',
      description: 'Text payload, e.g., CSV.',
      nullable: true
    }
  }
} as const

export const $ColumnType = {
  type: 'object',
  description: `A SQL column type description.

Matches the Calcite JSON format.`,
  required: ['nullable'],
  properties: {
    component: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    },
    fields: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Field'
      },
      description: `The fields of the type (if available).

For example this would specify the fields of a \`CREATE TYPE\` construct.

\`\`\`sql
CREATE TYPE person_typ AS (
firstname       VARCHAR(30),
lastname        VARCHAR(30),
address         ADDRESS_TYP
);
\`\`\`

Would lead to the following \`fields\` value:

\`\`\`sql
[
ColumnType { name: "firstname, ... },
ColumnType { name: "lastname", ... },
ColumnType { name: "address", fields: [ ... ] }
]
\`\`\``,
      nullable: true
    },
    key: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    },
    nullable: {
      type: 'boolean',
      description: 'Does the type accept NULL values?'
    },
    precision: {
      type: 'integer',
      format: 'int64',
      description: `Precision of the type.

# Examples
- \`VARCHAR\` sets precision to \`-1\`.
- \`VARCHAR(255)\` sets precision to \`255\`.
- \`BIGINT\`, \`DATE\`, \`FLOAT\`, \`DOUBLE\`, \`GEOMETRY\`, etc. sets precision
to None
- \`TIME\`, \`TIMESTAMP\` set precision to \`0\`.`,
      nullable: true
    },
    scale: {
      type: 'integer',
      format: 'int64',
      description: `The scale of the type.

# Example
- \`DECIMAL(1,2)\` sets scale to \`2\`.`,
      nullable: true
    },
    type: {
      $ref: '#/components/schemas/SqlType'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    }
  }
} as const

export const $CompilationProfile = {
  type: 'string',
  description: `Enumeration of possible compilation profiles that can be passed to the Rust compiler
as an argument via \`cargo build --profile <>\`. A compilation profile affects among
other things the compilation speed (how long till the program is ready to be run)
and runtime speed (the performance while running).`,
  enum: ['dev', 'unoptimized', 'optimized']
} as const

export const $CompletionStatus = {
  type: 'string',
  description: 'Completion token status returned by the `/completion_status` endpoint.',
  enum: ['complete', 'inprogress']
} as const

export const $CompletionStatusArgs = {
  type: 'object',
  description: 'URL-encoded arguments to the `/completion_status` endpoint.',
  required: ['token'],
  properties: {
    token: {
      type: 'string',
      description: `Completion token returned by the \`/completion_token\` or \`/ingress\`
endpoint.`
    }
  }
} as const

export const $CompletionStatusResponse = {
  type: 'object',
  description: 'Response to a completion token status request.',
  required: ['status'],
  properties: {
    status: {
      $ref: '#/components/schemas/CompletionStatus'
    }
  }
} as const

export const $CompletionTokenResponse = {
  type: 'object',
  description: 'Response to a completion token creation request.',
  required: ['token'],
  properties: {
    token: {
      type: 'string',
      description: `Completion token.

An opaque string associated with the current position in the input stream
generated by an input connector.
Pass this string to the \`/completion_status\` endpoint to check whether all
inputs associated with the token have been fully processed by the pipeline.`
    }
  }
} as const

export const $Configuration = {
  type: 'object',
  required: ['telemetry', 'edition', 'version', 'revision', 'changelog_url'],
  properties: {
    changelog_url: {
      type: 'string',
      description: 'URL that navigates to the changelog of the current version'
    },
    edition: {
      type: 'string',
      description: 'Feldera edition: "Open source" or "Enterprise"'
    },
    license_info: {
      allOf: [
        {
          $ref: '#/components/schemas/LicenseInformation'
        }
      ],
      nullable: true
    },
    revision: {
      type: 'string',
      description: `Specific revision corresponding to the edition \`version\` (e.g., git commit hash).
This is an empty string if it is unspecified.`
    },
    telemetry: {
      type: 'string',
      description: 'Telemetry key.'
    },
    update_info: {
      allOf: [
        {
          $ref: '#/components/schemas/UpdateInformation'
        }
      ],
      nullable: true
    },
    version: {
      type: 'string',
      description: `The version corresponding to the type of \`edition\`.
Format is \`x.y.z\`.`
    }
  }
} as const

export const $ConnectorConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/OutputBufferConfig'
    },
    {
      type: 'object',
      required: ['transport'],
      properties: {
        format: {
          allOf: [
            {
              $ref: '#/components/schemas/FormatConfig'
            }
          ],
          nullable: true
        },
        index: {
          type: 'string',
          description: `Name of the index that the connector is attached to.

This property is valid for output connectors only.  It is used with data
transports and formats that expect output updates in the form of key/value
pairs, where the key typically represents a unique id associated with the
table or view.

To support such output formats, an output connector can be attached to an
index created using the SQL CREATE INDEX statement.  An index of a table
or view contains the same updates as the table or view itself, indexed by
one or more key columns.

See individual connector documentation for details on how they work
with indexes.`,
          nullable: true
        },
        labels: {
          type: 'array',
          items: {
            type: 'string'
          },
          description: `Arbitrary user-defined text labels associated with the connector.

These labels can be used in conjunction with the \`start_after\` property
to control the start order of connectors.`
        },
        max_batch_size: {
          type: 'integer',
          format: 'int64',
          description: `Maximum batch size, in records.

This is the maximum number of records to process in one batch through
the circuit.  The time and space cost of processing a batch is
asymptotically superlinear in the size of the batch, but very small
batches are less efficient due to constant factors.

This should usually be less than \`max_queued_records\`, to give the
connector a round-trip time to restart and refill the buffer while
batches are being processed.

Some input adapters might not honor this setting.

The default is 10,000.`,
          minimum: 0
        },
        max_queued_records: {
          type: 'integer',
          format: 'int64',
          description: `Backpressure threshold.

Maximal number of records queued by the endpoint before the endpoint
is paused by the backpressure mechanism.

For input endpoints, this setting bounds the number of records that have
been received from the input transport but haven't yet been consumed by
the circuit since the circuit, since the circuit is still busy processing
previous inputs.

For output endpoints, this setting bounds the number of records that have
been produced by the circuit but not yet sent via the output transport endpoint
nor stored in the output buffer (see \`enable_output_buffer\`).

Note that this is not a hard bound: there can be a small delay between
the backpressure mechanism is triggered and the endpoint is paused, during
which more data may be queued.

The default is 1 million.`,
          minimum: 0
        },
        paused: {
          type: 'boolean',
          description: `Create connector in paused state.

The default is \`false\`.`
        },
        start_after: {
          type: 'array',
          items: {
            type: 'string'
          },
          description: `Start the connector after all connectors with specified labels.

This property is used to control the start order of connectors.
The connector will not start until all connectors with the specified
labels have finished processing all inputs.`,
          nullable: true
        },
        transport: {
          $ref: '#/components/schemas/TransportConfig'
        }
      }
    }
  ],
  description: "A data connector's configuration"
} as const

export const $DatagenInputConfig = {
  type: 'object',
  description: 'Configuration for generating random data for a table.',
  properties: {
    plan: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/GenerationPlan'
      },
      description: `The sequence of generations to perform.

If not set, the generator will produce a single sequence with default settings.
If set, the generator will produce the specified sequences in sequential order.

Note that if one of the sequences before the last one generates an unlimited number of rows
the following sequences will not be executed.`,
      default: [
        {
          rate: null,
          limit: null,
          worker_chunk_size: null,
          fields: {}
        }
      ]
    },
    seed: {
      type: 'integer',
      format: 'int64',
      description: `Optional seed for the random generator.

Setting this to a fixed value will make the generator produce the same sequence of records
every time the pipeline is run.

# Notes
- To ensure the set of generated input records is deterministic across multiple runs,
apart from setting a seed, \`workers\` also needs to remain unchanged.
- The input will arrive in non-deterministic order if \`workers > 1\`.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    workers: {
      type: 'integer',
      description: 'Number of workers to use for generating data.',
      default: 1,
      minimum: 0
    }
  },
  additionalProperties: false
} as const

export const $DatagenStrategy = {
  type: 'string',
  description: 'Strategy used to generate values.',
  enum: [
    'increment',
    'uniform',
    'zipf',
    'word',
    'words',
    'sentence',
    'sentences',
    'paragraph',
    'paragraphs',
    'first_name',
    'last_name',
    'title',
    'suffix',
    'name',
    'name_with_title',
    'domain_suffix',
    'email',
    'username',
    'password',
    'field',
    'position',
    'seniority',
    'job_title',
    'ipv4',
    'ipv6',
    'ip',
    'mac_address',
    'user_agent',
    'rfc_status_code',
    'valid_status_code',
    'company_suffix',
    'company_name',
    'buzzword',
    'buzzword_middle',
    'buzzword_tail',
    'catch_phrase',
    'bs_verb',
    'bs_adj',
    'bs_noun',
    'bs',
    'profession',
    'industry',
    'currency_code',
    'currency_name',
    'currency_symbol',
    'credit_card_number',
    'city_prefix',
    'city_suffix',
    'city_name',
    'country_name',
    'country_code',
    'street_suffix',
    'street_name',
    'time_zone',
    'state_name',
    'state_abbr',
    'secondary_address_type',
    'secondary_address',
    'zip_code',
    'post_code',
    'building_number',
    'latitude',
    'longitude',
    'isbn',
    'isbn13',
    'isbn10',
    'phone_number',
    'cell_number',
    'file_path',
    'file_name',
    'file_extension',
    'dir_path'
  ]
} as const

export const $DeltaTableIngestMode = {
  type: 'string',
  description: `Delta table read mode.

Three options are available:

* \`snapshot\` - read a snapshot of the table and stop.

* \`follow\` - continuously ingest changes to the table, starting from a specified version
or timestamp.

* \`snapshot_and_follow\` - read a snapshot of the table before switching to continuous ingestion
mode.`,
  enum: ['snapshot', 'follow', 'snapshot_and_follow', 'cdc']
} as const

export const $DeltaTableReaderConfig = {
  type: 'object',
  description: 'Delta table input connector configuration.',
  required: ['uri', 'mode'],
  properties: {
    cdc_delete_filter: {
      type: 'string',
      description: `A predicate that determines whether the record represents a deletion.

This setting is only valid in the 'cdc' mode. It specifies a predicate applied to
each row in the Delta table to determine whether the row represents a deletion event.
Its value must be a valid Boolean SQL expression that can be used in a query of the
form \`SELECT * from <table> WHERE <cdc_delete_filter>\`.`,
      nullable: true
    },
    cdc_order_by: {
      type: 'string',
      description: `An expression that determines the ordering of updates in the Delta table.

This setting is only valid in the 'cdc' mode. It specifies a predicate applied to
each row in the Delta table to determine the order in which updates in the table should
be applied. Its value must be a valid SQL expression that can be used in a query of the
form \`SELECT * from <table> ORDER BY <cdc_order_by>\`.`,
      nullable: true
    },
    datetime: {
      type: 'string',
      description: `Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
"2024-12-09T16:09:53+00:00".

When this option is set, the connector finds and opens the version of the table as of the
specified point in time (based on the server time recorded in the transaction log, not the
event time encoded in the data).  In \`snapshot\` and \`snapshot_and_follow\` modes, it
retrieves the snapshot of this version of the table.  In \`follow\` and \`snapshot_and_follow\`
modes, it follows transaction log records **after** this version.

Note: at most one of \`version\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
      nullable: true
    },
    filter: {
      type: 'string',
      description: `Optional row filter.

When specified, only rows that satisfy the filter condition are read from the delta table.
The condition must be a valid SQL Boolean expression that can be used in
the \`where\` clause of the \`select * from my_table where ...\` query.`,
      nullable: true
    },
    max_concurrent_readers: {
      type: 'integer',
      format: 'int32',
      description: `Maximum number of concurrent object store reads performed by all Delta Lake connectors.

This setting is used to limit the number of concurrent reads of the object store in a
pipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously
reading from the object store, this can lead to transport timeouts.

When enabled, this setting limits the number of concurrent reads across all connectors.
This is a global setting that affects all Delta Lake connectors, and not just the connector
where it is specified. It should therefore be used at most once in a pipeline.  If multiple
connectors specify this setting, they must all use the same value.

The default value is 6.`,
      nullable: true,
      minimum: 0
    },
    mode: {
      $ref: '#/components/schemas/DeltaTableIngestMode'
    },
    num_parsers: {
      type: 'integer',
      format: 'int32',
      description: `The number of parallel parsing tasks the connector uses to process data read from the
table. Increasing this value can enhance performance by allowing more concurrent processing.
Recommended range: 1–10. The default is 4.`,
      minimum: 0
    },
    skip_unused_columns: {
      type: 'boolean',
      description: `Don't read unused columns from the Delta table.

When set to \`true\`, this option instructs the connector to avoid reading
columns from the Delta table that are not used in any view definitions.
To be skipped, the columns must be either nullable or have default
values. This can improve ingestion performance, especially for wide
tables.

Note: The simplest way to exclude unused columns is to omit them from the Feldera SQL table
declaration. The connector never reads columns that aren't declared in the SQL schema.
Additionally, the SQL compiler emits warnings for declared but unused columns—use these as
a guide to optimize your schema.`
    },
    snapshot_filter: {
      type: 'string',
      description: `Optional snapshot filter.

This option is only valid when \`mode\` is set to \`snapshot\` or \`snapshot_and_follow\`.

When specified, only rows that satisfy the filter condition are included in the
snapshot.  The condition must be a valid SQL Boolean expression that can be used in
the \`where\` clause of the \`select * from snapshot where ...\` query.

Unlike the \`filter\` option, which applies to all records retrieved from the table, this
filter only applies to rows in the initial snapshot of the table.
For instance, it can be used to specify the range of event times to include in the snapshot,
e.g.: \`ts BETWEEN TIMESTAMP '2005-01-01 00:00:00' AND TIMESTAMP '2010-12-31 23:59:59'\`.

This option can be used together with the \`filter\` option. During the initial snapshot,
only rows that satisfy both \`filter\` and \`snapshot_filter\` are retrieved from the Delta table.
When subsequently following changes in the the transaction log (\`mode = snapshot_and_follow\`),
all rows that meet the \`filter\` condition are ingested, regardless of \`snapshot_filter\`.`,
      nullable: true
    },
    timestamp_column: {
      type: 'string',
      description: `Table column that serves as an event timestamp.

When this option is specified, and \`mode\` is one of \`snapshot\` or \`snapshot_and_follow\`,
table rows are ingested in the timestamp order, respecting the
[\`LATENESS\`](https://docs.feldera.com/sql/streaming#lateness-expressions)
property of the column: each ingested row has a timestamp no more than \`LATENESS\`
time units earlier than the most recent timestamp of any previously ingested row.
The ingestion is performed by partitioning the table into timestamp ranges of width
\`LATENESS\`. Each range is processed sequentially, in increasing timestamp order.

# Example

Consider a table with timestamp column of type \`TIMESTAMP\` and lateness attribute
\`INTERVAL 1 DAY\`. Assuming that the oldest timestamp in the table is
\`2024-01-01T00:00:00\`\`, the connector will fetch all records with timestamps
from \`2024-01-01\`, then all records for \`2024-01-02\`, \`2024-01-03\`, etc., until all records
in the table have been ingested.

# Requirements

* The timestamp column must be of a supported type: integer, \`DATE\`, or \`TIMESTAMP\`.
* The timestamp column must be declared with non-zero \`LATENESS\`.
* For efficient ingest, the table must be optimized for timestamp-based
queries using partitioning, Z-ordering, or liquid clustering.`,
      nullable: true
    },
    uri: {
      type: 'string',
      description: `Table URI.

Example: "s3://feldera-fraud-detection-data/demographics_train"`
    },
    version: {
      type: 'integer',
      format: 'int64',
      description: `Optional table version.

When this option is set, the connector finds and opens the specified version of the table.
In \`snapshot\` and \`snapshot_and_follow\` modes, it retrieves the snapshot of this version of
the table.  In \`follow\` and \`snapshot_and_follow\` modes, it follows transaction log records
**after** this version.

Note: at most one of \`version\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
      nullable: true
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Storage options for configuring backend object store.

For specific options available for different storage backends, see:
* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)`
  }
} as const

export const $DeltaTableWriteMode = {
  type: 'string',
  description: `Delta table write mode.

Determines how the Delta table connector handles an existing table at the target location.`,
  enum: ['append', 'truncate', 'error_if_exists']
} as const

export const $DeltaTableWriterConfig = {
  type: 'object',
  description: 'Delta table output connector configuration.',
  required: ['uri'],
  properties: {
    mode: {
      $ref: '#/components/schemas/DeltaTableWriteMode'
    },
    uri: {
      type: 'string',
      description: 'Table URI.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Storage options for configuring backend object store.

For specific options available for different storage backends, see:
* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)`
  }
} as const

export const $Demo = {
  type: 'object',
  required: ['name', 'title', 'description', 'program_code', 'udf_rust', 'udf_toml'],
  properties: {
    description: {
      type: 'string',
      description: 'Description of the demo (parsed from SQL preamble).'
    },
    name: {
      type: 'string',
      description: 'Name of the demo (parsed from SQL preamble).'
    },
    program_code: {
      type: 'string',
      description: 'Program SQL code.'
    },
    title: {
      type: 'string',
      description: 'Title of the demo (parsed from SQL preamble).'
    },
    udf_rust: {
      type: 'string',
      description: 'User defined function (UDF) Rust code.'
    },
    udf_toml: {
      type: 'string',
      description: 'User defined function (UDF) TOML dependencies.'
    }
  }
} as const

export const $DisplaySchedule = {
  oneOf: [
    {
      type: 'string',
      description: 'Display it only once: after dismissal do not show it again',
      enum: ['Once']
    },
    {
      type: 'string',
      description: 'Display it again the next session if it is dismissed',
      enum: ['Session']
    },
    {
      type: 'object',
      required: ['Every'],
      properties: {
        Every: {
          type: 'object',
          description: 'Display it again after a certain period of time after it is dismissed',
          required: ['seconds'],
          properties: {
            seconds: {
              type: 'integer',
              format: 'int64',
              minimum: 0
            }
          }
        }
      }
    },
    {
      type: 'string',
      description: 'Always display it, do not allow it to be dismissed',
      enum: ['Always']
    }
  ]
} as const

export const $ErrorResponse = {
  type: 'object',
  description: 'Information returned by REST API endpoints on error.',
  required: ['message', 'error_code', 'details'],
  properties: {
    details: {
      type: 'object',
      description: `Detailed error metadata.
The contents of this field is determined by \`error_code\`.`
    },
    error_code: {
      type: 'string',
      description: 'Error code is a string that specifies this error type.',
      example: 'CodeSpecifyingErrorType'
    },
    message: {
      type: 'string',
      description: 'Human-readable error message.',
      example: 'Explanation of the error that occurred.'
    }
  }
} as const

export const $Field = {
  allOf: [
    {
      $ref: '#/components/schemas/SqlIdentifier'
    },
    {
      type: 'object',
      required: ['columntype', 'unused'],
      properties: {
        columntype: {
          $ref: '#/components/schemas/ColumnType'
        },
        default: {
          type: 'string',
          nullable: true
        },
        lateness: {
          type: 'string',
          nullable: true
        },
        unused: {
          type: 'boolean'
        },
        watermark: {
          type: 'string',
          nullable: true
        }
      }
    }
  ],
  description: `A SQL field.

Matches the SQL compiler JSON format.`
} as const

export const $FileInputConfig = {
  type: 'object',
  description: 'Configuration for reading data from a file with `FileInputTransport`',
  required: ['path'],
  properties: {
    buffer_size_bytes: {
      type: 'integer',
      description: `Read buffer size.

Default: when this parameter is not specified, a platform-specific
default is used.`,
      nullable: true,
      minimum: 0
    },
    follow: {
      type: 'boolean',
      description: `Enable file following.

When \`false\`, the endpoint outputs an \`InputConsumer::eoi\`
message and stops upon reaching the end of file.  When \`true\`, the
endpoint will keep watching the file and outputting any new content
appended to it.`
    },
    path: {
      type: 'string',
      description: 'File path.'
    }
  }
} as const

export const $FileOutputConfig = {
  type: 'object',
  description: 'Configuration for writing data to a file with `FileOutputTransport`.',
  required: ['path'],
  properties: {
    path: {
      type: 'string',
      description: 'File path.'
    }
  }
} as const

export const $FormatConfig = {
  type: 'object',
  description: `Data format specification used to parse raw data received from the
endpoint or to encode data sent to the endpoint.`,
  required: ['name'],
  properties: {
    config: {
      type: 'object',
      description: 'Format-specific parser or encoder configuration.'
    },
    name: {
      type: 'string',
      description: 'Format name, e.g., "csv", "json", "bincode", etc.'
    }
  }
} as const

export const $FtConfig = {
  type: 'object',
  description: `Fault-tolerance configuration.

The default [FtConfig] (via [FtConfig::default]) disables fault tolerance,
which is the configuration that one gets if [RuntimeConfig] omits fault
tolerance configuration.

The default value for [FtConfig::model] enables fault tolerance, as
\`Some(FtModel::default())\`.  This is the configuration that one gets if
[RuntimeConfig] includes a fault tolerance configuration but does not
specify a particular model.`,
  properties: {
    checkpoint_interval_secs: {
      type: 'integer',
      format: 'int64',
      description: `Interval between automatic checkpoints, in seconds.

The default is 60 seconds.  Values less than 1 or greater than 3600 will
be forced into that range.`,
      minimum: 0
    },
    model: {
      oneOf: [
        {
          $ref: '#/components/schemas/FtModel'
        },
        {
          type: 'string',
          enum: ['none']
        }
      ],
      default: 'exactly_once'
    }
  }
} as const

export const $FtModel = {
  type: 'string',
  description: `Fault tolerance model.

The ordering is significant: we consider [Self::ExactlyOnce] to be a "higher
level" of fault tolerance than [Self::AtLeastOnce].`,
  enum: ['at_least_once', 'exactly_once']
} as const

export const $GenerationPlan = {
  type: 'object',
  description:
    'A random generation plan for a table that generates either a limited amount of rows or runs continuously.',
  properties: {
    fields: {
      type: 'object',
      description: 'Specifies the values that the generator should produce.',
      default: {},
      additionalProperties: {
        $ref: '#/components/schemas/RngFieldSettings'
      }
    },
    limit: {
      type: 'integer',
      description: `Total number of new rows to generate.

If not set, the generator will produce new/unique records as long as the pipeline is running.
If set to 0, the table will always remain empty.
If set, the generator will produce new records until the specified limit is reached.

Note that if the table has one or more primary keys that don't use the \`increment\` strategy to
generate the key there is a potential that an update is generated instead of an insert. In
this case it's possible the total number of records is less than the specified limit.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    rate: {
      type: 'integer',
      format: 'int32',
      description: `Non-zero number of rows to generate per second.

If not set, the generator will produce rows as fast as possible.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    worker_chunk_size: {
      type: 'integer',
      description: `When multiple workers are used, each worker will pick a consecutive "chunk" of
records to generate.

By default, if not specified, the generator will use the formula \`min(rate, 10_000)\`
to determine it. This works well in most situations. However, if you're
running tests with lateness and many workers you can e.g., reduce the
chunk size to make sure a smaller range of records is being ingested in parallel.

# Example
Assume you generate a total of 125 records with 4 workers and a chunk size of 25.
In this case, worker A will generate records 0..25, worker B will generate records 25..50,
etc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk
will pick up the last chunk of records (100..125) to generate.`,
      default: null,
      nullable: true,
      minimum: 0
    }
  },
  additionalProperties: false
} as const

export const $GetPipelineParameters = {
  type: 'object',
  description: 'Query parameters to GET a pipeline or a list of pipelines.',
  properties: {
    selector: {
      $ref: '#/components/schemas/PipelineFieldSelector'
    }
  }
} as const

export const $GlueCatalogConfig = {
  type: 'object',
  description: 'AWS Glue catalog config.',
  properties: {
    'glue.access-key-id': {
      type: 'string',
      description: 'Access key id used to access the Glue catalog.',
      nullable: true
    },
    'glue.endpoint': {
      type: 'string',
      description: `Configure an alternative endpoint of the Glue service for Glue catalog to access.

Example: \`"https://glue.us-east-1.amazonaws.com"\``,
      nullable: true
    },
    'glue.id': {
      type: 'string',
      description: 'The 12-digit ID of the Glue catalog.',
      nullable: true
    },
    'glue.profile-name': {
      type: 'string',
      description: 'Profile used to access the Glue catalog.',
      nullable: true
    },
    'glue.region': {
      type: 'string',
      description: 'Region of the Glue catalog.',
      nullable: true
    },
    'glue.secret-access-key': {
      type: 'string',
      description: 'Secret access key used to access the Glue catalog.',
      nullable: true
    },
    'glue.session-token': {
      type: 'string',
      nullable: true
    },
    'glue.warehouse': {
      type: 'string',
      description: `Location for table metadata.

Example: \`"s3://my-data-warehouse/tables/"\``,
      nullable: true
    }
  }
} as const

export const $HttpInputConfig = {
  type: 'object',
  description: `Configuration for reading data via HTTP.

HTTP input adapters cannot be usefully configured as part of pipeline
configuration.  Instead, instantiate them through the REST API as
\`/pipelines/{pipeline_name}/ingress/{table_name}\`.`,
  required: ['name'],
  properties: {
    name: {
      type: 'string',
      description: 'Autogenerated name.'
    }
  }
} as const

export const $IcebergCatalogType = {
  type: 'string',
  enum: ['rest', 'glue']
} as const

export const $IcebergIngestMode = {
  type: 'string',
  description: `Iceberg table read mode.

Three options are available:

* \`snapshot\` - read a snapshot of the table and stop.

* \`follow\` - continuously ingest changes to the table, starting from a specified snapshot
or timestamp.

* \`snapshot_and_follow\` - read a snapshot of the table before switching to continuous ingestion
mode.`,
  enum: ['snapshot', 'follow', 'snapshot_and_follow']
} as const

export const $IcebergReaderConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/GlueCatalogConfig'
    },
    {
      $ref: '#/components/schemas/RestCatalogConfig'
    },
    {
      type: 'object',
      required: ['mode'],
      properties: {
        catalog_type: {
          allOf: [
            {
              $ref: '#/components/schemas/IcebergCatalogType'
            }
          ],
          nullable: true
        },
        datetime: {
          type: 'string',
          description: `Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
"2024-12-09T16:09:53+00:00".

When this option is set, the connector finds and opens the snapshot of the table as of the
specified point in time (based on the server time recorded in the transaction
log, not the event time encoded in the data).  In \`snapshot\` and \`snapshot_and_follow\`
modes, it retrieves this snapshot.  In \`follow\` and \`snapshot_and_follow\` modes, it
follows transaction log records **after** this snapshot.

Note: at most one of \`snapshot_id\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
          nullable: true
        },
        metadata_location: {
          type: 'string',
          description: `Location of the table metadata JSON file.

This propery is used to access an Iceberg table without a catalog. It is mutually
exclusive with the \`catalog_type\` property.`,
          nullable: true
        },
        mode: {
          $ref: '#/components/schemas/IcebergIngestMode'
        },
        snapshot_filter: {
          type: 'string',
          description: `Optional row filter.

This option is only valid when \`mode\` is set to \`snapshot\` or \`snapshot_and_follow\`.

When specified, only rows that satisfy the filter condition are included in the
snapshot.  The condition must be a valid SQL Boolean expression that can be used in
the \`where\` clause of the \`select * from snapshot where ...\` query.

This option can be used to specify the range of event times to include in the snapshot,
e.g.: \`ts BETWEEN '2005-01-01 00:00:00' AND '2010-12-31 23:59:59'\`.`,
          nullable: true
        },
        snapshot_id: {
          type: 'integer',
          format: 'int64',
          description: `Optional snapshot id.

When this option is set, the connector finds the specified snapshot of the table.
In \`snapshot\` and \`snapshot_and_follow\` modes, it loads this snapshot.
In \`follow\` and \`snapshot_and_follow\` modes, it follows table updates
**after** this snapshot.

Note: at most one of \`snapshot_id\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
          nullable: true
        },
        table_name: {
          type: 'string',
          description: `Specifies the Iceberg table name in the "namespace.table" format.

This option is applicable when an Iceberg catalog is configured using the \`catalog_type\` property.`,
          nullable: true
        },
        timestamp_column: {
          type: 'string',
          description: `Table column that serves as an event timestamp.

When this option is specified, and \`mode\` is one of \`snapshot\` or \`snapshot_and_follow\`,
table rows are ingested in the timestamp order, respecting the
[\`LATENESS\`](https://docs.feldera.com/sql/streaming#lateness-expressions)
property of the column: each ingested row has a timestamp no more than \`LATENESS\`
time units earlier than the most recent timestamp of any previously ingested row.
The ingestion is performed by partitioning the table into timestamp ranges of width
\`LATENESS\`. Each range is processed sequentially, in increasing timestamp order.

# Example

Consider a table with timestamp column of type \`TIMESTAMP\` and lateness attribute
\`INTERVAL 1 DAY\`. Assuming that the oldest timestamp in the table is
\`2024-01-01T00:00:00\`\`, the connector will fetch all records with timestamps
from \`2024-01-01\`, then all records for \`2024-01-02\`, \`2024-01-03\`, etc., until all records
in the table have been ingested.

# Requirements

* The timestamp column must be of a supported type: integer, \`DATE\`, or \`TIMESTAMP\`.
* The timestamp column must be declared with non-zero \`LATENESS\`.
* For efficient ingest, the table must be optimized for timestamp-based
queries using partitioning, Z-ordering, or liquid clustering.`,
          nullable: true
        }
      },
      additionalProperties: {
        type: 'string',
        description: `Storage options for configuring backend object store.

See the [list of available options in PyIceberg documentation](https://py.iceberg.apache.org/configuration/#fileio).`
      }
    }
  ],
  description: 'Iceberg input connector configuration.'
} as const

export const $InputEndpointConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    {
      type: 'object',
      required: ['stream'],
      properties: {
        stream: {
          type: 'string',
          description: `The name of the input stream of the circuit that this endpoint is
connected to.`
        }
      }
    }
  ],
  description: 'Describes an input connector configuration'
} as const

export const $IntervalUnit = {
  type: 'string',
  description: `The specified units for SQL Interval types.

\`INTERVAL 1 DAY\`, \`INTERVAL 1 DAY TO HOUR\`, \`INTERVAL 1 DAY TO MINUTE\`,
would yield \`Day\`, \`DayToHour\`, \`DayToMinute\`, as the \`IntervalUnit\` respectively.`,
  enum: [
    'Day',
    'DayToHour',
    'DayToMinute',
    'DayToSecond',
    'Hour',
    'HourToMinute',
    'HourToSecond',
    'Minute',
    'MinuteToSecond',
    'Month',
    'Second',
    'Year',
    'YearToMonth'
  ]
} as const

export const $JsonLines = {
  type: 'string',
  description: 'Whether JSON values can span multiple lines.',
  enum: ['multiple', 'single']
} as const

export const $JsonUpdateFormat = {
  type: 'string',
  description: `Supported JSON data change event formats.

Each element in a JSON-formatted input stream specifies
an update to one or more records in an input table.  We support
several different ways to represent such updates.

### \`InsertDelete\`

Each element in the input stream consists of an "insert" or "delete"
command and a record to be inserted to or deleted from the input table.

\`\`\`json
{"insert": {"column1": "hello, world!", "column2": 100}}
\`\`\`

### \`Weighted\`

Each element in the input stream consists of a record and a weight
which indicates how many times the row appears.

\`\`\`json
{"weight": 2, "data": {"column1": "hello, world!", "column2": 100}}
\`\`\`

Note that the line above would be equivalent to the following input in the \`InsertDelete\` format:

\`\`\`json
{"insert": {"column1": "hello, world!", "column2": 100}}
{"insert": {"column1": "hello, world!", "column2": 100}}
\`\`\`

Similarly, negative weights are equivalent to deletions:

\`\`\`json
{"weight": -1, "data": {"column1": "hello, world!", "column2": 100}}
\`\`\`

is equivalent to in the \`InsertDelete\` format:

\`\`\`json
{"delete": {"column1": "hello, world!", "column2": 100}}
\`\`\`

### \`Debezium\`

Debezium CDC format.  Refer to [Debezium input connector documentation](https://docs.feldera.com/connectors/sources/debezium) for details.

### \`Snowflake\`

Uses flat structure so that fields can get parsed directly into SQL
columns.  Defines three metadata fields:

* \`__action\` - "insert" or "delete"
* \`__stream_id\` - unique 64-bit ID of the output stream (records within
a stream are totally ordered)
* \`__seq_number\` - monotonically increasing sequence number relative to
the start of the stream.

\`\`\`json
{"PART":1,"VENDOR":2,"EFFECTIVE_SINCE":"2019-05-21","PRICE":"10000","__action":"insert","__stream_id":4523666124030717756,"__seq_number":1}
\`\`\`

### \`Raw\`

This format is suitable for insert-only streams (no deletions).
Each element in the input stream contains a record without any
additional envelope that gets inserted in the input table.`,
  enum: ['insert_delete', 'weighted', 'debezium', 'snowflake', 'raw', 'redis']
} as const

export const $KafkaHeader = {
  type: 'object',
  description: 'Kafka message header.',
  required: ['key'],
  properties: {
    key: {
      type: 'string'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaHeaderValue'
        }
      ],
      nullable: true
    }
  }
} as const

export const $KafkaHeaderValue = {
  type: 'string',
  format: 'binary',
  description: 'Kafka header value encoded as a UTF-8 string or a byte array.'
} as const

export const $KafkaInputConfig = {
  type: 'object',
  description: 'Configuration for reading data from Kafka topics with `InputTransport`.',
  required: ['topic'],
  properties: {
    group_join_timeout_secs: {
      type: 'integer',
      format: 'int32',
      description: `Maximum timeout in seconds to wait for the endpoint to join the Kafka
consumer group during initialization.`,
      minimum: 0
    },
    log_level: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaLogLevel'
        }
      ],
      nullable: true
    },
    poller_threads: {
      type: 'integer',
      description: `Set to 1 or more to fix the number of threads used to poll
\`rdkafka\`. Multiple threads can increase performance with small Kafka
messages; for large messages, one thread is enough. In either case, too
many threads can harm performance. If unset, the default is 3, which
helps with small messages but will not harm performance with large
messagee`,
      nullable: true,
      minimum: 0
    },
    start_from: {
      $ref: '#/components/schemas/KafkaStartFromConfig'
    },
    topic: {
      type: 'string',
      description: 'Topic to subscribe to.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Options passed directly to \`rdkafka\`.

[\`librdkafka\` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
used to configure the Kafka consumer.

This input connector does not use consumer groups, so options related to
consumer groups are rejected, including:

* \`group.id\`, if present, is ignored.
* \`auto.offset.reset\` (use \`start_from\` instead).
* "enable.auto.commit", if present, must be set to "false".
* "enable.auto.offset.store", if present, must be set to "false".`
  }
} as const

export const $KafkaLogLevel = {
  type: 'string',
  description: 'Kafka logging levels.',
  enum: ['emerg', 'alert', 'critical', 'error', 'warning', 'notice', 'info', 'debug']
} as const

export const $KafkaOutputConfig = {
  type: 'object',
  description: 'Configuration for writing data to a Kafka topic with `OutputTransport`.',
  required: ['topic'],
  properties: {
    fault_tolerance: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaOutputFtConfig'
        }
      ],
      nullable: true
    },
    headers: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/KafkaHeader'
      },
      description: 'Kafka headers to be added to each message produced by this connector.'
    },
    initialization_timeout_secs: {
      type: 'integer',
      format: 'int32',
      description: `Maximum timeout in seconds to wait for the endpoint to connect to
a Kafka broker.

Defaults to 60.`,
      minimum: 0
    },
    kafka_service: {
      type: 'string',
      description: 'If specified, this service is used to provide defaults for the Kafka options.',
      nullable: true
    },
    log_level: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaLogLevel'
        }
      ],
      nullable: true
    },
    topic: {
      type: 'string',
      description: 'Topic to write to.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Options passed directly to \`rdkafka\`.

See [\`librdkafka\` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
used to configure the Kafka producer.`
  }
} as const

export const $KafkaOutputFtConfig = {
  type: 'object',
  description: 'Fault tolerance configuration for Kafka output connector.',
  properties: {
    consumer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for consumers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for consumers, and may be empty.`,
      default: {},
      additionalProperties: {
        type: 'string'
      }
    },
    producer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for producers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for producers, and may be empty.`,
      default: {},
      additionalProperties: {
        type: 'string'
      }
    }
  }
} as const

export const $KafkaStartFromConfig = {
  oneOf: [
    {
      type: 'string',
      description: 'Start from the beginning of the topic.',
      enum: ['earliest']
    },
    {
      type: 'string',
      description: `Start from the current end of the topic.

This will only read any data that is added to the topic after the
connector initializes.`,
      enum: ['latest']
    },
    {
      type: 'object',
      required: ['offsets'],
      properties: {
        offsets: {
          type: 'array',
          items: {
            type: 'integer',
            format: 'int64'
          },
          description: `Start from particular offsets in the topic.

The number of offsets must match the number of partitions in the topic.`
        }
      }
    }
  ],
  description: 'Where to begin reading a Kafka topic.'
} as const

export const $LicenseInformation = {
  type: 'object',
  required: [
    'expires_in_seconds',
    'expires_at',
    'is_expired',
    'is_trial',
    'description_html',
    'extension_url',
    'remind_starting_at',
    'remind_schedule'
  ],
  properties: {
    description_html: {
      type: 'string',
      description:
        'Optional description of the advantages of extending the license / upgrading from a trial'
    },
    expires_at: {
      type: 'string',
      format: 'date-time',
      description: 'Timestamp at which the license expires'
    },
    expires_in_seconds: {
      type: 'integer',
      format: 'int64',
      description: 'Duration until the license expires',
      minimum: 0
    },
    extension_url: {
      type: 'string',
      description: 'URL that navigates the user to extend / upgrade their license'
    },
    is_expired: {
      type: 'boolean',
      description: 'Whether the license is expired'
    },
    is_trial: {
      type: 'boolean',
      description: 'Whether the license is a trial'
    },
    remind_schedule: {
      $ref: '#/components/schemas/DisplaySchedule'
    },
    remind_starting_at: {
      type: 'string',
      format: 'date-time',
      description: 'Timestamp from which the user should be reminded of the license expiring soon'
    }
  }
} as const

export const $MetricsFormat = {
  type: 'string',
  description: `Circuit metrics output format.
- \`prometheus\`: [format](https://github.com/prometheus/docs/blob/main/content/docs/instrumenting/exposition_formats.md) expected by Prometheus
- \`json\`: JSON format`,
  enum: ['prometheus', 'json']
} as const

export const $MetricsParameters = {
  type: 'object',
  description: 'Query parameters to retrieve pipeline circuit metrics.',
  properties: {
    format: {
      $ref: '#/components/schemas/MetricsFormat'
    }
  }
} as const

export const $NewApiKeyRequest = {
  type: 'object',
  description: 'Request to create a new API key.',
  required: ['name'],
  properties: {
    name: {
      type: 'string',
      description: 'Key name.',
      example: 'my-api-key'
    }
  }
} as const

export const $NewApiKeyResponse = {
  type: 'object',
  description: 'Response to a successful API key creation.',
  required: ['id', 'name', 'api_key'],
  properties: {
    api_key: {
      type: 'string',
      description: `Generated secret API key. There is no way to retrieve this
key again through the API, so store it securely.`,
      example:
        'apikey:v5y5QNtlPNVMwkmNjKwFU8bbIu5lMge3yHbyddxAOdXlEo84SEoNn32DUhQaf1KLeI9aOOfnJjhQ1pYzMrU4wQXON6pm6BS7Zgzj46U2b8pwz1280vYBEtx41hiDBRP'
    },
    id: {
      $ref: '#/components/schemas/ApiKeyId'
    },
    name: {
      type: 'string',
      description: 'API key name provided by the user.',
      example: 'my-api-key'
    }
  }
} as const

export const $NexmarkInputConfig = {
  type: 'object',
  description: `Configuration for generating Nexmark input data.

This connector must be used exactly three times in a pipeline if it is used
at all, once for each [\`NexmarkTable\`].`,
  required: ['table'],
  properties: {
    options: {
      allOf: [
        {
          $ref: '#/components/schemas/NexmarkInputOptions'
        }
      ],
      nullable: true
    },
    table: {
      $ref: '#/components/schemas/NexmarkTable'
    }
  }
} as const

export const $NexmarkInputOptions = {
  type: 'object',
  description: 'Configuration for generating Nexmark input data.',
  properties: {
    batch_size_per_thread: {
      type: 'integer',
      format: 'int64',
      description: `Number of events to generate and submit together, per thread.

Each thread generates this many records, which are then combined with
the records generated by the other threads, to form combined input
batches of size \`threads × batch_size_per_thread\`.`,
      default: 1000,
      minimum: 0
    },
    events: {
      type: 'integer',
      format: 'int64',
      description: 'Number of events to generate.',
      default: 100000000,
      minimum: 0
    },
    max_step_size_per_thread: {
      type: 'integer',
      format: 'int64',
      description: `Maximum number of events to submit in a single step, per thread.

This should really be per worker thread, not per generator thread, but
the connector does not know how many worker threads there are.

This stands in for \`max_batch_size\` from the connector configuration
because it must be a constant across all three of the nexmark tables.`,
      default: 10000,
      minimum: 0
    },
    threads: {
      type: 'integer',
      description: `Number of event generator threads.

It's reasonable to choose the same number of generator threads as worker
threads.`,
      default: 4,
      minimum: 0
    }
  }
} as const

export const $NexmarkTable = {
  type: 'string',
  description: 'Table in Nexmark.',
  enum: ['bid', 'auction', 'person']
} as const

export const $ObjectStorageConfig = {
  type: 'object',
  required: ['url'],
  properties: {
    url: {
      type: 'string',
      description: `URL.

The following URL schemes are supported:

* S3:
- \`s3://<bucket>/<path>\`
- \`s3a://<bucket>/<path>\`
- \`https://s3.<region>.amazonaws.com/<bucket>\`
- \`https://<bucket>.s3.<region>.amazonaws.com\`
- \`https://ACCOUNT_ID.r2.cloudflarestorage.com/bucket\`
* Google Cloud Storage:
- \`gs://<bucket>/<path>\`
* Microsoft Azure Blob Storage:
- \`abfs[s]://<container>/<path>\` (according to [fsspec](https://github.com/fsspec/adlfs))
- \`abfs[s]://<file_system>@<account_name>.dfs.core.windows.net/<path>\`
- \`abfs[s]://<file_system>@<account_name>.dfs.fabric.microsoft.com/<path>\`
- \`az://<container>/<path>\` (according to [fsspec](https://github.com/fsspec/adlfs))
- \`adl://<container>/<path>\` (according to [fsspec](https://github.com/fsspec/adlfs))
- \`azure://<container>/<path>\` (custom)
- \`https://<account>.dfs.core.windows.net\`
- \`https://<account>.blob.core.windows.net\`
- \`https://<account>.blob.core.windows.net/<container>\`
- \`https://<account>.dfs.fabric.microsoft.com\`
- \`https://<account>.dfs.fabric.microsoft.com/<container>\`
- \`https://<account>.blob.fabric.microsoft.com\`
- \`https://<account>.blob.fabric.microsoft.com/<container>\`

Settings derived from the URL will override other settings.`
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Additional options as key-value pairs.

The following keys are supported:

* S3:
- \`access_key_id\`: AWS Access Key.
- \`secret_access_key\`: AWS Secret Access Key.
- \`region\`: Region.
- \`default_region\`: Default region.
- \`endpoint\`: Custom endpoint for communicating with S3,
e.g. \`https://localhost:4566\` for testing against a localstack
instance.
- \`token\`: Token to use for requests (passed to underlying provider).
- [Other keys](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html#variants).
* Google Cloud Storage:
- \`service_account\`: Path to the service account file.
- \`service_account_key\`: The serialized service account key.
- \`google_application_credentials\`: Application credentials path.
- [Other keys](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html).
* Microsoft Azure Blob Storage:
- \`access_key\`: Azure Access Key.
- \`container_name\`: Azure Container Name.
- \`account\`: Azure Account.
- \`bearer_token_authorization\`: Static bearer token for authorizing requests.
- \`client_id\`: Client ID for use in client secret or Kubernetes federated credential flow.
- \`client_secret\`: Client secret for use in client secret flow.
- \`tenant_id\`: Tenant ID for use in client secret or Kubernetes federated credential flow.
- \`endpoint\`: Override the endpoint for communicating with blob storage.
- [Other keys](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html#variants).

Options set through the URL take precedence over those set with these
options.`
  }
} as const

export const $OutputBufferConfig = {
  type: 'object',
  properties: {
    enable_output_buffer: {
      type: 'boolean',
      description: `Enable output buffering.

The output buffering mechanism allows decoupling the rate at which the pipeline
pushes changes to the output transport from the rate of input changes.

By default, output updates produced by the pipeline are pushed directly to
the output transport. Some destinations may prefer to receive updates in fewer
bigger batches. For instance, when writing Parquet files, producing
one bigger file every few minutes is usually better than creating
small files every few milliseconds.

To achieve such input/output decoupling, users can enable output buffering by
setting the \`enable_output_buffer\` flag to \`true\`.  When buffering is enabled, output
updates produced by the pipeline are consolidated in an internal buffer and are
pushed to the output transport when one of several conditions is satisfied:

* data has been accumulated in the buffer for more than \`max_output_buffer_time_millis\`
milliseconds.
* buffer size exceeds \`max_output_buffer_size_records\` records.

This flag is \`false\` by default.`,
      default: false
    },
    max_output_buffer_size_records: {
      type: 'integer',
      description: `Maximum number of updates to be kept in the output buffer.

This parameter bounds the maximal size of the buffer.
Note that the size of the buffer is not always equal to the
total number of updates output by the pipeline. Updates to the
same record can overwrite or cancel previous updates.

By default, the buffer can grow indefinitely until one of
the other output conditions is satisfied.

NOTE: this configuration option requires the \`enable_output_buffer\` flag
to be set.`,
      default: 18446744073709552000,
      minimum: 0
    },
    max_output_buffer_time_millis: {
      type: 'integer',
      description: `Maximum time in milliseconds data is kept in the output buffer.

By default, data is kept in the buffer indefinitely until one of
the other output conditions is satisfied.  When this option is
set the buffer will be flushed at most every
\`max_output_buffer_time_millis\` milliseconds.

NOTE: this configuration option requires the \`enable_output_buffer\` flag
to be set.`,
      default: 18446744073709552000,
      minimum: 0
    }
  }
} as const

export const $OutputEndpointConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    {
      type: 'object',
      required: ['stream'],
      properties: {
        stream: {
          type: 'string',
          description: `The name of the output stream of the circuit that this endpoint is
connected to.`
        }
      }
    }
  ],
  description: 'Describes an output connector configuration'
} as const

export const $PartialProgramInfo = {
  type: 'object',
  description: 'Program information is the result of the SQL compilation.',
  required: ['schema', 'udf_stubs', 'input_connectors', 'output_connectors'],
  properties: {
    input_connectors: {
      type: 'object',
      description: 'Input connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/InputEndpointConfig'
      }
    },
    output_connectors: {
      type: 'object',
      description: 'Output connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/OutputEndpointConfig'
      }
    },
    schema: {
      $ref: '#/components/schemas/ProgramSchema'
    },
    udf_stubs: {
      type: 'string',
      description: 'Generated user defined function (UDF) stubs Rust code: stubs.rs'
    }
  }
} as const

export const $PatchPipeline = {
  type: 'object',
  description: `Partially update the pipeline (PATCH).

Note that the patching only applies to the main fields, not subfields.
For instance, it is not possible to update only the number of workers;
it is required to again pass the whole runtime configuration with the
change.`,
  properties: {
    description: {
      type: 'string',
      nullable: true
    },
    name: {
      type: 'string',
      nullable: true
    },
    program_code: {
      type: 'string',
      nullable: true
    },
    program_config: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramConfig'
        }
      ],
      nullable: true
    },
    runtime_config: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeConfig'
        }
      ],
      nullable: true
    },
    udf_rust: {
      type: 'string',
      nullable: true
    },
    udf_toml: {
      type: 'string',
      nullable: true
    }
  }
} as const

export const $PipelineConfig = {
  allOf: [
    {
      type: 'object',
      description: `Global pipeline configuration settings. This is the publicly
exposed type for users to configure pipelines.`,
      properties: {
        clock_resolution_usecs: {
          type: 'integer',
          format: 'int64',
          description: `Real-time clock resolution in microseconds.

This parameter controls the execution of queries that use the \`NOW()\` function.  The output of such
queries depends on the real-time clock and can change over time without any external
inputs.  The pipeline will update the clock value and trigger incremental recomputation
at most each \`clock_resolution_usecs\` microseconds.

It is set to 100 milliseconds (100,000 microseconds) by default.

Set to \`null\` to disable periodic clock updates.`,
          default: 100000,
          nullable: true,
          minimum: 0
        },
        cpu_profiler: {
          type: 'boolean',
          description: `Enable CPU profiler.

The default value is \`true\`.`,
          default: true
        },
        fault_tolerance: {
          allOf: [
            {
              $ref: '#/components/schemas/FtConfig'
            }
          ],
          default: {
            model: 'none',
            checkpoint_interval_secs: 60
          }
        },
        max_buffering_delay_usecs: {
          type: 'integer',
          format: 'int64',
          description: `Maximal delay in microseconds to wait for \`min_batch_size_records\` to
get buffered by the controller, defaults to 0.`,
          default: 0,
          minimum: 0
        },
        max_parallel_connector_init: {
          type: 'integer',
          format: 'int64',
          description: `The maximum number of connectors initialized in parallel during pipeline
startup.

At startup, the pipeline must initialize all of its input and output connectors.
Depending on the number and types of connectors, this can take a long time.
To accelerate the process, multiple connectors are initialized concurrently.
This option controls the maximum number of connectors that can be intitialized
in parallel.

The default is 10.`,
          default: null,
          nullable: true,
          minimum: 0
        },
        min_batch_size_records: {
          type: 'integer',
          format: 'int64',
          description: `Minimal input batch size.

The controller delays pushing input records to the circuit until at
least \`min_batch_size_records\` records have been received (total
across all endpoints) or \`max_buffering_delay_usecs\` microseconds
have passed since at least one input records has been buffered.
Defaults to 0.`,
          default: 0,
          minimum: 0
        },
        pin_cpus: {
          type: 'array',
          items: {
            type: 'integer',
            minimum: 0
          },
          description: `Optionally, a list of CPU numbers for CPUs to which the pipeline may pin
its worker threads.  Specify at least twice as many CPU numbers as
workers.  CPUs are generally numbered starting from 0.  The pipeline
might not be able to honor CPU pinning requests.

CPU pinning can make pipelines run faster and perform more consistently,
as long as different pipelines running on the same machine are pinned to
different CPUs.`,
          default: []
        },
        provisioning_timeout_secs: {
          type: 'integer',
          format: 'int64',
          description: `Timeout in seconds for the \`Provisioning\` phase of the pipeline.
Setting this value will override the default of the runner.`,
          default: null,
          nullable: true,
          minimum: 0
        },
        resources: {
          allOf: [
            {
              $ref: '#/components/schemas/ResourceConfig'
            }
          ],
          default: {
            cpu_cores_min: null,
            cpu_cores_max: null,
            memory_mb_min: null,
            memory_mb_max: null,
            storage_mb_max: null,
            storage_class: null
          }
        },
        storage: {
          allOf: [
            {
              $ref: '#/components/schemas/StorageOptions'
            }
          ],
          default: {
            backend: {
              name: 'default'
            },
            min_storage_bytes: null,
            min_step_storage_bytes: null,
            compression: 'default',
            cache_mib: null
          },
          nullable: true
        },
        tracing: {
          type: 'boolean',
          description: 'Enable pipeline tracing.',
          default: false
        },
        tracing_endpoint_jaeger: {
          type: 'string',
          description: 'Jaeger tracing endpoint to send tracing information to.',
          default: '127.0.0.1:6831'
        },
        workers: {
          type: 'integer',
          format: 'int32',
          description: `Number of DBSP worker threads.

Each DBSP "foreground" worker thread is paired with a "background"
thread for LSM merging, making the total number of threads twice the
specified number.`,
          default: 8,
          minimum: 0
        }
      }
    },
    {
      type: 'object',
      required: ['inputs'],
      properties: {
        inputs: {
          type: 'object',
          description: 'Input endpoint configuration.',
          additionalProperties: {
            $ref: '#/components/schemas/InputEndpointConfig'
          }
        },
        name: {
          type: 'string',
          description: 'Pipeline name.',
          nullable: true
        },
        outputs: {
          type: 'object',
          description: 'Output endpoint configuration.',
          additionalProperties: {
            $ref: '#/components/schemas/OutputEndpointConfig'
          }
        },
        storage_config: {
          allOf: [
            {
              $ref: '#/components/schemas/StorageConfig'
            }
          ],
          nullable: true
        }
      }
    }
  ],
  description: `Pipeline deployment configuration.
It represents configuration entries directly provided by the user
(e.g., runtime configuration) and entries derived from the schema
of the compiled program (e.g., connectors). Storage configuration,
if applicable, is set by the runner.`
} as const

export const $PipelineDesiredStatus = {
  type: 'string',
  enum: ['Shutdown', 'Paused', 'Running']
} as const

export const $PipelineFieldSelector = {
  type: 'string',
  enum: ['all', 'status']
} as const

export const $PipelineId = {
  type: 'string',
  format: 'uuid',
  description: 'Pipeline identifier.'
} as const

export const $PipelineInfo = {
  type: 'object',
  description: `Pipeline information.
It both includes fields which are user-provided and system-generated.`,
  required: [
    'id',
    'name',
    'description',
    'created_at',
    'version',
    'platform_version',
    'runtime_config',
    'program_code',
    'udf_rust',
    'udf_toml',
    'program_config',
    'program_version',
    'program_status',
    'program_status_since',
    'program_error',
    'deployment_status',
    'deployment_status_since',
    'deployment_desired_status',
    'refresh_version'
  ],
  properties: {
    created_at: {
      type: 'string',
      format: 'date-time'
    },
    deployment_desired_status: {
      $ref: '#/components/schemas/PipelineDesiredStatus'
    },
    deployment_error: {
      allOf: [
        {
          $ref: '#/components/schemas/ErrorResponse'
        }
      ],
      nullable: true
    },
    deployment_status: {
      $ref: '#/components/schemas/PipelineStatus'
    },
    deployment_status_since: {
      type: 'string',
      format: 'date-time'
    },
    description: {
      type: 'string'
    },
    id: {
      $ref: '#/components/schemas/PipelineId'
    },
    name: {
      type: 'string'
    },
    platform_version: {
      type: 'string'
    },
    program_code: {
      type: 'string'
    },
    program_config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    program_error: {
      $ref: '#/components/schemas/ProgramError'
    },
    program_info: {
      allOf: [
        {
          $ref: '#/components/schemas/PartialProgramInfo'
        }
      ],
      nullable: true
    },
    program_status: {
      $ref: '#/components/schemas/ProgramStatus'
    },
    program_status_since: {
      type: 'string',
      format: 'date-time'
    },
    program_version: {
      $ref: '#/components/schemas/Version'
    },
    refresh_version: {
      $ref: '#/components/schemas/Version'
    },
    runtime_config: {
      $ref: '#/components/schemas/RuntimeConfig'
    },
    udf_rust: {
      type: 'string'
    },
    udf_toml: {
      type: 'string'
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $PipelineSelectedInfo = {
  type: 'object',
  description: `Pipeline information which has a selected subset of optional fields.
It both includes fields which are user-provided and system-generated.
If an optional field is not selected (i.e., is \`None\`), it will not be serialized.`,
  required: [
    'id',
    'name',
    'description',
    'created_at',
    'version',
    'platform_version',
    'program_version',
    'program_status',
    'program_status_since',
    'deployment_status',
    'deployment_status_since',
    'deployment_desired_status',
    'refresh_version'
  ],
  properties: {
    created_at: {
      type: 'string',
      format: 'date-time'
    },
    deployment_desired_status: {
      $ref: '#/components/schemas/PipelineDesiredStatus'
    },
    deployment_error: {
      allOf: [
        {
          $ref: '#/components/schemas/ErrorResponse'
        }
      ],
      nullable: true
    },
    deployment_status: {
      $ref: '#/components/schemas/PipelineStatus'
    },
    deployment_status_since: {
      type: 'string',
      format: 'date-time'
    },
    description: {
      type: 'string'
    },
    id: {
      $ref: '#/components/schemas/PipelineId'
    },
    name: {
      type: 'string'
    },
    platform_version: {
      type: 'string'
    },
    program_code: {
      type: 'string',
      nullable: true
    },
    program_config: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramConfig'
        }
      ],
      nullable: true
    },
    program_error: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramError'
        }
      ],
      nullable: true
    },
    program_info: {
      allOf: [
        {
          $ref: '#/components/schemas/PartialProgramInfo'
        }
      ],
      nullable: true
    },
    program_status: {
      $ref: '#/components/schemas/ProgramStatus'
    },
    program_status_since: {
      type: 'string',
      format: 'date-time'
    },
    program_version: {
      $ref: '#/components/schemas/Version'
    },
    refresh_version: {
      $ref: '#/components/schemas/Version'
    },
    runtime_config: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeConfig'
        }
      ],
      nullable: true
    },
    udf_rust: {
      type: 'string',
      nullable: true
    },
    udf_toml: {
      type: 'string',
      nullable: true
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $PipelineStatus = {
  type: 'string',
  description: `Pipeline status.

This type represents the state of the pipeline tracked by the pipeline
runner and observed by the API client via the \`GET /v0/pipelines/{name}\` endpoint.

### The lifecycle of a pipeline

The following automaton captures the lifecycle of the pipeline.
Individual states and transitions of the automaton are described below.

* States labeled with the hourglass symbol (⌛) are **timed** states. The
automaton stays in timed state until the corresponding operation completes
or until it transitions to become failed after the pre-defined timeout
period expires.

* State transitions labeled with API endpoint names (\`/start\`, \`/pause\`,
\`/shutdown\`) are triggered by invoking corresponding endpoint,
e.g., \`POST /v0/pipelines/{name}/start\`. Note that these only express
desired state, and are applied asynchronously by the automata.

\`\`\`text
Shutdown◄────────────────────┐
│                        │
/start or /pause│                    ShuttingDown ◄────── Failed
│                        ▲                  ▲
▼              /shutdown │                  │
⌛Provisioning ──────────────────┤        Shutdown, Provisioning,
│                        │        Initializing, Paused,
│                        │         Running, Unavailable
▼                        │    (all states except ShuttingDown
⌛Initializing ──────────────────┤      can transition to Failed)
│                        │
┌─────────┼────────────────────────┴─┐
│         ▼                          │
│       Paused  ◄──────► Unavailable │
│       │    ▲                ▲      │
│ /start│    │/pause          │      │
│       ▼    │                │      │
│      Running ◄──────────────┘      │
└────────────────────────────────────┘
\`\`\`

### Desired and actual status

We use the desired state model to manage the lifecycle of a pipeline.
In this model, the pipeline has two status attributes associated with
it at runtime: the **desired** status, which represents what the user
would like the pipeline to do, and the **current** status, which
represents the actual state of the pipeline.  The pipeline runner
service continuously monitors both fields and steers the pipeline
towards the desired state specified by the user.
Only three of the states in the pipeline automaton above can be
used as desired statuses: \`Paused\`, \`Running\`, and \`Shutdown\`.
These statuses are selected by invoking REST endpoints shown
in the diagram.

The user can monitor the current state of the pipeline via the
\`GET /v0/pipelines/{name}\` endpoint. In a typical scenario,
the user first sets the desired state, e.g., by invoking the
\`/start\` endpoint, and then polls the \`GET /v0/pipelines/{name}\`
endpoint to monitor the actual status of the pipeline until its
\`deployment_status\` attribute changes to \`Running\` indicating
that the pipeline has been successfully initialized and is
processing data, or \`Failed\`, indicating an error.`,
  enum: [
    'Shutdown',
    'Provisioning',
    'Initializing',
    'Paused',
    'Running',
    'Unavailable',
    'Failed',
    'ShuttingDown'
  ]
} as const

export const $PostPutPipeline = {
  type: 'object',
  description: `Create a new pipeline (POST), or fully update an existing pipeline (PUT).
Fields which are optional and not provided will be set to their empty type value
(for strings: an empty string \`""\`, for objects: an empty dictionary \`{}\`).`,
  required: ['name', 'program_code'],
  properties: {
    description: {
      type: 'string',
      nullable: true
    },
    name: {
      type: 'string'
    },
    program_code: {
      type: 'string'
    },
    program_config: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramConfig'
        }
      ],
      nullable: true
    },
    runtime_config: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeConfig'
        }
      ],
      nullable: true
    },
    udf_rust: {
      type: 'string',
      nullable: true
    },
    udf_toml: {
      type: 'string',
      nullable: true
    }
  }
} as const

export const $PostgresReaderConfig = {
  type: 'object',
  description: 'Postgres input connector configuration.',
  required: ['uri', 'query'],
  properties: {
    query: {
      type: 'string',
      description: 'Query that specifies what data to fetch from postgres.'
    },
    uri: {
      type: 'string',
      description: `Postgres URI.
See: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>`
    }
  }
} as const

export const $PostgresWriterConfig = {
  type: 'object',
  description: 'Postgres output connector configuration.',
  required: ['uri', 'table'],
  properties: {
    table: {
      type: 'string',
      description: 'The table to write the output to.'
    },
    uri: {
      type: 'string',
      description: `Postgres URI.
See: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>`
    }
  }
} as const

export const $ProgramConfig = {
  type: 'object',
  description: 'Program configuration.',
  properties: {
    cache: {
      type: 'boolean',
      description: `If \`true\` (default), when a prior compilation with the same checksum
already exists, the output of that (i.e., binary) is used.
Set \`false\` to always trigger a new compilation, which might take longer
and as well can result in overriding an existing binary.`,
      default: true
    },
    profile: {
      allOf: [
        {
          $ref: '#/components/schemas/CompilationProfile'
        }
      ],
      default: null,
      nullable: true
    }
  }
} as const

export const $ProgramError = {
  type: 'object',
  description: 'Log, warning and error information about the program compilation.',
  properties: {
    rust_compilation: {
      allOf: [
        {
          $ref: '#/components/schemas/RustCompilationInfo'
        }
      ],
      nullable: true
    },
    sql_compilation: {
      allOf: [
        {
          $ref: '#/components/schemas/SqlCompilationInfo'
        }
      ],
      nullable: true
    },
    system_error: {
      type: 'string',
      description: `System error that occurred.
- Set \`Some(...)\` upon transition to \`SystemError\`
- Set \`None\` upon transition to \`Pending\``,
      nullable: true
    }
  }
} as const

export const $ProgramInfo = {
  type: 'object',
  description: `Program information is the output of the SQL compiler.

It includes information needed for Rust compilation (e.g., generated Rust code)
as well as only for runtime (e.g., schema, input/output connectors).`,
  required: ['schema', 'input_connectors', 'output_connectors'],
  properties: {
    dataflow: {
      description: 'Dataflow graph of the program.'
    },
    input_connectors: {
      type: 'object',
      description: 'Input connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/InputEndpointConfig'
      }
    },
    main_rust: {
      type: 'string',
      description: 'Generated main program Rust code: main.rs'
    },
    output_connectors: {
      type: 'object',
      description: 'Output connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/OutputEndpointConfig'
      }
    },
    schema: {
      $ref: '#/components/schemas/ProgramSchema'
    },
    udf_stubs: {
      type: 'string',
      description: 'Generated user defined function (UDF) stubs Rust code: stubs.rs'
    }
  }
} as const

export const $ProgramSchema = {
  type: 'object',
  description: `A struct containing the tables (inputs) and views for a program.

Parse from the JSON data-type of the DDL generated by the SQL compiler.`,
  required: ['inputs', 'outputs'],
  properties: {
    inputs: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Relation'
      }
    },
    outputs: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Relation'
      }
    }
  }
} as const

export const $ProgramStatus = {
  type: 'string',
  description: 'Program compilation status.',
  enum: [
    'Pending',
    'CompilingSql',
    'SqlCompiled',
    'CompilingRust',
    'Success',
    'SqlError',
    'RustError',
    'SystemError'
  ]
} as const

export const $PropertyValue = {
  type: 'object',
  required: ['value', 'key_position', 'value_position'],
  properties: {
    key_position: {
      $ref: '#/components/schemas/SourcePosition'
    },
    value: {
      type: 'string'
    },
    value_position: {
      $ref: '#/components/schemas/SourcePosition'
    }
  }
} as const

export const $ProviderAwsCognito = {
  type: 'object',
  required: ['jwk_uri', 'login_url', 'logout_url'],
  properties: {
    jwk_uri: {
      type: 'string'
    },
    login_url: {
      type: 'string'
    },
    logout_url: {
      type: 'string'
    }
  }
} as const

export const $ProviderGoogleIdentity = {
  type: 'object',
  required: ['jwk_uri', 'client_id'],
  properties: {
    client_id: {
      type: 'string'
    },
    jwk_uri: {
      type: 'string'
    }
  }
} as const

export const $PubSubInputConfig = {
  type: 'object',
  description: 'Google Pub/Sub input connector configuration.',
  required: ['subscription'],
  properties: {
    connect_timeout_seconds: {
      type: 'integer',
      format: 'int32',
      description: 'gRPC connection timeout.',
      nullable: true,
      minimum: 0
    },
    credentials: {
      type: 'string',
      description: `The content of a Google Cloud credentials JSON file.

When this option is specified, the connector will use the provided credentials for
authentication.  Otherwise, it will use Application Default Credentials (ADC) configured
in the environment where the Feldera service is running.  See
[Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)
for information on configuring application default credentials.

When running Feldera in an environment where ADC are not configured,
e.g., a Docker container, use this option to ship Google Cloud credentials from another environment.
For example, if you use the
[\`gcloud auth application-default login\`](https://cloud.google.com/pubsub/docs/authentication#client-libs)
command for authentication in your local development environment, ADC are stored in the
\`.config/gcloud/application_default_credentials.json\` file in your home directory.`,
      nullable: true
    },
    emulator: {
      type: 'string',
      description: `Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)
instead of the production service, e.g., 'localhost:8681'.`,
      nullable: true
    },
    endpoint: {
      type: 'string',
      description: "Override the default service endpoint 'pubsub.googleapis.com'",
      nullable: true
    },
    pool_size: {
      type: 'integer',
      format: 'int32',
      description: 'gRPC channel pool size.',
      nullable: true,
      minimum: 0
    },
    project_id: {
      type: 'string',
      description: `Google Cloud project_id.

When not specified, the connector will use the project id associated
with the authenticated account.`,
      nullable: true
    },
    snapshot: {
      type: 'string',
      description: `Reset subscription's backlog to a given snapshot on startup,
using the Pub/Sub \`Seek\` API.

This option is mutually exclusive with the \`timestamp\` option.`,
      nullable: true
    },
    subscription: {
      type: 'string',
      description: 'Subscription name.'
    },
    timeout_seconds: {
      type: 'integer',
      format: 'int32',
      description: 'gRPC request timeout.',
      nullable: true,
      minimum: 0
    },
    timestamp: {
      type: 'string',
      description: `Reset subscription's backlog to a given timestamp on startup,
using the Pub/Sub \`Seek\` API.

The value of this option is an ISO 8601-encoded UTC time, e.g., "2024-08-17T16:39:57-08:00".

This option is mutually exclusive with the \`snapshot\` option.`,
      nullable: true
    }
  }
} as const

export const $RedisOutputConfig = {
  type: 'object',
  description: 'Redis output connector configuration.',
  required: ['connection_string'],
  properties: {
    connection_string: {
      type: 'string',
      description: `The URL format: \`redis://[<username>][:<password>@]<hostname>[:port][/[<db>][?protocol=<protocol>]]\`
This is parsed by the [redis](https://docs.rs/redis/latest/redis/#connection-parameters) crate.`
    },
    key_separator: {
      type: 'string',
      description: `Separator used to join multiple components into a single key.
":" by default.`
    }
  }
} as const

export const $Relation = {
  allOf: [
    {
      $ref: '#/components/schemas/SqlIdentifier'
    },
    {
      type: 'object',
      required: ['fields'],
      properties: {
        fields: {
          type: 'array',
          items: {
            $ref: '#/components/schemas/Field'
          }
        },
        materialized: {
          type: 'boolean'
        },
        properties: {
          type: 'object',
          additionalProperties: {
            $ref: '#/components/schemas/PropertyValue'
          }
        }
      }
    }
  ],
  description: `A SQL table or view. It has a name and a list of fields.

Matches the Calcite JSON format.`
} as const

export const $ResourceConfig = {
  type: 'object',
  properties: {
    cpu_cores_max: {
      type: 'integer',
      format: 'int64',
      description: `The maximum number of CPU cores to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    },
    cpu_cores_min: {
      type: 'integer',
      format: 'int64',
      description: `The minimum number of CPU cores to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    },
    memory_mb_max: {
      type: 'integer',
      format: 'int64',
      description: `The maximum memory in Megabytes to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    },
    memory_mb_min: {
      type: 'integer',
      format: 'int64',
      description: `The minimum memory in Megabytes to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    },
    storage_class: {
      type: 'string',
      description: `Storage class to use for an instance of this pipeline.
The class determines storage performance such as IOPS and throughput.`,
      default: null,
      nullable: true
    },
    storage_mb_max: {
      type: 'integer',
      format: 'int64',
      description: `The total storage in Megabytes to reserve
for an instance of this pipeline`,
      default: null,
      nullable: true,
      minimum: 0
    }
  }
} as const

export const $RestCatalogConfig = {
  type: 'object',
  description: 'Iceberg REST catalog config.',
  properties: {
    'rest.audience': {
      type: 'string',
      description: 'Logical name of target resource or service.',
      nullable: true
    },
    'rest.credential': {
      type: 'string',
      description: `Credential to use for OAuth2 credential flow when initializing the catalog.

A key and secret pair separated by ":" (key is optional).`,
      nullable: true
    },
    'rest.headers': {
      type: 'array',
      items: {
        type: 'array',
        items: {
          allOf: [
            {
              type: 'string'
            },
            {
              type: 'string'
            }
          ]
        }
      },
      description: 'Additional HTTP request headers added to each catalog REST API call.',
      nullable: true
    },
    'rest.oauth2-server-uri': {
      type: 'string',
      description:
        "Authentication URL to use for client credentials authentication (default: uri + 'v1/oauth/tokens')",
      nullable: true
    },
    'rest.prefix': {
      type: 'string',
      description: `Customize table storage paths.

When combined with the \`warehouse\` property, the prefix determines
how table data is organized within the storage.`,
      nullable: true
    },
    'rest.resource': {
      type: 'string',
      description: 'URI for the target resource or service.',
      nullable: true
    },
    'rest.scope': {
      type: 'string',
      nullable: true
    },
    'rest.token': {
      type: 'string',
      description: 'Bearer token value to use for `Authorization` header.',
      nullable: true
    },
    'rest.uri': {
      type: 'string',
      description: 'URI identifying the REST catalog server.',
      nullable: true
    },
    'rest.warehouse': {
      type: 'string',
      description: 'The default location for managed tables created by the catalog.',
      nullable: true
    }
  }
} as const

export const $RngFieldSettings = {
  type: 'object',
  description: 'Configuration for generating random data for a field of a table.',
  properties: {
    e: {
      type: 'integer',
      format: 'int64',
      description: `The frequency rank exponent for the Zipf distribution.

- This value is only used if the strategy is set to \`Zipf\`.
- The default value is 1.0.`,
      default: 1
    },
    fields: {
      type: 'object',
      description:
        'Specifies the values that the generator should produce in case the field is a struct type.',
      default: null,
      additionalProperties: {
        $ref: '#/components/schemas/RngFieldSettings'
      },
      nullable: true
    },
    key: {
      allOf: [
        {
          $ref: '#/components/schemas/RngFieldSettings'
        }
      ],
      default: null,
      nullable: true
    },
    null_percentage: {
      type: 'integer',
      description: `Percentage of records where this field should be set to NULL.

If not set, the generator will produce only records with non-NULL values.
If set to \`1..=100\`, the generator will produce records with NULL values with the specified percentage.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    range: {
      type: 'object',
      description: `An optional, exclusive range [a, b) to limit the range of values the generator should produce.

- For integer/floating point types specifies min/max values as an integer.
If not set, the generator will produce values for the entire range of the type for number types.
- For string/binary types specifies min/max length as an integer, values are required to be >=0.
If not set, a range of [0, 25) is used by default.
- For timestamp types specifies the min/max as two strings in the RFC 3339 format
(e.g., ["2021-01-01T00:00:00Z", "2022-01-02T00:00:00Z"]).
Alternatively, the range values can be specified as a number of non-leap
milliseconds since January 1, 1970 0:00:00.000 UTC (aka “UNIX timestamp”).
If not set, a range of ["1970-01-01T00:00:00Z", "2100-01-01T00:00:00Z") or [0, 4102444800000)
is used by default.
- For time types specifies the min/max as two strings in the "HH:MM:SS" format.
Alternatively, the range values can be specified in milliseconds as two positive integers.
If not set, the range is 24h.
- For date types, the min/max range is specified as two strings in the "YYYY-MM-DD" format.
Alternatively, two integers that represent number of days since January 1, 1970 can be used.
If not set, a range of ["1970-01-01", "2100-01-01") or [0, 54787) is used by default.
- For array types specifies the min/max number of elements as an integer.
If not set, a range of [0, 5) is used by default. Range values are required to be >=0.
- For map types specifies the min/max number of key-value pairs as an integer.
If not set, a range of [0, 5) is used by default.
- For struct/boolean/null types \`range\` is ignored.`
    },
    scale: {
      type: 'integer',
      format: 'int64',
      description: `A scale factor to apply a multiplier to the generated value.

- For integer/floating point types, the value is multiplied by the scale factor.
- For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.
- For time types, the generated value (milliseconds) is multiplied by the scale factor.
- For date types, the generated value (days) is multiplied by the scale factor.
- For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.

- If \`values\` is specified, the scale factor is ignored.
- If \`range\` is specified and the range is required to be positive (struct, map, array etc.)
the scale factor is required to be positive too.

The default scale factor is 1.`,
      default: 1
    },
    strategy: {
      allOf: [
        {
          $ref: '#/components/schemas/DatagenStrategy'
        }
      ],
      default: 'increment'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/RngFieldSettings'
        }
      ],
      default: null,
      nullable: true
    },
    values: {
      type: 'array',
      items: {
        type: 'object'
      },
      description: `An optional set of values the generator will pick from.

If set, the generator will pick values from the specified set.
If not set, the generator will produce values according to the specified range.
If set to an empty set, the generator will produce NULL values.
If set to a single value, the generator will produce only that value.

Note that \`range\` is ignored if \`values\` is set.`,
      default: null,
      nullable: true
    }
  },
  additionalProperties: false
} as const

export const $RuntimeConfig = {
  type: 'object',
  description: `Global pipeline configuration settings. This is the publicly
exposed type for users to configure pipelines.`,
  properties: {
    clock_resolution_usecs: {
      type: 'integer',
      format: 'int64',
      description: `Real-time clock resolution in microseconds.

This parameter controls the execution of queries that use the \`NOW()\` function.  The output of such
queries depends on the real-time clock and can change over time without any external
inputs.  The pipeline will update the clock value and trigger incremental recomputation
at most each \`clock_resolution_usecs\` microseconds.

It is set to 100 milliseconds (100,000 microseconds) by default.

Set to \`null\` to disable periodic clock updates.`,
      default: 100000,
      nullable: true,
      minimum: 0
    },
    cpu_profiler: {
      type: 'boolean',
      description: `Enable CPU profiler.

The default value is \`true\`.`,
      default: true
    },
    fault_tolerance: {
      allOf: [
        {
          $ref: '#/components/schemas/FtConfig'
        }
      ],
      default: {
        model: 'none',
        checkpoint_interval_secs: 60
      }
    },
    max_buffering_delay_usecs: {
      type: 'integer',
      format: 'int64',
      description: `Maximal delay in microseconds to wait for \`min_batch_size_records\` to
get buffered by the controller, defaults to 0.`,
      default: 0,
      minimum: 0
    },
    max_parallel_connector_init: {
      type: 'integer',
      format: 'int64',
      description: `The maximum number of connectors initialized in parallel during pipeline
startup.

At startup, the pipeline must initialize all of its input and output connectors.
Depending on the number and types of connectors, this can take a long time.
To accelerate the process, multiple connectors are initialized concurrently.
This option controls the maximum number of connectors that can be intitialized
in parallel.

The default is 10.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    min_batch_size_records: {
      type: 'integer',
      format: 'int64',
      description: `Minimal input batch size.

The controller delays pushing input records to the circuit until at
least \`min_batch_size_records\` records have been received (total
across all endpoints) or \`max_buffering_delay_usecs\` microseconds
have passed since at least one input records has been buffered.
Defaults to 0.`,
      default: 0,
      minimum: 0
    },
    pin_cpus: {
      type: 'array',
      items: {
        type: 'integer',
        minimum: 0
      },
      description: `Optionally, a list of CPU numbers for CPUs to which the pipeline may pin
its worker threads.  Specify at least twice as many CPU numbers as
workers.  CPUs are generally numbered starting from 0.  The pipeline
might not be able to honor CPU pinning requests.

CPU pinning can make pipelines run faster and perform more consistently,
as long as different pipelines running on the same machine are pinned to
different CPUs.`,
      default: []
    },
    provisioning_timeout_secs: {
      type: 'integer',
      format: 'int64',
      description: `Timeout in seconds for the \`Provisioning\` phase of the pipeline.
Setting this value will override the default of the runner.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    resources: {
      allOf: [
        {
          $ref: '#/components/schemas/ResourceConfig'
        }
      ],
      default: {
        cpu_cores_min: null,
        cpu_cores_max: null,
        memory_mb_min: null,
        memory_mb_max: null,
        storage_mb_max: null,
        storage_class: null
      }
    },
    storage: {
      allOf: [
        {
          $ref: '#/components/schemas/StorageOptions'
        }
      ],
      default: {
        backend: {
          name: 'default'
        },
        min_storage_bytes: null,
        min_step_storage_bytes: null,
        compression: 'default',
        cache_mib: null
      },
      nullable: true
    },
    tracing: {
      type: 'boolean',
      description: 'Enable pipeline tracing.',
      default: false
    },
    tracing_endpoint_jaeger: {
      type: 'string',
      description: 'Jaeger tracing endpoint to send tracing information to.',
      default: '127.0.0.1:6831'
    },
    workers: {
      type: 'integer',
      format: 'int32',
      description: `Number of DBSP worker threads.

Each DBSP "foreground" worker thread is paired with a "background"
thread for LSM merging, making the total number of threads twice the
specified number.`,
      default: 8,
      minimum: 0
    }
  }
} as const

export const $RustCompilationInfo = {
  type: 'object',
  description: 'Rust compilation information.',
  required: ['exit_code', 'stdout', 'stderr'],
  properties: {
    exit_code: {
      type: 'integer',
      format: 'int32',
      description: 'Exit code of the `cargo` compilation command.'
    },
    stderr: {
      type: 'string',
      description: 'Output printed to stderr by the `cargo` compilation command.'
    },
    stdout: {
      type: 'string',
      description: 'Output printed to stdout by the `cargo` compilation command.'
    }
  }
} as const

export const $S3InputConfig = {
  type: 'object',
  description: 'Configuration for reading data from AWS S3.',
  required: ['region', 'bucket_name'],
  properties: {
    aws_access_key_id: {
      type: 'string',
      description:
        'AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.',
      nullable: true
    },
    aws_secret_access_key: {
      type: 'string',
      description:
        'Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.',
      nullable: true
    },
    bucket_name: {
      type: 'string',
      description: 'S3 bucket name to access.'
    },
    endpoint_url: {
      type: 'string',
      description: `The endpoint URL used to communicate with this service. Can be used to make this connector
talk to non-AWS services with an S3 API.`,
      nullable: true
    },
    key: {
      type: 'string',
      description: 'Read a single object specified by a key.',
      nullable: true
    },
    no_sign_request: {
      type: 'boolean',
      description:
        'Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI.'
    },
    prefix: {
      type: 'string',
      description:
        'Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.',
      nullable: true
    },
    region: {
      type: 'string',
      description: 'AWS region.'
    }
  }
} as const

export const $SourcePosition = {
  type: 'object',
  required: ['start_line_number', 'start_column', 'end_line_number', 'end_column'],
  properties: {
    end_column: {
      type: 'integer',
      minimum: 0
    },
    end_line_number: {
      type: 'integer',
      minimum: 0
    },
    start_column: {
      type: 'integer',
      minimum: 0
    },
    start_line_number: {
      type: 'integer',
      minimum: 0
    }
  }
} as const

export const $SqlCompilationInfo = {
  type: 'object',
  description: 'SQL compilation information.',
  required: ['exit_code', 'messages'],
  properties: {
    exit_code: {
      type: 'integer',
      format: 'int32',
      description: 'Exit code of the SQL compiler.'
    },
    messages: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/SqlCompilerMessage'
      },
      description: 'Messages (warnings and errors) generated by the SQL compiler.'
    }
  }
} as const

export const $SqlCompilerMessage = {
  type: 'object',
  description: `A SQL compiler error.

The SQL compiler returns a list of errors in the following JSON format if
it's invoked with the \`-je\` option.

\`\`\`ignore
[ {
"start_line_number" : 2,
"start_column" : 4,
"end_line_number" : 2,
"end_column" : 8,
"warning" : false,
"error_type" : "PRIMARY KEY cannot be nullable",
"message" : "PRIMARY KEY column 'C' has type INTEGER, which is nullable",
"snippet" : "    2|   c INT PRIMARY KEY\n         ^^^^^\n    3|);\n"
} ]
\`\`\``,
  required: [
    'start_line_number',
    'start_column',
    'end_line_number',
    'end_column',
    'warning',
    'error_type',
    'message'
  ],
  properties: {
    end_column: {
      type: 'integer',
      minimum: 0
    },
    end_line_number: {
      type: 'integer',
      minimum: 0
    },
    error_type: {
      type: 'string'
    },
    message: {
      type: 'string'
    },
    snippet: {
      type: 'string',
      nullable: true
    },
    start_column: {
      type: 'integer',
      minimum: 0
    },
    start_line_number: {
      type: 'integer',
      minimum: 0
    },
    warning: {
      type: 'boolean'
    }
  }
} as const

export const $SqlIdentifier = {
  type: 'object',
  description: `An SQL identifier.

This struct is used to represent SQL identifiers in a canonical form.
We store table names or field names as identifiers in the schema.`,
  required: ['name', 'case_sensitive'],
  properties: {
    case_sensitive: {
      type: 'boolean'
    },
    name: {
      type: 'string'
    }
  }
} as const

export const $SqlType = {
  oneOf: [
    {
      type: 'string',
      description: 'SQL `BOOLEAN` type.',
      enum: ['Boolean']
    },
    {
      type: 'string',
      description: 'SQL `TINYINT` type.',
      enum: ['TinyInt']
    },
    {
      type: 'string',
      description: 'SQL `SMALLINT` or `INT2` type.',
      enum: ['SmallInt']
    },
    {
      type: 'string',
      description: 'SQL `INTEGER`, `INT`, `SIGNED`, `INT4` type.',
      enum: ['Int']
    },
    {
      type: 'string',
      description: 'SQL `BIGINT` or `INT64` type.',
      enum: ['BigInt']
    },
    {
      type: 'string',
      description: 'SQL `REAL` or `FLOAT4` or `FLOAT32` type.',
      enum: ['Real']
    },
    {
      type: 'string',
      description: 'SQL `DOUBLE` or `FLOAT8` or `FLOAT64` type.',
      enum: ['Double']
    },
    {
      type: 'string',
      description: 'SQL `DECIMAL` or `DEC` or `NUMERIC` type.',
      enum: ['Decimal']
    },
    {
      type: 'string',
      description: 'SQL `CHAR(n)` or `CHARACTER(n)` type.',
      enum: ['Char']
    },
    {
      type: 'string',
      description: 'SQL `VARCHAR`, `CHARACTER VARYING`, `TEXT`, or `STRING` type.',
      enum: ['Varchar']
    },
    {
      type: 'string',
      description: 'SQL `BINARY(n)` type.',
      enum: ['Binary']
    },
    {
      type: 'string',
      description: 'SQL `VARBINARY` or `BYTEA` type.',
      enum: ['Varbinary']
    },
    {
      type: 'string',
      description: 'SQL `TIME` type.',
      enum: ['Time']
    },
    {
      type: 'string',
      description: 'SQL `DATE` type.',
      enum: ['Date']
    },
    {
      type: 'string',
      description: 'SQL `TIMESTAMP` type.',
      enum: ['Timestamp']
    },
    {
      type: 'object',
      required: ['Interval'],
      properties: {
        Interval: {
          $ref: '#/components/schemas/IntervalUnit'
        }
      }
    },
    {
      type: 'string',
      description: 'SQL `ARRAY` type.',
      enum: ['Array']
    },
    {
      type: 'string',
      description: 'A complex SQL struct type (`CREATE TYPE x ...`).',
      enum: ['Struct']
    },
    {
      type: 'string',
      description: 'SQL `MAP` type.',
      enum: ['Map']
    },
    {
      type: 'string',
      description: 'SQL `NULL` type.',
      enum: ['Null']
    },
    {
      type: 'string',
      description: 'SQL `UUID` type.',
      enum: ['Uuid']
    },
    {
      type: 'string',
      description: 'SQL `VARIANT` type.',
      enum: ['Variant']
    }
  ],
  description: 'The available SQL types as specified in `CREATE` statements.'
} as const

export const $StorageBackendConfig = {
  oneOf: [
    {
      type: 'object',
      required: ['name'],
      properties: {
        name: {
          type: 'string',
          enum: ['default']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/ObjectStorageConfig'
        },
        name: {
          type: 'string',
          enum: ['object']
        }
      }
    }
  ],
  description: 'Backend storage configuration.',
  discriminator: {
    propertyName: 'name'
  }
} as const

export const $StorageCacheConfig = {
  type: 'string',
  description: 'How to cache access to storage within a Feldera pipeline.',
  enum: ['page_cache', 'feldera_cache']
} as const

export const $StorageCompression = {
  type: 'string',
  description: 'Storage compression algorithm.',
  enum: ['default', 'none', 'snappy']
} as const

export const $StorageConfig = {
  type: 'object',
  description: 'Configuration for persistent storage in a [`PipelineConfig`].',
  required: ['path'],
  properties: {
    cache: {
      $ref: '#/components/schemas/StorageCacheConfig'
    },
    path: {
      type: 'string',
      description: `A directory to keep pipeline state, as a path on the filesystem of the
machine or container where the pipeline will run.

When storage is enabled, this directory stores the data for
[StorageBackendConfig::Default].

When fault tolerance is enabled, this directory stores checkpoints and
the log.`
    }
  }
} as const

export const $StorageOptions = {
  type: 'object',
  description: 'Storage configuration for a pipeline.',
  properties: {
    backend: {
      allOf: [
        {
          $ref: '#/components/schemas/StorageBackendConfig'
        }
      ],
      default: {
        name: 'default'
      }
    },
    cache_mib: {
      type: 'integer',
      description: `The maximum size of the in-memory storage cache, in MiB.

If set, the specified cache size is spread across all the foreground and
background threads. If unset, each foreground or background thread cache
is limited to 256 MiB.`,
      default: null,
      nullable: true,
      minimum: 0
    },
    compression: {
      allOf: [
        {
          $ref: '#/components/schemas/StorageCompression'
        }
      ],
      default: 'default'
    },
    min_step_storage_bytes: {
      type: 'integer',
      description: `For a batch of data passed through the pipeline during a single step,
the minimum estimated number of bytes to write it to storage.

This is provided for debugging and fine-tuning and should ordinarily be
left unset.  If it is set, it should ordinarily be greater than or equal
to \`min_storage_bytes\`.

A value of 0 will write even empty batches to storage, and nonzero
values provide a threshold.  \`usize::MAX\` would effectively disable
storage for such batches.  The default is 10,485,760 (10 MiB).`,
      default: null,
      nullable: true,
      minimum: 0
    },
    min_storage_bytes: {
      type: 'integer',
      description: `For a batch of data maintained as a persistent index during a pipeline
run, the minimum estimated number of bytes to write it to storage.

This is provided for debugging and fine-tuning and should ordinarily be
left unset.

A value of 0 will write even empty batches to storage, and nonzero
values provide a threshold.  \`usize::MAX\` would effectively disable
storage for such batches.  The default is 1,048,576 (1 MiB).`,
      default: null,
      nullable: true,
      minimum: 0
    }
  }
} as const

export const $TransportConfig = {
  oneOf: [
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileInputConfig'
        },
        name: {
          type: 'string',
          enum: ['file_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['file_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/KafkaInputConfig'
        },
        name: {
          type: 'string',
          enum: ['kafka_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/KafkaOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['kafka_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/PubSubInputConfig'
        },
        name: {
          type: 'string',
          enum: ['pub_sub_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/UrlInputConfig'
        },
        name: {
          type: 'string',
          enum: ['url_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/S3InputConfig'
        },
        name: {
          type: 'string',
          enum: ['s3_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DeltaTableReaderConfig'
        },
        name: {
          type: 'string',
          enum: ['delta_table_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DeltaTableWriterConfig'
        },
        name: {
          type: 'string',
          enum: ['delta_table_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/RedisOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['redis_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/IcebergReaderConfig'
        },
        name: {
          type: 'string',
          enum: ['iceberg_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/PostgresReaderConfig'
        },
        name: {
          type: 'string',
          enum: ['postgres_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/PostgresWriterConfig'
        },
        name: {
          type: 'string',
          enum: ['postgres_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DatagenInputConfig'
        },
        name: {
          type: 'string',
          enum: ['datagen']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/NexmarkInputConfig'
        },
        name: {
          type: 'string',
          enum: ['nexmark']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/HttpInputConfig'
        },
        name: {
          type: 'string',
          enum: ['http_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name'],
      properties: {
        name: {
          type: 'string',
          enum: ['http_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/AdHocInputConfig'
        },
        name: {
          type: 'string',
          enum: ['ad_hoc_input']
        }
      }
    }
  ],
  description: `Transport-specific endpoint configuration passed to
\`crate::OutputTransport::new_endpoint\`
and \`crate::InputTransport::new_endpoint\`.`,
  discriminator: {
    propertyName: 'name'
  }
} as const

export const $UpdateInformation = {
  type: 'object',
  required: ['latest_version', 'is_latest_version', 'instructions_url', 'remind_schedule'],
  properties: {
    instructions_url: {
      type: 'string',
      description:
        "URL that navigates the user to instructions on how to update their deployment's version"
    },
    is_latest_version: {
      type: 'boolean',
      description: 'Whether the current version matches the latest version'
    },
    latest_version: {
      type: 'string',
      description: 'Latest version corresponding to the edition'
    },
    remind_schedule: {
      $ref: '#/components/schemas/DisplaySchedule'
    }
  }
} as const

export const $UrlInputConfig = {
  type: 'object',
  description: `Configuration for reading data from an HTTP or HTTPS URL with
\`UrlInputTransport\`.`,
  required: ['path'],
  properties: {
    path: {
      type: 'string',
      description: 'URL.'
    },
    pause_timeout: {
      type: 'integer',
      format: 'int32',
      description: `Timeout before disconnection when paused, in seconds.

If the pipeline is paused, or if the input adapter reads data faster
than the pipeline can process it, then the controller will pause the
input adapter. If the input adapter stays paused longer than this
timeout, it will drop the network connection to the server. It will
automatically reconnect when the input adapter starts running again.`,
      minimum: 0
    }
  }
} as const

export const $Version = {
  type: 'integer',
  format: 'int64',
  description: 'Version number.'
} as const
