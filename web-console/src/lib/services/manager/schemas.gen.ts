// This file is auto-generated by @hey-api/openapi-ts

export const $ApiKeyDescr = {
  type: 'object',
  description: 'API key descriptor.',
  required: ['id', 'name', 'scopes'],
  properties: {
    id: {
      $ref: '#/components/schemas/ApiKeyId'
    },
    name: {
      type: 'string'
    },
    scopes: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/ApiPermission'
      }
    }
  }
} as const

export const $ApiKeyId = {
  type: 'string',
  format: 'uuid',
  description: 'API key identifier.'
} as const

export const $ApiPermission = {
  type: 'string',
  description: 'Permission types for invoking API endpoints.',
  enum: ['Read', 'Write']
} as const

export const $AuthProvider = {
  oneOf: [
    {
      type: 'object',
      required: ['AwsCognito'],
      properties: {
        AwsCognito: {
          $ref: '#/components/schemas/ProviderAwsCognito'
        }
      }
    },
    {
      type: 'object',
      required: ['GoogleIdentity'],
      properties: {
        GoogleIdentity: {
          $ref: '#/components/schemas/ProviderGoogleIdentity'
        }
      }
    }
  ]
} as const

export const $AwsCredentials = {
  oneOf: [
    {
      type: 'object',
      required: ['type'],
      properties: {
        type: {
          type: 'string',
          enum: ['NoSignRequest']
        }
      }
    },
    {
      type: 'object',
      description: 'Authenticate using a long-lived AWS access key and secret',
      required: ['aws_access_key_id', 'aws_secret_access_key', 'type'],
      properties: {
        aws_access_key_id: {
          type: 'string'
        },
        aws_secret_access_key: {
          type: 'string'
        },
        type: {
          type: 'string',
          enum: ['AccessKey']
        }
      }
    }
  ],
  description: 'Configuration to authenticate against AWS',
  discriminator: {
    propertyName: 'type'
  }
} as const

export const $Chunk = {
  type: 'object',
  description: `A set of updates to a SQL table or view.

The \`sequence_number\` field stores the offset of the chunk relative to the
start of the stream and can be used to implement reliable delivery.
The payload is stored in the \`bin_data\`, \`text_data\`, or \`json_data\` field
depending on the data format used.`,
  required: ['sequence_number'],
  properties: {
    bin_data: {
      type: 'string',
      format: 'binary',
      description: 'Base64 encoded binary payload, e.g., bincode.',
      nullable: true
    },
    json_data: {
      type: 'object',
      description: 'JSON payload.',
      nullable: true
    },
    sequence_number: {
      type: 'integer',
      format: 'int64',
      minimum: 0
    },
    text_data: {
      type: 'string',
      description: 'Text payload, e.g., CSV.',
      nullable: true
    }
  }
} as const

export const $ColumnType = {
  type: 'object',
  description: `A SQL column type description.

Matches the Calcite JSON format.`,
  required: ['nullable'],
  properties: {
    component: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    },
    fields: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Field'
      },
      description: `The fields of the type (if available).

For example this would specify the fields of a \`CREATE TYPE\` construct.

\`\`\`sql
CREATE TYPE person_typ AS (
firstname       VARCHAR(30),
lastname        VARCHAR(30),
address         ADDRESS_TYP
);
\`\`\`

Would lead to the following \`fields\` value:

\`\`\`sql
[
ColumnType { name: "firstname, ... },
ColumnType { name: "lastname", ... },
ColumnType { name: "address", fields: [ ... ] }
]
\`\`\``,
      nullable: true
    },
    key: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    },
    nullable: {
      type: 'boolean',
      description: 'Does the type accept NULL values?'
    },
    precision: {
      type: 'integer',
      format: 'int64',
      description: `Precision of the type.

# Examples
- \`VARCHAR\` sets precision to \`-1\`.
- \`VARCHAR(255)\` sets precision to \`255\`.
- \`BIGINT\`, \`DATE\`, \`FLOAT\`, \`DOUBLE\`, \`GEOMETRY\`, etc. sets precision
to None
- \`TIME\`, \`TIMESTAMP\` set precision to \`0\`.`,
      nullable: true
    },
    scale: {
      type: 'integer',
      format: 'int64',
      description: `The scale of the type.

# Example
- \`DECIMAL(1,2)\` sets scale to \`2\`.`,
      nullable: true
    },
    type: {
      $ref: '#/components/schemas/SqlType'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    }
  }
} as const

export const $CompilationProfile = {
  type: 'string',
  description: `Enumeration of possible compilation profiles that can be passed to the Rust compiler
as an argument via \`cargo build --profile <>\`. A compilation profile affects among
other things the compilation speed (how long till the program is ready to be run)
and runtime speed (the performance while running).`,
  enum: ['dev', 'unoptimized', 'optimized']
} as const

export const $ConnectorConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/OutputBufferConfig'
    },
    {
      type: 'object',
      required: ['transport'],
      properties: {
        format: {
          allOf: [
            {
              $ref: '#/components/schemas/FormatConfig'
            }
          ],
          nullable: true
        },
        max_queued_records: {
          type: 'integer',
          format: 'int64',
          description: `Backpressure threshold.

Maximal number of records queued by the endpoint before the endpoint
is paused by the backpressure mechanism.

For input endpoints, this setting bounds the number of records that have
been received from the input transport but haven't yet been consumed by
the circuit since the circuit, since the circuit is still busy processing
previous inputs.

For output endpoints, this setting bounds the number of records that have
been produced by the circuit but not yet sent via the output transport endpoint
nor stored in the output buffer (see \`enable_output_buffer\`).

Note that this is not a hard bound: there can be a small delay between
the backpressure mechanism is triggered and the endpoint is paused, during
which more data may be queued.

The default is 1 million.`,
          minimum: 0
        },
        paused: {
          type: 'boolean',
          description: `Create connector in paused state.

The default is \`false\`.`
        },
        transport: {
          $ref: '#/components/schemas/TransportConfig'
        }
      }
    }
  ],
  description: "A data connector's configuration"
} as const

export const $ConsumeStrategy = {
  oneOf: [
    {
      type: 'object',
      required: ['type'],
      properties: {
        type: {
          type: 'string',
          enum: ['Fragment']
        }
      }
    },
    {
      type: 'object',
      required: ['type'],
      properties: {
        type: {
          type: 'string',
          enum: ['Object']
        }
      }
    }
  ],
  description: 'Strategy to feed a fetched object into an InputConsumer.'
} as const

export const $DatagenInputConfig = {
  type: 'object',
  description: 'Configuration for generating random data for a table.',
  properties: {
    plan: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/GenerationPlan'
      },
      description: `The sequence of generations to perform.

If not set, the generator will produce a single sequence with default settings.
If set, the generator will produce the specified sequences in sequential order.

Note that if one of the sequences before the last one generates an unlimited number of rows
the following sequences will not be executed.`
    },
    seed: {
      type: 'integer',
      format: 'int64',
      description: `Optional seed for the random generator.

Setting this to a fixed value will make the generator produce the same sequence of records
every time the pipeline is run.

# Notes
- To ensure the set of generated input records is deterministic across multiple runs,
apart from setting a seed, \`workers\` also needs to remain unchanged.
- The input will arrive in non-deterministic order if \`workers > 1\`.`,
      nullable: true,
      minimum: 0
    },
    workers: {
      type: 'integer',
      description: 'Number of workers to use for generating data.',
      minimum: 0
    }
  }
} as const

export const $DatagenStrategy = {
  type: 'string',
  description: 'Strategy used to generate values.',
  enum: [
    'increment',
    'uniform',
    'zipf',
    'word',
    'words',
    'sentence',
    'sentences',
    'paragraph',
    'paragraphs',
    'first_name',
    'last_name',
    'title',
    'suffix',
    'name',
    'name_with_title',
    'domain_suffix',
    'email',
    'username',
    'password',
    'field',
    'position',
    'seniority',
    'job_title',
    'i_pv4',
    'i_pv6',
    'i_p',
    'm_a_c_address',
    'user_agent',
    'rfc_status_code',
    'valid_status_code',
    'company_suffix',
    'company_name',
    'buzzword',
    'buzzword_middle',
    'buzzword_tail',
    'catch_phrase',
    'bs_verb',
    'bs_adj',
    'bs_noun',
    'bs',
    'profession',
    'industry',
    'currency_code',
    'currency_name',
    'currency_symbol',
    'credit_card_number',
    'city_prefix',
    'city_suffix',
    'city_name',
    'country_name',
    'country_code',
    'street_suffix',
    'street_name',
    'time_zone',
    'state_name',
    'state_abbr',
    'secondary_address_type',
    'secondary_address',
    'zip_code',
    'post_code',
    'building_number',
    'latitude',
    'longitude',
    'isbn',
    'isbn13',
    'isbn10',
    'phone_number',
    'cell_number',
    'file_path',
    'file_name',
    'file_extension',
    'dir_path'
  ]
} as const

export const $DeltaTableIngestMode = {
  type: 'string',
  description: `Delta table read mode.

Three options are available:

* \`snapshot\` - read a snapshot of the table and stop.

* \`follow\` - continuously ingest changes to the table, starting from a specified version
or timestamp.

* \`snapshot_and_follow\` - read a snapshot of the table before switching to continuous ingestion
mode.`,
  enum: ['snapshot', 'follow', 'snapshot_and_follow']
} as const

export const $DeltaTableReaderConfig = {
  type: 'object',
  description: 'Delta table output connector configuration.',
  required: ['uri', 'mode'],
  properties: {
    datetime: {
      type: 'string',
      description: `Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
"2024-12-09T16:09:53+00:00.

When this option is set, the connector finds and opens the version of the table as of the
specified point in time.  In \`snapshot\` and \`snapshot_and_follow\` modes, it retrieves the
snapshot of this version of the table (based on the server time recorded in the transaction
log, not the event time encoded in the data).  In \`follow\` and \`snapshot_and_follow\` modes, it
follows transaction log records **after** this version.

Note: at most one of \`version\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
      nullable: true
    },
    mode: {
      $ref: '#/components/schemas/DeltaTableIngestMode'
    },
    snapshot_filter: {
      type: 'string',
      description: `Optional row filter.

This option is only valid when \`mode\` is set to \`snapshot\` or \`snapshot_and_follow\`.

When specified, only rows that satisfy the filter condition are included in the
snapshot.  The condition must be a valid SQL Boolean expression that can be used in
the \`where\` clause of the \`select * from snapshot where ...\` query.

This option can be used to specify the range of event times to include in the snapshot,
e.g.: \`ts BETWEEN '2005-01-01 00:00:00' AND '2010-12-31 23:59:59'\`.`,
      nullable: true
    },
    timestamp_column: {
      type: 'string',
      description: `Table column that serves as an event timestamp.


When this option is specified, and \`mode\` is one of \`snapshot\` or \`snapshot_and_follow\`,
the snapshot of the table is ingested in the timestamp order.  This setting is required
for tables declared with the
[\`LATENESS\`](https://www.feldera.com/docs/sql/streaming#lateness-expressions) attribute
in Feldera SQL. It impacts the performance of the connector, since data must be sorted
before pushing it to the pipeline; therefore it is not recommended to use this
settings for tables without \`LATENESS\`.`,
      nullable: true
    },
    uri: {
      type: 'string',
      description: `Table URI.

Example: "s3://feldera-fraud-detection-data/demographics_train"`
    },
    version: {
      type: 'integer',
      format: 'int64',
      description: `Optional table version.

When this option is set, the connector finds and opens the specified version of the table.
In \`snapshot\` and \`snapshot_and_follow\` modes, it retrieves the snapshot of this version of
the table.  In \`follow\` and \`snapshot_and_follow\` modes, it follows transaction log records
**after** this version.

Note: at most one of \`version\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
      nullable: true
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Storage options for configuring backend object store.

For specific options available for different storage backends, see:
* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)`
  }
} as const

export const $DeltaTableWriteMode = {
  type: 'string',
  description: `Delta table write mode.

Determines how the Delta table connector handles an existing table at the target location.`,
  enum: ['append', 'truncate', 'error_if_exists']
} as const

export const $DeltaTableWriterConfig = {
  type: 'object',
  description: 'Delta table output connector configuration.',
  required: ['uri'],
  properties: {
    mode: {
      $ref: '#/components/schemas/DeltaTableWriteMode'
    },
    uri: {
      type: 'string',
      description: 'Table URI.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Storage options for configuring backend object store.

For specific options available for different storage backends, see:
* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)`
  }
} as const

export const $Demo = {
  type: 'object',
  required: ['title', 'pipeline'],
  properties: {
    pipeline: {
      $ref: '#/components/schemas/PipelineDescr'
    },
    title: {
      type: 'string',
      description: 'Demo title.'
    }
  }
} as const

export const $EgressMode = {
  type: 'string',
  enum: ['watch', 'snapshot']
} as const

export const $ErrorResponse = {
  type: 'object',
  description: 'Information returned by REST API endpoints on error.',
  required: ['message', 'error_code', 'details'],
  properties: {
    details: {
      type: 'object',
      description: `Detailed error metadata.
The contents of this field is determined by \`error_code\`.`
    },
    error_code: {
      type: 'string',
      description: 'Error code is a string that specifies this error type.',
      example: 'UnknownInputFormat'
    },
    message: {
      type: 'string',
      description: 'Human-readable error message.',
      example: "Unknown input format 'xml'."
    }
  }
} as const

export const $ExtendedPipelineDescr = {
  type: 'object',
  description: `Pipeline descriptor which besides the basic fields in direct regular control of the user
also has all additional fields generated and maintained by the back-end.`,
  required: [
    'id',
    'name',
    'description',
    'version',
    'created_at',
    'runtime_config',
    'program_code',
    'program_config',
    'program_version',
    'program_status',
    'program_status_since',
    'deployment_status',
    'deployment_status_since',
    'deployment_desired_status'
  ],
  properties: {
    created_at: {
      type: 'string',
      format: 'date-time',
      description: 'Timestamp when the pipeline was originally created.'
    },
    deployment_config: {
      allOf: [
        {
          $ref: '#/components/schemas/PipelineConfig'
        }
      ],
      nullable: true
    },
    deployment_desired_status: {
      $ref: '#/components/schemas/PipelineStatus'
    },
    deployment_error: {
      allOf: [
        {
          $ref: '#/components/schemas/ErrorResponse'
        }
      ],
      nullable: true
    },
    deployment_location: {
      type: 'string',
      description: `Location where the pipeline can be reached at runtime
(e.g., a TCP port number or a URI).`,
      nullable: true
    },
    deployment_status: {
      $ref: '#/components/schemas/PipelineStatus'
    },
    deployment_status_since: {
      type: 'string',
      format: 'date-time',
      description: `Time when the pipeline was assigned its current status
of the pipeline.`
    },
    description: {
      type: 'string',
      description: 'Pipeline description.'
    },
    id: {
      $ref: '#/components/schemas/PipelineId'
    },
    name: {
      type: 'string',
      description: 'Pipeline name.'
    },
    program_binary_url: {
      type: 'string',
      description: 'URL where to download the program binary from.',
      nullable: true
    },
    program_code: {
      type: 'string',
      description: 'Program SQL code.'
    },
    program_config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    program_info: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramInfo'
        }
      ],
      nullable: true
    },
    program_status: {
      $ref: '#/components/schemas/ProgramStatus'
    },
    program_status_since: {
      type: 'string',
      format: 'date-time',
      description: 'Timestamp when the current program status was set.'
    },
    program_version: {
      $ref: '#/components/schemas/Version'
    },
    runtime_config: {
      $ref: '#/components/schemas/RuntimeConfig'
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $ExtendedPipelineDescrOptionalCode = {
  type: 'object',
  description: 'Extended pipeline descriptor with code being optionally included.',
  required: [
    'id',
    'name',
    'description',
    'version',
    'created_at',
    'runtime_config',
    'program_config',
    'program_version',
    'program_status',
    'program_status_since',
    'deployment_status',
    'deployment_status_since',
    'deployment_desired_status'
  ],
  properties: {
    created_at: {
      type: 'string',
      format: 'date-time'
    },
    deployment_config: {
      allOf: [
        {
          $ref: '#/components/schemas/PipelineConfig'
        }
      ],
      nullable: true
    },
    deployment_desired_status: {
      $ref: '#/components/schemas/PipelineStatus'
    },
    deployment_error: {
      allOf: [
        {
          $ref: '#/components/schemas/ErrorResponse'
        }
      ],
      nullable: true
    },
    deployment_location: {
      type: 'string',
      nullable: true
    },
    deployment_status: {
      $ref: '#/components/schemas/PipelineStatus'
    },
    deployment_status_since: {
      type: 'string',
      format: 'date-time'
    },
    description: {
      type: 'string'
    },
    id: {
      $ref: '#/components/schemas/PipelineId'
    },
    name: {
      type: 'string'
    },
    program_binary_url: {
      type: 'string',
      nullable: true
    },
    program_code: {
      type: 'string',
      nullable: true
    },
    program_config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    program_info: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramInfo'
        }
      ],
      nullable: true
    },
    program_status: {
      $ref: '#/components/schemas/ProgramStatus'
    },
    program_status_since: {
      type: 'string',
      format: 'date-time'
    },
    program_version: {
      $ref: '#/components/schemas/Version'
    },
    runtime_config: {
      $ref: '#/components/schemas/RuntimeConfig'
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $Field = {
  type: 'object',
  description: `A SQL field.

Matches the SQL compiler JSON format.`,
  required: ['name', 'columntype'],
  properties: {
    case_sensitive: {
      type: 'boolean'
    },
    columntype: {
      $ref: '#/components/schemas/ColumnType'
    },
    name: {
      type: 'string'
    }
  }
} as const

export const $FileInputConfig = {
  type: 'object',
  description: 'Configuration for reading data from a file with `FileInputTransport`',
  required: ['path'],
  properties: {
    buffer_size_bytes: {
      type: 'integer',
      description: `Read buffer size.

Default: when this parameter is not specified, a platform-specific
default is used.`,
      nullable: true,
      minimum: 0
    },
    follow: {
      type: 'boolean',
      description: `Enable file following.

When \`false\`, the endpoint outputs an \`InputConsumer::eoi\`
message and stops upon reaching the end of file.  When \`true\`, the
endpoint will keep watching the file and outputting any new content
appended to it.`
    },
    path: {
      type: 'string',
      description: 'File path.'
    }
  }
} as const

export const $FileOutputConfig = {
  type: 'object',
  description: 'Configuration for writing data to a file with `FileOutputTransport`.',
  required: ['path'],
  properties: {
    path: {
      type: 'string',
      description: 'File path.'
    }
  }
} as const

export const $FormatConfig = {
  type: 'object',
  description: `Data format specification used to parse raw data received from the
endpoint or to encode data sent to the endpoint.`,
  required: ['name'],
  properties: {
    config: {
      type: 'object',
      description: 'Format-specific parser or encoder configuration.'
    },
    name: {
      type: 'string',
      description: 'Format name, e.g., "csv", "json", "bincode", etc.'
    }
  }
} as const

export const $GenerationPlan = {
  type: 'object',
  description:
    'A random generation plan for a table that generates either a limited amount of rows or runs continuously.',
  properties: {
    fields: {
      type: 'object',
      description: 'Specifies the values that the generator should produce.',
      additionalProperties: {
        $ref: '#/components/schemas/RngFieldSettings'
      }
    },
    limit: {
      type: 'integer',
      description: `Total number of new rows to generate.

If not set, the generator will produce new/unique records as long as the pipeline is running.
If set to 0, the table will always remain empty.
If set, the generator will produce new records until the specified limit is reached.

Note that if the table has one or more primary keys that don't use the \`increment\` strategy to
generate the key there is a potential that an update is generated instead of an insert. In
this case it's possible the total number of records is less than the specified limit.`,
      nullable: true,
      minimum: 0
    },
    rate: {
      type: 'integer',
      format: 'int32',
      description: `Non-zero number of rows to generate per second.

If not set, the generator will produce rows as fast as possible.`,
      nullable: true,
      minimum: 0
    }
  }
} as const

export const $InputEndpointConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    {
      type: 'object',
      required: ['stream'],
      properties: {
        stream: {
          type: 'string',
          description: `The name of the input stream of the circuit that this endpoint is
connected to.`
        }
      }
    }
  ],
  description: 'Describes an input connector configuration'
} as const

export const $IntervalUnit = {
  type: 'string',
  description: `The specified units for SQL Interval types.

\`INTERVAL 1 DAY\`, \`INTERVAL 1 DAY TO HOUR\`, \`INTERVAL 1 DAY TO MINUTE\`,
would yield \`Day\`, \`DayToHour\`, \`DayToMinute\`, as the \`IntervalUnit\` respectively.`,
  enum: [
    'DAY',
    'DAYTOHOUR',
    'DAYTOMINUTE',
    'DAYTOSECOND',
    'HOUR',
    'HOURTOMINUTE',
    'HOURTOSECOND',
    'MINUTE',
    'MINUTETOSECOND',
    'MONTH',
    'SECOND',
    'YEAR',
    'YEARTOMONTH'
  ]
} as const

export const $JsonUpdateFormat = {
  type: 'string',
  description: `Supported JSON data change event formats.

Each element in a JSON-formatted input stream specifies
an update to one or more records in an input table.  We support
several different ways to represent such updates.`,
  enum: ['insert_delete', 'weighted', 'debezium', 'snowflake', 'raw']
} as const

export const $KafkaHeader = {
  type: 'object',
  description: 'Kafka message header.',
  required: ['key'],
  properties: {
    key: {
      type: 'string'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaHeaderValue'
        }
      ],
      nullable: true
    }
  }
} as const

export const $KafkaHeaderValue = {
  type: 'string',
  format: 'binary',
  description: 'Kafka header value encoded as a UTF-8 string or a byte array.'
} as const

export const $KafkaInputConfig = {
  type: 'object',
  description: 'Configuration for reading data from Kafka topics with `InputTransport`.',
  required: ['topics'],
  properties: {
    fault_tolerance: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaInputFtConfig'
        }
      ],
      nullable: true
    },
    group_join_timeout_secs: {
      type: 'integer',
      format: 'int32',
      description: `Maximum timeout in seconds to wait for the endpoint to join the Kafka
consumer group during initialization.`,
      minimum: 0
    },
    kafka_service: {
      type: 'string',
      description: 'If specified, this service is used to provide defaults for the Kafka options.',
      nullable: true
    },
    log_level: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaLogLevel'
        }
      ],
      nullable: true
    },
    poller_threads: {
      type: 'integer',
      description: `Set to 1 or more to fix the number of threads used to poll
\`rdkafka\`. Multiple threads can increase performance with small Kafka
messages; for large messages, one thread is enough. In either case, too
many threads can harm performance. If unset, the default is 3, which
helps with small messages but will not harm performance with large
messagee`,
      nullable: true,
      minimum: 0
    },
    topics: {
      type: 'array',
      items: {
        type: 'string'
      },
      description: 'List of topics to subscribe to.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Options passed directly to \`rdkafka\`.

[\`librdkafka\` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
used to configure the Kafka consumer.  Not all options are valid with
this Kafka adapter:

* "enable.auto.commit", if present, must be set to "false",
* "enable.auto.offset.store", if present, must be set to "false"`
  }
} as const

export const $KafkaInputFtConfig = {
  type: 'object',
  description: 'Fault tolerance configuration for Kafka input connector.',
  properties: {
    consumer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for consumers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for consumers, and may be empty.`,
      additionalProperties: {
        type: 'string'
      }
    },
    create_missing_index: {
      type: 'boolean',
      description: `If this is true or unset, then the connector will create missing index
topics as needed.  If this is false, then a missing index topic is a
fatal error.`,
      nullable: true
    },
    index_suffix: {
      type: 'string',
      description: `Suffix to append to each data topic name, to give the name of a topic
that the connector uses for recording the division of the corresponding
data topic into steps.  Defaults to \`_input-index\`.

An index topic must have the same number of partitions as its
corresponding data topic.

If two or more fault-tolerant Kafka endpoints read from overlapping sets
of topics, they must specify different \`index_suffix\` values.`,
      nullable: true
    },
    max_step_bytes: {
      type: 'integer',
      format: 'int64',
      description: `Maximum number of bytes in a step.  Any individual message bigger than
this will be given a step of its own.`,
      nullable: true,
      minimum: 0
    },
    max_step_messages: {
      type: 'integer',
      format: 'int64',
      description: 'Maximum number of messages in a step.',
      nullable: true,
      minimum: 0
    },
    producer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for producers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for producers, and may be empty.`,
      additionalProperties: {
        type: 'string'
      }
    }
  }
} as const

export const $KafkaLogLevel = {
  type: 'string',
  description: 'Kafka logging levels.',
  enum: ['emerg', 'alert', 'critical', 'error', 'warning', 'notice', 'info', 'debug']
} as const

export const $KafkaOutputConfig = {
  type: 'object',
  description: 'Configuration for writing data to a Kafka topic with `OutputTransport`.',
  required: ['topic'],
  properties: {
    fault_tolerance: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaOutputFtConfig'
        }
      ],
      nullable: true
    },
    headers: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/KafkaHeader'
      },
      description: 'Kafka headers to be added to each message produced by this connector.'
    },
    initialization_timeout_secs: {
      type: 'integer',
      format: 'int32',
      description: `Maximum timeout in seconds to wait for the endpoint to connect to
a Kafka broker.

Defaults to 60.`,
      minimum: 0
    },
    kafka_service: {
      type: 'string',
      description: 'If specified, this service is used to provide defaults for the Kafka options.',
      nullable: true
    },
    log_level: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaLogLevel'
        }
      ],
      nullable: true
    },
    max_inflight_messages: {
      type: 'integer',
      format: 'int32',
      description: `Maximum number of unacknowledged messages buffered by the Kafka
producer.

Kafka producer buffers outgoing messages until it receives an
acknowledgement from the broker.  This configuration parameter
bounds the number of unacknowledged messages.  When the number of
unacknowledged messages reaches this limit, sending of a new message
blocks until additional acknowledgements arrive from the broker.

Defaults to 1000.`,
      minimum: 0
    },
    topic: {
      type: 'string',
      description: 'Topic to write to.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Options passed directly to \`rdkafka\`.

See [\`librdkafka\` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
used to configure the Kafka producer.`
  }
} as const

export const $KafkaOutputFtConfig = {
  type: 'object',
  description: 'Fault tolerance configuration for Kafka output connector.',
  properties: {
    consumer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for consumers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for consumers, and may be empty.`,
      additionalProperties: {
        type: 'string'
      }
    },
    producer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for producers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for producers, and may be empty.`,
      additionalProperties: {
        type: 'string'
      }
    }
  }
} as const

export const $ListPipelinesQueryParameters = {
  type: 'object',
  description: 'Query parameters for GET the list of pipelines.',
  properties: {
    code: {
      type: 'boolean',
      description: `Whether to include program code in the response (default: \`true\`).
Passing \`false\` reduces the response size, which is particularly handy
when frequently monitoring the endpoint over low bandwidth connections.`
    }
  }
} as const

export const $NeighborhoodQuery = {
  type: 'object',
  description: `A request to output a specific neighborhood of a table or view.
The neighborhood is defined in terms of its central point (\`anchor\`)
and the number of rows preceding and following the anchor to output.`,
  required: ['before', 'after'],
  properties: {
    after: {
      type: 'integer',
      format: 'int32',
      minimum: 0
    },
    anchor: {
      type: 'object',
      nullable: true
    },
    before: {
      type: 'integer',
      format: 'int32',
      minimum: 0
    }
  }
} as const

export const $NewApiKeyRequest = {
  type: 'object',
  description: 'Request to create a new API key.',
  required: ['name'],
  properties: {
    name: {
      type: 'string',
      description: 'Key name.',
      example: 'my-api-key'
    }
  }
} as const

export const $NewApiKeyResponse = {
  type: 'object',
  description: 'Response to a successful API key creation.',
  required: ['api_key_id', 'name', 'api_key'],
  properties: {
    api_key: {
      type: 'string',
      description: `Generated API key. There is no way to
retrieve this key again from the
pipeline-manager, so store it securely.`,
      example:
        'apikey:v5y5QNtlPNVMwkmNjKwFU8bbIu5lMge3yHbyddxAOdXlEo84SEoNn32DUhQaf1KLeI9aOOfnJjhQ1pYzMrU4wQXON6pm6BS7Zgzj46U2b8pwz1280vYBEtx41hiDBRP'
    },
    api_key_id: {
      $ref: '#/components/schemas/ApiKeyId'
    },
    name: {
      type: 'string',
      description: 'API key name',
      example: 'my-api-key'
    }
  }
} as const

export const $OutputBufferConfig = {
  type: 'object',
  properties: {
    enable_output_buffer: {
      type: 'boolean',
      description: `Enable output buffering.

The output buffering mechanism allows decoupling the rate at which the pipeline
pushes changes to the output transport from the rate of input changes.

By default, output updates produced by the pipeline are pushed directly to
the output transport. Some destinations may prefer to receive updates in fewer
bigger batches. For instance, when writing Parquet files, producing
one bigger file every few minutes is usually better than creating
small files every few milliseconds.

To achieve such input/output decoupling, users can enable output buffering by
setting the \`enable_output_buffer\` flag to \`true\`.  When buffering is enabled, output
updates produced by the pipeline are consolidated in an internal buffer and are
pushed to the output transport when one of several conditions is satisfied:

* data has been accumulated in the buffer for more than \`max_output_buffer_time_millis\`
milliseconds.
* buffer size exceeds \`max_output_buffer_size_records\` records.

This flag is \`false\` by default.`
    },
    max_output_buffer_size_records: {
      type: 'integer',
      description: `Maximum number of updates to be kept in the output buffer.

This parameter bounds the maximal size of the buffer.
Note that the size of the buffer is not always equal to the
total number of updates output by the pipeline. Updates to the
same record can overwrite or cancel previous updates.

By default, the buffer can grow indefinitely until one of
the other output conditions is satisfied.

NOTE: this configuration option requires the \`enable_output_buffer\` flag
to be set.`,
      minimum: 0
    },
    max_output_buffer_time_millis: {
      type: 'integer',
      description: `Maximum time in milliseconds data is kept in the output buffer.

By default, data is kept in the buffer indefinitely until one of
the other output conditions is satisfied.  When this option is
set the buffer will be flushed at most every
\`max_output_buffer_time_millis\` milliseconds.

NOTE: this configuration option requires the \`enable_output_buffer\` flag
to be set.`,
      minimum: 0
    }
  }
} as const

export const $OutputEndpointConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    {
      type: 'object',
      required: ['stream'],
      properties: {
        stream: {
          type: 'string',
          description: `The name of the output stream of the circuit that this endpoint is
connected to.`
        }
      }
    }
  ],
  description: 'Describes an output connector configuration'
} as const

export const $OutputQuery = {
  type: 'string',
  description: `A query over an output stream.

We currently do not support ad hoc queries.  Instead the client can use
three pre-defined queries to inspect the contents of a table or view.`,
  enum: ['table', 'neighborhood', 'quantiles']
} as const

export const $PatchPipeline = {
  type: 'object',
  description: `Patch (partially) update the pipeline.

Note that the patching only applies to the main fields, not subfields.
For instance, it is not possible to update only the number of workers;
it is required to again pass the whole runtime configuration with the
change.`,
  properties: {
    description: {
      type: 'string',
      nullable: true
    },
    name: {
      type: 'string',
      nullable: true
    },
    program_code: {
      type: 'string',
      nullable: true
    },
    program_config: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramConfig'
        }
      ],
      nullable: true
    },
    runtime_config: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeConfig'
        }
      ],
      nullable: true
    }
  }
} as const

export const $PipelineConfig = {
  allOf: [
    {
      type: 'object',
      description: `Global pipeline configuration settings. This is the publicly
exposed type for users to configure pipelines.`,
      properties: {
        cpu_profiler: {
          type: 'boolean',
          description: `Enable CPU profiler.

The default value is \`true\`.`
        },
        max_buffering_delay_usecs: {
          type: 'integer',
          format: 'int64',
          description: `Maximal delay in microseconds to wait for \`min_batch_size_records\` to
get buffered by the controller, defaults to 0.`,
          minimum: 0
        },
        min_batch_size_records: {
          type: 'integer',
          format: 'int64',
          description: `Minimal input batch size.

The controller delays pushing input records to the circuit until at
least \`min_batch_size_records\` records have been received (total
across all endpoints) or \`max_buffering_delay_usecs\` microseconds
have passed since at least one input records has been buffered.
Defaults to 0.`,
          minimum: 0
        },
        min_storage_bytes: {
          type: 'integer',
          description: `The minimum estimated number of bytes in a batch of data to write it to
storage.  This is provided for debugging and fine-tuning and should
ordinarily be left unset. It only has an effect when \`storage\` is set to
true.

A value of 0 will write even empty batches to storage, and nonzero
values provide a threshold.  \`usize::MAX\` would effectively disable
storage.`,
          nullable: true,
          minimum: 0
        },
        resources: {
          $ref: '#/components/schemas/ResourceConfig'
        },
        storage: {
          type: 'boolean',
          description: `Should persistent storage be enabled for this pipeline?

- If \`false\` (default), the pipeline's state is kept in in-memory data-structures.
This is useful if the pipeline is ephemeral and does not need to be recovered
after a restart. The pipeline will most likely run faster since it does not
need to read from, or write to disk

- If \`true\`, the pipeline state is stored in the specified location,
is persisted across restarts, and can be checkpointed and recovered.
This feature is currently experimental.`
        },
        tracing: {
          type: 'boolean',
          description: 'Enable pipeline tracing.'
        },
        tracing_endpoint_jaeger: {
          type: 'string',
          description: 'Jaeger tracing endpoint to send tracing information to.'
        },
        workers: {
          type: 'integer',
          format: 'int32',
          description: 'Number of DBSP worker threads.',
          minimum: 0
        }
      }
    },
    {
      type: 'object',
      required: ['inputs'],
      properties: {
        inputs: {
          type: 'object',
          description: 'Input endpoint configuration.',
          additionalProperties: {
            $ref: '#/components/schemas/InputEndpointConfig'
          }
        },
        name: {
          type: 'string',
          description: 'Pipeline name.',
          nullable: true
        },
        outputs: {
          type: 'object',
          description: 'Output endpoint configuration.',
          additionalProperties: {
            $ref: '#/components/schemas/OutputEndpointConfig'
          }
        },
        storage_config: {
          allOf: [
            {
              $ref: '#/components/schemas/StorageConfig'
            }
          ],
          nullable: true
        }
      }
    }
  ],
  description: `Pipeline deployment configuration.
It represents configuration entries directly provided by the user
(e.g., runtime configuration) and entries derived from the schema
of the compiled program (e.g., connectors). Storage configuration,
if applicable, is set by the runner.`
} as const

export const $PipelineDescr = {
  type: 'object',
  description: 'Pipeline descriptor.',
  required: ['name', 'description', 'runtime_config', 'program_code', 'program_config'],
  properties: {
    description: {
      type: 'string',
      description: 'Pipeline description.'
    },
    name: {
      type: 'string',
      description: 'Pipeline name.'
    },
    program_code: {
      type: 'string',
      description: 'Program SQL code.'
    },
    program_config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    runtime_config: {
      $ref: '#/components/schemas/RuntimeConfig'
    }
  }
} as const

export const $PipelineId = {
  type: 'string',
  format: 'uuid',
  description: 'Pipeline identifier.'
} as const

export const $PipelineStatus = {
  type: 'string',
  description: `Pipeline status.

This type represents the state of the pipeline tracked by the pipeline
runner and observed by the API client via the \`GET /pipeline\` endpoint.

### The lifecycle of a pipeline

The following automaton captures the lifecycle of the pipeline.  Individual
states and transitions of the automaton are described below.

* In addition to the transitions shown in the diagram, all states have an
implicit "forced shutdown" transition to the \`Shutdown\` state.  This
transition is triggered when the pipeline runner is unable to communicate
with the pipeline and thereby forces a shutdown.

* States labeled with the hourglass symbol (⌛) are **timed** states.  The
automaton stays in timed state until the corresponding operation completes
or until the runner performs a forced shutdown of the pipeline after a
pre-defined timeout period.

* State transitions labeled with API endpoint names (\`/deploy\`, \`/start\`,
\`/pause\`, \`/shutdown\`) are triggered by invoking corresponding endpoint,
e.g., \`POST /v0/pipelines/{pipeline_id}/start\`.

\`\`\`text
Shutdown◄────┐
│         │
/deploy│         │
│   ⌛ShuttingDown
▼         ▲
⌛Provisioning    │
│         │
Provisioned        │         │
▼         │/shutdown
⌛Initializing    │
│         │
┌────────┴─────────┴─┐
│        ▼           │
│      Paused        │
│      │    ▲        │
│/start│    │/pause  │
│      ▼    │        │
│     Running        │
└──────────┬─────────┘
│
▼
Failed
\`\`\`

### Desired and actual status

We use the desired state model to manage the lifecycle of a pipeline.
In this model, the pipeline has two status attributes associated with
it at runtime: the **desired** status, which represents what the user
would like the pipeline to do, and the **current** status, which
represents the actual state of the pipeline.  The pipeline runner
service continuously monitors both fields and steers the pipeline
towards the desired state specified by the user.
Only three of the states in the pipeline automaton above can be
used as desired statuses: \`Paused\`, \`Running\`, and \`Shutdown\`.
These statuses are selected by invoking REST endpoints shown
in the diagram.

The user can monitor the current state of the pipeline via the
\`/status\` endpoint, which returns an object of type \`Pipeline\`.
In a typical scenario, the user first sets
the desired state, e.g., by invoking the \`/deploy\` endpoint, and
then polls the \`GET /pipeline\` endpoint to monitor the actual status
of the pipeline until its \`state.current_status\` attribute changes
to "paused" indicating that the pipeline has been successfully
initialized, or "failed", indicating an error.`,
  enum: ['Shutdown', 'Provisioning', 'Initializing', 'Paused', 'Running', 'ShuttingDown', 'Failed']
} as const

export const $ProgramConfig = {
  type: 'object',
  description: 'Program configuration.',
  properties: {
    profile: {
      allOf: [
        {
          $ref: '#/components/schemas/CompilationProfile'
        }
      ],
      nullable: true
    }
  }
} as const

export const $ProgramInfo = {
  type: 'object',
  description: 'Program information which includes schema, input connectors and output connectors.',
  required: ['schema', 'input_connectors', 'output_connectors'],
  properties: {
    input_connectors: {
      type: 'object',
      description: 'Input connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/InputEndpointConfig'
      }
    },
    output_connectors: {
      type: 'object',
      description: 'Output connectors derived from the schema.',
      additionalProperties: {
        $ref: '#/components/schemas/OutputEndpointConfig'
      }
    },
    schema: {
      $ref: '#/components/schemas/ProgramSchema'
    }
  }
} as const

export const $ProgramSchema = {
  type: 'object',
  description: `A struct containing the tables (inputs) and views for a program.

Parse from the JSON data-type of the DDL generated by the SQL compiler.`,
  required: ['inputs', 'outputs'],
  properties: {
    inputs: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Relation'
      }
    },
    outputs: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Relation'
      }
    }
  }
} as const

export const $ProgramStatus = {
  oneOf: [
    {
      type: 'string',
      description: `Compilation request received from the user; program has been placed
in the queue.`,
      enum: ['Pending']
    },
    {
      type: 'string',
      description: 'Compilation of SQL -> Rust in progress.',
      enum: ['CompilingSql']
    },
    {
      type: 'string',
      description: 'Compiling Rust -> executable in progress.',
      enum: ['CompilingRust']
    },
    {
      type: 'string',
      description: 'Compilation succeeded.',
      enum: ['Success']
    },
    {
      type: 'object',
      required: ['SqlError'],
      properties: {
        SqlError: {
          type: 'array',
          items: {
            $ref: '#/components/schemas/SqlCompilerMessage'
          },
          description: 'SQL compiler returned an error.'
        }
      }
    },
    {
      type: 'object',
      required: ['RustError'],
      properties: {
        RustError: {
          type: 'string',
          description: 'Rust compiler returned an error.'
        }
      }
    },
    {
      type: 'object',
      required: ['SystemError'],
      properties: {
        SystemError: {
          type: 'string',
          description: 'System/OS returned an error when trying to invoke commands.'
        }
      }
    }
  ],
  description: 'Program compilation status.'
} as const

export const $ProviderAwsCognito = {
  type: 'object',
  required: ['jwk_uri', 'login_url', 'logout_url'],
  properties: {
    jwk_uri: {
      type: 'string'
    },
    login_url: {
      type: 'string'
    },
    logout_url: {
      type: 'string'
    }
  }
} as const

export const $ProviderGoogleIdentity = {
  type: 'object',
  required: ['jwk_uri', 'client_id'],
  properties: {
    client_id: {
      type: 'string'
    },
    jwk_uri: {
      type: 'string'
    }
  }
} as const

export const $ReadStrategy = {
  oneOf: [
    {
      type: 'object',
      description: 'Read a single object specified by a key',
      required: ['key', 'type'],
      properties: {
        key: {
          type: 'string'
        },
        type: {
          type: 'string',
          enum: ['SingleKey']
        }
      }
    },
    {
      type: 'object',
      description: 'Read all objects whose keys match a prefix',
      required: ['prefix', 'type'],
      properties: {
        prefix: {
          type: 'string'
        },
        type: {
          type: 'string',
          enum: ['Prefix']
        }
      }
    }
  ],
  description: 'Strategy that determines which objects to read from a given bucket',
  discriminator: {
    propertyName: 'type'
  }
} as const

export const $Relation = {
  type: 'object',
  description: `A SQL table or view. It has a name and a list of fields.

Matches the Calcite JSON format.`,
  required: ['name', 'fields'],
  properties: {
    case_sensitive: {
      type: 'boolean'
    },
    fields: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Field'
      }
    },
    materialized: {
      type: 'boolean'
    },
    name: {
      type: 'string'
    },
    properties: {
      type: 'object',
      additionalProperties: {
        type: 'string'
      }
    }
  }
} as const

export const $ResourceConfig = {
  type: 'object',
  properties: {
    cpu_cores_max: {
      type: 'integer',
      format: 'int64',
      description: `The maximum number of CPU cores to reserve
for an instance of this pipeline`,
      nullable: true,
      minimum: 0
    },
    cpu_cores_min: {
      type: 'integer',
      format: 'int64',
      description: `The minimum number of CPU cores to reserve
for an instance of this pipeline`,
      nullable: true,
      minimum: 0
    },
    memory_mb_max: {
      type: 'integer',
      format: 'int64',
      description: `The maximum memory in Megabytes to reserve
for an instance of this pipeline`,
      nullable: true,
      minimum: 0
    },
    memory_mb_min: {
      type: 'integer',
      format: 'int64',
      description: `The minimum memory in Megabytes to reserve
for an instance of this pipeline`,
      nullable: true,
      minimum: 0
    },
    storage_class: {
      type: 'string',
      description: `Storage class to use for an instance of this pipeline.
The class determines storage performance such as IOPS and throughput.`,
      nullable: true
    },
    storage_mb_max: {
      type: 'integer',
      format: 'int64',
      description: `The total storage in Megabytes to reserve
for an instance of this pipeline`,
      nullable: true,
      minimum: 0
    }
  }
} as const

export const $RngFieldSettings = {
  type: 'object',
  description: 'Configuration for generating random data for a field of a table.',
  properties: {
    e: {
      type: 'integer',
      format: 'int64',
      description: `The frequency rank exponent for the Zipf distribution.

- This value is only used if the strategy is set to \`Zipf\`.
- The default value is 1.0.`
    },
    fields: {
      type: 'object',
      description:
        'Specifies the values that the generator should produce in case the field is a struct type.',
      additionalProperties: {
        $ref: '#/components/schemas/RngFieldSettings'
      },
      nullable: true
    },
    key: {
      allOf: [
        {
          $ref: '#/components/schemas/RngFieldSettings'
        }
      ],
      nullable: true
    },
    null_percentage: {
      type: 'integer',
      description: `Percentage of records where this field should be set to NULL.

If not set, the generator will produce only records with non-NULL values.
If set to \`1..=100\`, the generator will produce records with NULL values with the specified percentage.`,
      nullable: true,
      minimum: 0
    },
    range: {
      type: 'array',
      items: {
        allOf: [
          {
            type: 'integer',
            format: 'int64'
          },
          {
            type: 'integer',
            format: 'int64'
          }
        ]
      },
      description: `An optional, exclusive range [a, b) to limit the range of values the generator should produce.

- For integer/floating point types specifies min/max values.
If not set, the generator will produce values for the entire range of the type for number types.
- For string/binary types specifies min/max length, values are required to be >=0.
If not set, a range of [0, 25) is used by default.
- For timestamp types specifies the min/max in milliseconds from the number of non-leap
milliseconds since January 1, 1970 0:00:00.000 UTC (aka “UNIX timestamp”).
If not set, a range of [0, 4102444800) is used by default (1970-01-01 -- 2100-01-01).
- For time types specifies the min/max in milliseconds.
If not set, the range is 24h. Range values are required to be >=0.
- For date types specifies the min/max in days from the number of days since January 1, 1970.
If not set, a range of [0, 54787) is used by default (1970-01-01 -- 2100-01-01).
- For array types specifies the min/max number of elements.
If not set, a range of [0, 5) is used by default. Range values are required to be >=0.
- For map types specifies the min/max number of key-value pairs.
If not set, a range of [0, 5) is used by default.
- For struct/boolean/null types \`range\` is ignored.`,
      nullable: true
    },
    scale: {
      type: 'integer',
      format: 'int64',
      description: `A scale factor to apply a multiplier to the generated value.

- For integer/floating point types, the value is multiplied by the scale factor.
- For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.
- For time types, the generated value (milliseconds) is multiplied by the scale factor.
- For date types, the generated value (days) is multiplied by the scale factor.
- For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.

- If \`values\` is specified, the scale factor is ignored.
- If \`range\` is specified and the range is required to be positive (struct, map, array etc.)
the scale factor is required to be positive too.

The default scale factor is 1.`
    },
    strategy: {
      $ref: '#/components/schemas/DatagenStrategy'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/RngFieldSettings'
        }
      ],
      nullable: true
    },
    values: {
      type: 'array',
      items: {
        type: 'object'
      },
      description: `An optional set of values the generator will pick from.

If set, the generator will pick values from the specified set.
If not set, the generator will produce values according to the specified range.
If set to an empty set, the generator will produce NULL values.
If set to a single value, the generator will produce only that value.

Note that \`range\` is ignored if \`values\` is set.`,
      nullable: true
    }
  }
} as const

export const $RuntimeConfig = {
  type: 'object',
  description: `Global pipeline configuration settings. This is the publicly
exposed type for users to configure pipelines.`,
  properties: {
    cpu_profiler: {
      type: 'boolean',
      description: `Enable CPU profiler.

The default value is \`true\`.`
    },
    max_buffering_delay_usecs: {
      type: 'integer',
      format: 'int64',
      description: `Maximal delay in microseconds to wait for \`min_batch_size_records\` to
get buffered by the controller, defaults to 0.`,
      minimum: 0
    },
    min_batch_size_records: {
      type: 'integer',
      format: 'int64',
      description: `Minimal input batch size.

The controller delays pushing input records to the circuit until at
least \`min_batch_size_records\` records have been received (total
across all endpoints) or \`max_buffering_delay_usecs\` microseconds
have passed since at least one input records has been buffered.
Defaults to 0.`,
      minimum: 0
    },
    min_storage_bytes: {
      type: 'integer',
      description: `The minimum estimated number of bytes in a batch of data to write it to
storage.  This is provided for debugging and fine-tuning and should
ordinarily be left unset. It only has an effect when \`storage\` is set to
true.

A value of 0 will write even empty batches to storage, and nonzero
values provide a threshold.  \`usize::MAX\` would effectively disable
storage.`,
      nullable: true,
      minimum: 0
    },
    resources: {
      $ref: '#/components/schemas/ResourceConfig'
    },
    storage: {
      type: 'boolean',
      description: `Should persistent storage be enabled for this pipeline?

- If \`false\` (default), the pipeline's state is kept in in-memory data-structures.
This is useful if the pipeline is ephemeral and does not need to be recovered
after a restart. The pipeline will most likely run faster since it does not
need to read from, or write to disk

- If \`true\`, the pipeline state is stored in the specified location,
is persisted across restarts, and can be checkpointed and recovered.
This feature is currently experimental.`
    },
    tracing: {
      type: 'boolean',
      description: 'Enable pipeline tracing.'
    },
    tracing_endpoint_jaeger: {
      type: 'string',
      description: 'Jaeger tracing endpoint to send tracing information to.'
    },
    workers: {
      type: 'integer',
      format: 'int32',
      description: 'Number of DBSP worker threads.',
      minimum: 0
    }
  }
} as const

export const $S3InputConfig = {
  type: 'object',
  description: 'Configuration for reading data from AWS S3.',
  required: ['credentials', 'region', 'bucket_name', 'read_strategy'],
  properties: {
    bucket_name: {
      type: 'string',
      description: 'S3 bucket name to access'
    },
    consume_strategy: {
      $ref: '#/components/schemas/ConsumeStrategy'
    },
    credentials: {
      $ref: '#/components/schemas/AwsCredentials'
    },
    read_strategy: {
      $ref: '#/components/schemas/ReadStrategy'
    },
    region: {
      type: 'string',
      description: 'AWS region'
    }
  }
} as const

export const $SqlCompilerMessage = {
  type: 'object',
  description: `A SQL compiler error.

The SQL compiler returns a list of errors in the following JSON format if
it's invoked with the \`-je\` option.

\`\`\`ignore
[ {
"startLineNumber" : 2,
"startColumn" : 4,
"endLineNumber" : 2,
"endColumn" : 8,
"warning" : false,
"errorType" : "PRIMARY KEY cannot be nullable",
"message" : "PRIMARY KEY column 'C' has type INTEGER, which is nullable",
"snippet" : "    2|   c INT PRIMARY KEY\n         ^^^^^\n    3|);\n"
} ]
\`\`\``,
  required: [
    'startLineNumber',
    'startColumn',
    'endLineNumber',
    'endColumn',
    'warning',
    'errorType',
    'message'
  ],
  properties: {
    endColumn: {
      type: 'integer',
      minimum: 0
    },
    endLineNumber: {
      type: 'integer',
      minimum: 0
    },
    errorType: {
      type: 'string'
    },
    message: {
      type: 'string'
    },
    snippet: {
      type: 'string',
      nullable: true
    },
    startColumn: {
      type: 'integer',
      minimum: 0
    },
    startLineNumber: {
      type: 'integer',
      minimum: 0
    },
    warning: {
      type: 'boolean'
    }
  }
} as const

export const $SqlType = {
  oneOf: [
    {
      type: 'string',
      description: 'SQL `BOOLEAN` type.',
      enum: ['BOOLEAN']
    },
    {
      type: 'string',
      description: 'SQL `TINYINT` type.',
      enum: ['TINYINT']
    },
    {
      type: 'string',
      description: 'SQL `SMALLINT` or `INT2` type.',
      enum: ['SMALLINT']
    },
    {
      type: 'string',
      description: 'SQL `INTEGER`, `INT`, `SIGNED`, `INT4` type.',
      enum: ['INTEGER']
    },
    {
      type: 'string',
      description: 'SQL `BIGINT` or `INT64` type.',
      enum: ['BIGINT']
    },
    {
      type: 'string',
      description: 'SQL `REAL` or `FLOAT4` or `FLOAT32` type.',
      enum: ['REAL']
    },
    {
      type: 'string',
      description: 'SQL `DOUBLE` or `FLOAT8` or `FLOAT64` type.',
      enum: ['DOUBLE']
    },
    {
      type: 'string',
      description: 'SQL `DECIMAL` or `DEC` or `NUMERIC` type.',
      enum: ['DECIMAL']
    },
    {
      type: 'string',
      description: 'SQL `CHAR(n)` or `CHARACTER(n)` type.',
      enum: ['CHAR']
    },
    {
      type: 'string',
      description: 'SQL `VARCHAR`, `CHARACTER VARYING`, `TEXT`, or `STRING` type.',
      enum: ['VARCHAR']
    },
    {
      type: 'string',
      description: 'SQL `BINARY(n)` type.',
      enum: ['BINARY']
    },
    {
      type: 'string',
      description: 'SQL `VARBINARY` or `BYTEA` type.',
      enum: ['VARBINARY']
    },
    {
      type: 'string',
      description: 'SQL `TIME` type.',
      enum: ['TIME']
    },
    {
      type: 'string',
      description: 'SQL `DATE` type.',
      enum: ['DATE']
    },
    {
      type: 'string',
      description: 'SQL `TIMESTAMP` type.',
      enum: ['TIMESTAMP']
    },
    {
      type: 'object',
      required: ['Interval'],
      properties: {
        Interval: {
          $ref: '#/components/schemas/IntervalUnit'
        }
      }
    },
    {
      type: 'string',
      description: 'SQL `ARRAY` type.',
      enum: ['ARRAY']
    },
    {
      type: 'string',
      description: 'A complex SQL struct type (`CREATE TYPE x ...`).',
      enum: ['STRUCT']
    },
    {
      type: 'string',
      description: 'SQL `MAP` type.',
      enum: ['MAP']
    },
    {
      type: 'string',
      description: 'SQL `NULL` type.',
      enum: ['NULL']
    }
  ],
  description: 'The available SQL types as specified in `CREATE` statements.'
} as const

export const $StorageCacheConfig = {
  type: 'string',
  description: 'How to cache access to storage within a Feldera pipeline.',
  enum: ['page_cache', 'feldera_cache']
} as const

export const $StorageConfig = {
  type: 'object',
  description: 'Configuration for persistent storage in a [`PipelineConfig`].',
  required: ['path', 'cache'],
  properties: {
    cache: {
      $ref: '#/components/schemas/StorageCacheConfig'
    },
    path: {
      type: 'string',
      description: `The location where the pipeline state is stored or will be stored.

It should point to a path on the file-system of the machine/container
where the pipeline will run. If that path doesn't exist yet, or if it
does not contain any checkpoints, then the pipeline creates it and
starts from an initial state in which no data has yet been received. If
it does exist, then the pipeline starts from the most recent checkpoint
that already exists there. In either case, (further) checkpoints will be
written there.`
    }
  }
} as const

export const $TransportConfig = {
  oneOf: [
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileInputConfig'
        },
        name: {
          type: 'string',
          enum: ['file_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['file_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/KafkaInputConfig'
        },
        name: {
          type: 'string',
          enum: ['kafka_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/KafkaOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['kafka_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/UrlInputConfig'
        },
        name: {
          type: 'string',
          enum: ['url_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/S3InputConfig'
        },
        name: {
          type: 'string',
          enum: ['s3_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DeltaTableReaderConfig'
        },
        name: {
          type: 'string',
          enum: ['delta_table_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DeltaTableWriterConfig'
        },
        name: {
          type: 'string',
          enum: ['delta_table_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DatagenInputConfig'
        },
        name: {
          type: 'string',
          enum: ['datagen']
        }
      }
    },
    {
      type: 'object',
      required: ['name'],
      properties: {
        name: {
          type: 'string',
          enum: ['http_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name'],
      properties: {
        name: {
          type: 'string',
          enum: ['http_output']
        }
      }
    }
  ],
  description: `Transport-specific endpoint configuration passed to
\`crate::OutputTransport::new_endpoint\`
and \`crate::InputTransport::new_endpoint\`.`,
  discriminator: {
    propertyName: 'name'
  }
} as const

export const $UrlInputConfig = {
  type: 'object',
  description: `Configuration for reading data from an HTTP or HTTPS URL with
\`UrlInputTransport\`.`,
  required: ['path'],
  properties: {
    path: {
      type: 'string',
      description: 'URL.'
    },
    pause_timeout: {
      type: 'integer',
      format: 'int32',
      description: `Timeout before disconnection when paused.

If the pipeline is paused, or if the input adapter reads data faster
than the pipeline can process it, then the controller will pause the
input adapter. If the input adapter stays paused longer than this
timeout, it will drop the network connection to the server. It will
automatically reconnect when the input adapter starts running again.`,
      minimum: 0
    }
  }
} as const

export const $Version = {
  type: 'integer',
  format: 'int64',
  description: 'Version number.'
} as const
