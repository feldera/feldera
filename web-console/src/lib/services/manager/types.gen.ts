// This file is auto-generated by @hey-api/openapi-ts

/**
 * API key descriptor.
 */
export type ApiKeyDescr = {
  id: ApiKeyId
  name: string
  scopes: Array<ApiPermission>
}

/**
 * API key identifier.
 */
export type ApiKeyId = string

/**
 * Permission types for invoking API endpoints.
 */
export type ApiPermission = 'Read' | 'Write'

export type AuthProvider =
  | {
      AwsCognito: ProviderAwsCognito
    }
  | {
      GoogleIdentity: ProviderGoogleIdentity
    }

/**
 * Configuration to authenticate against AWS
 */
export type AwsCredentials =
  | {
      type: 'NoSignRequest'
    }
  | {
      aws_access_key_id: string
      aws_secret_access_key: string
      type: 'AccessKey'
    }

export type type = 'NoSignRequest'

/**
 * A set of updates to a SQL table or view.
 *
 * The `sequence_number` field stores the offset of the chunk relative to the
 * start of the stream and can be used to implement reliable delivery.
 * The payload is stored in the `bin_data`, `text_data`, or `json_data` field
 * depending on the data format used.
 */
export type Chunk = {
  /**
   * Base64 encoded binary payload, e.g., bincode.
   */
  bin_data?: (Blob | File) | null
  /**
   * JSON payload.
   */
  json_data?: {
    [key: string]: unknown
  } | null
  sequence_number: number
  /**
   * Text payload, e.g., CSV.
   */
  text_data?: string | null
}

/**
 * A SQL column type description.
 *
 * Matches the Calcite JSON format.
 */
export type ColumnType = {
  component?: ColumnType | null
  /**
   * The fields of the type (if available).
   *
   * For example this would specify the fields of a `CREATE TYPE` construct.
   *
   * ```sql
   * CREATE TYPE person_typ AS (
   * firstname       VARCHAR(30),
   * lastname        VARCHAR(30),
   * address         ADDRESS_TYP
   * );
   * ```
   *
   * Would lead to the following `fields` value:
   *
   * ```sql
   * [
   * ColumnType { name: "firstname, ... },
   * ColumnType { name: "lastname", ... },
   * ColumnType { name: "address", fields: [ ... ] }
   * ]
   * ```
   */
  fields?: Array<Field> | null
  key?: ColumnType | null
  /**
   * Does the type accept NULL values?
   */
  nullable: boolean
  /**
   * Precision of the type.
   *
   * # Examples
   * - `VARCHAR` sets precision to `-1`.
   * - `VARCHAR(255)` sets precision to `255`.
   * - `BIGINT`, `DATE`, `FLOAT`, `DOUBLE`, `GEOMETRY`, etc. sets precision
   * to None
   * - `TIME`, `TIMESTAMP` set precision to `0`.
   */
  precision?: number | null
  /**
   * The scale of the type.
   *
   * # Example
   * - `DECIMAL(1,2)` sets scale to `2`.
   */
  scale?: number | null
  type?: SqlType
  value?: ColumnType | null
}

/**
 * Enumeration of possible compilation profiles that can be passed to the Rust compiler
 * as an argument via `cargo build --profile <>`. A compilation profile affects among
 * other things the compilation speed (how long till the program is ready to be run)
 * and runtime speed (the performance while running).
 */
export type CompilationProfile = 'dev' | 'unoptimized' | 'optimized'

/**
 * A data connector's configuration
 */
export type ConnectorConfig = OutputBufferConfig & {
  format?: FormatConfig | null
  /**
   * Backpressure threshold.
   *
   * Maximal number of records queued by the endpoint before the endpoint
   * is paused by the backpressure mechanism.
   *
   * For input endpoints, this setting bounds the number of records that have
   * been received from the input transport but haven't yet been consumed by
   * the circuit since the circuit, since the circuit is still busy processing
   * previous inputs.
   *
   * For output endpoints, this setting bounds the number of records that have
   * been produced by the circuit but not yet sent via the output transport endpoint
   * nor stored in the output buffer (see `enable_output_buffer`).
   *
   * Note that this is not a hard bound: there can be a small delay between
   * the backpressure mechanism is triggered and the endpoint is paused, during
   * which more data may be queued.
   *
   * The default is 1 million.
   */
  max_queued_records?: number
  /**
   * Create connector in paused state.
   *
   * The default is `false`.
   */
  paused?: boolean
  transport: TransportConfig
}

/**
 * Strategy to feed a fetched object into an InputConsumer.
 */
export type ConsumeStrategy =
  | {
      type: 'Fragment'
    }
  | {
      type: 'Object'
    }

export type type2 = 'Fragment'

/**
 * Configuration for generating random data for a table.
 */
export type DatagenInputConfig = {
  /**
   * The sequence of generations to perform.
   *
   * If not set, the generator will produce a single sequence with default settings.
   * If set, the generator will produce the specified sequences in sequential order.
   *
   * Note that if one of the sequences before the last one generates an unlimited number of rows
   * the following sequences will not be executed.
   */
  plan?: Array<GenerationPlan>
  /**
   * Optional seed for the random generator.
   *
   * Setting this to a fixed value will make the generator produce the same sequence of records
   * every time the pipeline is run.
   *
   * # Notes
   * - To ensure the set of generated input records is deterministic across multiple runs,
   * apart from setting a seed, `workers` also needs to remain unchanged.
   * - The input will arrive in non-deterministic order if `workers > 1`.
   */
  seed?: number | null
  /**
   * Number of workers to use for generating data.
   */
  workers?: number
}

/**
 * Strategy used to generate values.
 */
export type DatagenStrategy =
  | 'increment'
  | 'uniform'
  | 'zipf'
  | 'word'
  | 'words'
  | 'sentence'
  | 'sentences'
  | 'paragraph'
  | 'paragraphs'
  | 'first_name'
  | 'last_name'
  | 'title'
  | 'suffix'
  | 'name'
  | 'name_with_title'
  | 'domain_suffix'
  | 'email'
  | 'username'
  | 'password'
  | 'field'
  | 'position'
  | 'seniority'
  | 'job_title'
  | 'i_pv4'
  | 'i_pv6'
  | 'i_p'
  | 'm_a_c_address'
  | 'user_agent'
  | 'rfc_status_code'
  | 'valid_status_code'
  | 'company_suffix'
  | 'company_name'
  | 'buzzword'
  | 'buzzword_middle'
  | 'buzzword_tail'
  | 'catch_phrase'
  | 'bs_verb'
  | 'bs_adj'
  | 'bs_noun'
  | 'bs'
  | 'profession'
  | 'industry'
  | 'currency_code'
  | 'currency_name'
  | 'currency_symbol'
  | 'credit_card_number'
  | 'city_prefix'
  | 'city_suffix'
  | 'city_name'
  | 'country_name'
  | 'country_code'
  | 'street_suffix'
  | 'street_name'
  | 'time_zone'
  | 'state_name'
  | 'state_abbr'
  | 'secondary_address_type'
  | 'secondary_address'
  | 'zip_code'
  | 'post_code'
  | 'building_number'
  | 'latitude'
  | 'longitude'
  | 'isbn'
  | 'isbn13'
  | 'isbn10'
  | 'phone_number'
  | 'cell_number'
  | 'file_path'
  | 'file_name'
  | 'file_extension'
  | 'dir_path'

/**
 * Delta table read mode.
 *
 * Three options are available:
 *
 * * `snapshot` - read a snapshot of the table and stop.
 *
 * * `follow` - continuously ingest changes to the table, starting from a specified version
 * or timestamp.
 *
 * * `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion
 * mode.
 */
export type DeltaTableIngestMode = 'snapshot' | 'follow' | 'snapshot_and_follow'

/**
 * Delta table output connector configuration.
 */
export type DeltaTableReaderConfig = {
  /**
   * Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
   * "2024-12-09T16:09:53+00:00.
   *
   * When this option is set, the connector finds and opens the version of the table as of the
   * specified point in time.  In `snapshot` and `snapshot_and_follow` modes, it retrieves the
   * snapshot of this version of the table (based on the server time recorded in the transaction
   * log, not the event time encoded in the data).  In `follow` and `snapshot_and_follow` modes, it
   * follows transaction log records **after** this version.
   *
   * Note: at most one of `version` and `datetime` options can be specified.
   * When neither of the two options is specified, the latest committed version of the table
   * is used.
   */
  datetime?: string | null
  mode: DeltaTableIngestMode
  /**
   * Optional row filter.
   *
   * This option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.
   *
   * When specified, only rows that satisfy the filter condition are included in the
   * snapshot.  The condition must be a valid SQL Boolean expression that can be used in
   * the `where` clause of the `select * from snapshot where ...` query.
   *
   * This option can be used to specify the range of event times to include in the snapshot,
   * e.g.: `ts BETWEEN '2005-01-01 00:00:00' AND '2010-12-31 23:59:59'`.
   */
  snapshot_filter?: string | null
  /**
   * Table column that serves as an event timestamp.
   *
   *
   * When this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,
   * the snapshot of the table is ingested in the timestamp order.  This setting is required
   * for tables declared with the
   * [`LATENESS`](https://www.feldera.com/docs/sql/streaming#lateness-expressions) attribute
   * in Feldera SQL. It impacts the performance of the connector, since data must be sorted
   * before pushing it to the pipeline; therefore it is not recommended to use this
   * settings for tables without `LATENESS`.
   */
  timestamp_column?: string | null
  /**
   * Table URI.
   *
   * Example: "s3://feldera-fraud-detection-data/demographics_train"
   */
  uri: string
  /**
   * Optional table version.
   *
   * When this option is set, the connector finds and opens the specified version of the table.
   * In `snapshot` and `snapshot_and_follow` modes, it retrieves the snapshot of this version of
   * the table.  In `follow` and `snapshot_and_follow` modes, it follows transaction log records
   * **after** this version.
   *
   * Note: at most one of `version` and `datetime` options can be specified.
   * When neither of the two options is specified, the latest committed version of the table
   * is used.
   */
  version?: number | null
  /**
   * Storage options for configuring backend object store.
   *
   * For specific options available for different storage backends, see:
   * * [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
   * * [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
   * * [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
   */
  '[key: string]': (string | DeltaTableIngestMode | number) | undefined
}

/**
 * Delta table write mode.
 *
 * Determines how the Delta table connector handles an existing table at the target location.
 */
export type DeltaTableWriteMode = 'append' | 'truncate' | 'error_if_exists'

/**
 * Delta table output connector configuration.
 */
export type DeltaTableWriterConfig = {
  mode?: DeltaTableWriteMode
  /**
   * Table URI.
   */
  uri: string
  /**
   * Storage options for configuring backend object store.
   *
   * For specific options available for different storage backends, see:
   * * [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
   * * [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
   * * [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
   */
  '[key: string]': (string | DeltaTableWriteMode) | undefined
}

export type Demo = {
  pipeline: PipelineDescr
  /**
   * Demo title.
   */
  title: string
}

export type EgressMode = 'watch' | 'snapshot'

/**
 * Information returned by REST API endpoints on error.
 */
export type ErrorResponse = {
  /**
   * Detailed error metadata.
   * The contents of this field is determined by `error_code`.
   */
  details: {
    [key: string]: unknown
  }
  /**
   * Error code is a string that specifies this error type.
   */
  error_code: string
  /**
   * Human-readable error message.
   */
  message: string
}

/**
 * Pipeline descriptor which besides the basic fields in direct regular control of the user
 * also has all additional fields generated and maintained by the back-end.
 */
export type ExtendedPipelineDescr = {
  /**
   * Timestamp when the pipeline was originally created.
   */
  created_at: string
  deployment_config?: PipelineConfig | null
  deployment_desired_status: PipelineStatus
  deployment_error?: ErrorResponse | null
  /**
   * Location where the pipeline can be reached at runtime
   * (e.g., a TCP port number or a URI).
   */
  deployment_location?: string | null
  deployment_status: PipelineStatus
  /**
   * Time when the pipeline was assigned its current status
   * of the pipeline.
   */
  deployment_status_since: string
  /**
   * Pipeline description.
   */
  description: string
  id: PipelineId
  /**
   * Pipeline name.
   */
  name: string
  /**
   * URL where to download the program binary from.
   */
  program_binary_url?: string | null
  /**
   * Program SQL code.
   */
  program_code: string
  program_config: ProgramConfig
  program_info?: ProgramInfo | null
  program_status: ProgramStatus
  /**
   * Timestamp when the current program status was set.
   */
  program_status_since: string
  program_version: Version
  runtime_config: RuntimeConfig
  version: Version
}

/**
 * Extended pipeline descriptor with code being optionally included.
 */
export type ExtendedPipelineDescrOptionalCode = {
  created_at: string
  deployment_config?: PipelineConfig | null
  deployment_desired_status: PipelineStatus
  deployment_error?: ErrorResponse | null
  deployment_location?: string | null
  deployment_status: PipelineStatus
  deployment_status_since: string
  description: string
  id: PipelineId
  name: string
  program_binary_url?: string | null
  program_code?: string | null
  program_config: ProgramConfig
  program_info?: ProgramInfo | null
  program_status: ProgramStatus
  program_status_since: string
  program_version: Version
  runtime_config: RuntimeConfig
  version: Version
}

/**
 * A SQL field.
 *
 * Matches the SQL compiler JSON format.
 */
export type Field = {
  case_sensitive?: boolean
  columntype: ColumnType
  name: string
}

/**
 * Configuration for reading data from a file with `FileInputTransport`
 */
export type FileInputConfig = {
  /**
   * Read buffer size.
   *
   * Default: when this parameter is not specified, a platform-specific
   * default is used.
   */
  buffer_size_bytes?: number | null
  /**
   * Enable file following.
   *
   * When `false`, the endpoint outputs an `InputConsumer::eoi`
   * message and stops upon reaching the end of file.  When `true`, the
   * endpoint will keep watching the file and outputting any new content
   * appended to it.
   */
  follow?: boolean
  /**
   * File path.
   */
  path: string
}

/**
 * Configuration for writing data to a file with `FileOutputTransport`.
 */
export type FileOutputConfig = {
  /**
   * File path.
   */
  path: string
}

/**
 * Data format specification used to parse raw data received from the
 * endpoint or to encode data sent to the endpoint.
 */
export type FormatConfig = {
  /**
   * Format-specific parser or encoder configuration.
   */
  config?: {
    [key: string]: unknown
  }
  /**
   * Format name, e.g., "csv", "json", "bincode", etc.
   */
  name: string
}

/**
 * A random generation plan for a table that generates either a limited amount of rows or runs continuously.
 */
export type GenerationPlan = {
  /**
   * Specifies the values that the generator should produce.
   */
  fields?: {
    [key: string]: RngFieldSettings
  }
  /**
   * Total number of new rows to generate.
   *
   * If not set, the generator will produce new/unique records as long as the pipeline is running.
   * If set to 0, the table will always remain empty.
   * If set, the generator will produce new records until the specified limit is reached.
   *
   * Note that if the table has one or more primary keys that don't use the `increment` strategy to
   * generate the key there is a potential that an update is generated instead of an insert. In
   * this case it's possible the total number of records is less than the specified limit.
   */
  limit?: number | null
  /**
   * Non-zero number of rows to generate per second.
   *
   * If not set, the generator will produce rows as fast as possible.
   */
  rate?: number | null
}

/**
 * Describes an input connector configuration
 */
export type InputEndpointConfig = ConnectorConfig & {
  /**
   * The name of the input stream of the circuit that this endpoint is
   * connected to.
   */
  stream: string
}

/**
 * The specified units for SQL Interval types.
 *
 * `INTERVAL 1 DAY`, `INTERVAL 1 DAY TO HOUR`, `INTERVAL 1 DAY TO MINUTE`,
 * would yield `Day`, `DayToHour`, `DayToMinute`, as the `IntervalUnit` respectively.
 */
export type IntervalUnit =
  | 'DAY'
  | 'DAYTOHOUR'
  | 'DAYTOMINUTE'
  | 'DAYTOSECOND'
  | 'HOUR'
  | 'HOURTOMINUTE'
  | 'HOURTOSECOND'
  | 'MINUTE'
  | 'MINUTETOSECOND'
  | 'MONTH'
  | 'SECOND'
  | 'YEAR'
  | 'YEARTOMONTH'

/**
 * Supported JSON data change event formats.
 *
 * Each element in a JSON-formatted input stream specifies
 * an update to one or more records in an input table.  We support
 * several different ways to represent such updates.
 */
export type JsonUpdateFormat = 'insert_delete' | 'weighted' | 'debezium' | 'snowflake' | 'raw'

/**
 * Kafka message header.
 */
export type KafkaHeader = {
  key: string
  value?: KafkaHeaderValue | null
}

/**
 * Kafka header value encoded as a UTF-8 string or a byte array.
 */
export type KafkaHeaderValue = Blob | File

/**
 * Configuration for reading data from Kafka topics with `InputTransport`.
 */
export type KafkaInputConfig = {
  fault_tolerance?: KafkaInputFtConfig | null
  /**
   * Maximum timeout in seconds to wait for the endpoint to join the Kafka
   * consumer group during initialization.
   */
  group_join_timeout_secs?: number
  /**
   * If specified, this service is used to provide defaults for the Kafka options.
   */
  kafka_service?: string | null
  log_level?: KafkaLogLevel | null
  /**
   * Set to 1 or more to fix the number of threads used to poll
   * `rdkafka`. Multiple threads can increase performance with small Kafka
   * messages; for large messages, one thread is enough. In either case, too
   * many threads can harm performance. If unset, the default is 3, which
   * helps with small messages but will not harm performance with large
   * messagee
   */
  poller_threads?: number | null
  /**
   * List of topics to subscribe to.
   */
  topics: Array<string>
  /**
   * Options passed directly to `rdkafka`.
   *
   * [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
   * used to configure the Kafka consumer.  Not all options are valid with
   * this Kafka adapter:
   *
   * * "enable.auto.commit", if present, must be set to "false",
   * * "enable.auto.offset.store", if present, must be set to "false"
   */
  '[key: string]': (string | unknown | number) | undefined
}

/**
 * Fault tolerance configuration for Kafka input connector.
 */
export type KafkaInputFtConfig = {
  /**
   * Options passed to `rdkafka` for consumers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for consumers, and may be empty.
   */
  consumer_options?: {
    [key: string]: string
  }
  /**
   * If this is true or unset, then the connector will create missing index
   * topics as needed.  If this is false, then a missing index topic is a
   * fatal error.
   */
  create_missing_index?: boolean | null
  /**
   * Suffix to append to each data topic name, to give the name of a topic
   * that the connector uses for recording the division of the corresponding
   * data topic into steps.  Defaults to `_input-index`.
   *
   * An index topic must have the same number of partitions as its
   * corresponding data topic.
   *
   * If two or more fault-tolerant Kafka endpoints read from overlapping sets
   * of topics, they must specify different `index_suffix` values.
   */
  index_suffix?: string | null
  /**
   * Maximum number of bytes in a step.  Any individual message bigger than
   * this will be given a step of its own.
   */
  max_step_bytes?: number | null
  /**
   * Maximum number of messages in a step.
   */
  max_step_messages?: number | null
  /**
   * Options passed to `rdkafka` for producers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for producers, and may be empty.
   */
  producer_options?: {
    [key: string]: string
  }
}

/**
 * Kafka logging levels.
 */
export type KafkaLogLevel =
  | 'emerg'
  | 'alert'
  | 'critical'
  | 'error'
  | 'warning'
  | 'notice'
  | 'info'
  | 'debug'

/**
 * Configuration for writing data to a Kafka topic with `OutputTransport`.
 */
export type KafkaOutputConfig = {
  fault_tolerance?: KafkaOutputFtConfig | null
  /**
   * Kafka headers to be added to each message produced by this connector.
   */
  headers?: Array<KafkaHeader>
  /**
   * Maximum timeout in seconds to wait for the endpoint to connect to
   * a Kafka broker.
   *
   * Defaults to 60.
   */
  initialization_timeout_secs?: number
  /**
   * If specified, this service is used to provide defaults for the Kafka options.
   */
  kafka_service?: string | null
  log_level?: KafkaLogLevel | null
  /**
   * Maximum number of unacknowledged messages buffered by the Kafka
   * producer.
   *
   * Kafka producer buffers outgoing messages until it receives an
   * acknowledgement from the broker.  This configuration parameter
   * bounds the number of unacknowledged messages.  When the number of
   * unacknowledged messages reaches this limit, sending of a new message
   * blocks until additional acknowledgements arrive from the broker.
   *
   * Defaults to 1000.
   */
  max_inflight_messages?: number
  /**
   * Topic to write to.
   */
  topic: string
  /**
   * Options passed directly to `rdkafka`.
   *
   * See [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
   * used to configure the Kafka producer.
   */
  '[key: string]': (string | unknown | KafkaHeader | number) | undefined
}

/**
 * Fault tolerance configuration for Kafka output connector.
 */
export type KafkaOutputFtConfig = {
  /**
   * Options passed to `rdkafka` for consumers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for consumers, and may be empty.
   */
  consumer_options?: {
    [key: string]: string
  }
  /**
   * Options passed to `rdkafka` for producers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for producers, and may be empty.
   */
  producer_options?: {
    [key: string]: string
  }
}

/**
 * Query parameters for GET the list of pipelines.
 */
export type ListPipelinesQueryParameters = {
  /**
   * Whether to include program code in the response (default: `true`).
   * Passing `false` reduces the response size, which is particularly handy
   * when frequently monitoring the endpoint over low bandwidth connections.
   */
  code?: boolean
}

/**
 * A request to output a specific neighborhood of a table or view.
 * The neighborhood is defined in terms of its central point (`anchor`)
 * and the number of rows preceding and following the anchor to output.
 */
export type NeighborhoodQuery = {
  after: number
  anchor?: {
    [key: string]: unknown
  } | null
  before: number
}

/**
 * Request to create a new API key.
 */
export type NewApiKeyRequest = {
  /**
   * Key name.
   */
  name: string
}

/**
 * Response to a successful API key creation.
 */
export type NewApiKeyResponse = {
  /**
   * Generated API key. There is no way to
   * retrieve this key again from the
   * pipeline-manager, so store it securely.
   */
  api_key: string
  api_key_id: ApiKeyId
  /**
   * API key name
   */
  name: string
}

/**
 * Configuration for generating Nexmark input data.
 *
 * This connector must be used exactly three times in a pipeline if it is used
 * at all, once for each [`NexmarkTable`].
 */
export type NexmarkInputConfig = {
  options?: NexmarkInputOptions | null
  table: NexmarkTable
}

/**
 * Configuration for generating Nexmark input data.
 */
export type NexmarkInputOptions = {
  /**
   * Number of events to generate and submit together.
   */
  batch_size?: number
  /**
   * Number of events to generate.
   */
  events?: number
  /**
   * Whether to synchronize event generator threads after submitting each
   * batch.
   *
   * If true (which is the default), then the event generator threads will
   * submit data in lockstep, clustering their event sequence numbers. If
   * false, scheduling can cause some threads to get ahead of others.
   */
  synchronize_threads?: boolean
  /**
   * Number of event generator threads.
   *
   * It's reasonable to choose the same number of generator threads as worker
   * threads.
   */
  threads?: number
}

/**
 * Table in Nexmark.
 */
export type NexmarkTable = 'bid' | 'auction' | 'person'

export type OutputBufferConfig = {
  /**
   * Enable output buffering.
   *
   * The output buffering mechanism allows decoupling the rate at which the pipeline
   * pushes changes to the output transport from the rate of input changes.
   *
   * By default, output updates produced by the pipeline are pushed directly to
   * the output transport. Some destinations may prefer to receive updates in fewer
   * bigger batches. For instance, when writing Parquet files, producing
   * one bigger file every few minutes is usually better than creating
   * small files every few milliseconds.
   *
   * To achieve such input/output decoupling, users can enable output buffering by
   * setting the `enable_output_buffer` flag to `true`.  When buffering is enabled, output
   * updates produced by the pipeline are consolidated in an internal buffer and are
   * pushed to the output transport when one of several conditions is satisfied:
   *
   * * data has been accumulated in the buffer for more than `max_output_buffer_time_millis`
   * milliseconds.
   * * buffer size exceeds `max_output_buffer_size_records` records.
   *
   * This flag is `false` by default.
   */
  enable_output_buffer?: boolean
  /**
   * Maximum number of updates to be kept in the output buffer.
   *
   * This parameter bounds the maximal size of the buffer.
   * Note that the size of the buffer is not always equal to the
   * total number of updates output by the pipeline. Updates to the
   * same record can overwrite or cancel previous updates.
   *
   * By default, the buffer can grow indefinitely until one of
   * the other output conditions is satisfied.
   *
   * NOTE: this configuration option requires the `enable_output_buffer` flag
   * to be set.
   */
  max_output_buffer_size_records?: number
  /**
   * Maximum time in milliseconds data is kept in the output buffer.
   *
   * By default, data is kept in the buffer indefinitely until one of
   * the other output conditions is satisfied.  When this option is
   * set the buffer will be flushed at most every
   * `max_output_buffer_time_millis` milliseconds.
   *
   * NOTE: this configuration option requires the `enable_output_buffer` flag
   * to be set.
   */
  max_output_buffer_time_millis?: number
}

/**
 * Describes an output connector configuration
 */
export type OutputEndpointConfig = ConnectorConfig & {
  /**
   * The name of the output stream of the circuit that this endpoint is
   * connected to.
   */
  stream: string
}

/**
 * A query over an output stream.
 *
 * We currently do not support ad hoc queries.  Instead the client can use
 * three pre-defined queries to inspect the contents of a table or view.
 */
export type OutputQuery = 'table' | 'neighborhood' | 'quantiles'

/**
 * Patch (partially) update the pipeline.
 *
 * Note that the patching only applies to the main fields, not subfields.
 * For instance, it is not possible to update only the number of workers;
 * it is required to again pass the whole runtime configuration with the
 * change.
 */
export type PatchPipeline = {
  description?: string | null
  name?: string | null
  program_code?: string | null
  program_config?: ProgramConfig | null
  runtime_config?: RuntimeConfig | null
}

/**
 * Pipeline deployment configuration.
 * It represents configuration entries directly provided by the user
 * (e.g., runtime configuration) and entries derived from the schema
 * of the compiled program (e.g., connectors). Storage configuration,
 * if applicable, is set by the runner.
 */
export type PipelineConfig = {
  /**
   * Enable CPU profiler.
   *
   * The default value is `true`.
   */
  cpu_profiler?: boolean
  /**
   * Maximal delay in microseconds to wait for `min_batch_size_records` to
   * get buffered by the controller, defaults to 0.
   */
  max_buffering_delay_usecs?: number
  /**
   * Minimal input batch size.
   *
   * The controller delays pushing input records to the circuit until at
   * least `min_batch_size_records` records have been received (total
   * across all endpoints) or `max_buffering_delay_usecs` microseconds
   * have passed since at least one input records has been buffered.
   * Defaults to 0.
   */
  min_batch_size_records?: number
  /**
   * The minimum estimated number of bytes in a batch of data to write it to
   * storage.  This is provided for debugging and fine-tuning and should
   * ordinarily be left unset. It only has an effect when `storage` is set to
   * true.
   *
   * A value of 0 will write even empty batches to storage, and nonzero
   * values provide a threshold.  `usize::MAX` would effectively disable
   * storage.
   */
  min_storage_bytes?: number | null
  resources?: ResourceConfig
  /**
   * Should persistent storage be enabled for this pipeline?
   *
   * - If `false` (default), the pipeline's state is kept in in-memory data-structures.
   * This is useful if the pipeline is ephemeral and does not need to be recovered
   * after a restart. The pipeline will most likely run faster since it does not
   * need to read from, or write to disk
   *
   * - If `true`, the pipeline state is stored in the specified location,
   * is persisted across restarts, and can be checkpointed and recovered.
   * This feature is currently experimental.
   */
  storage?: boolean
  /**
   * Enable pipeline tracing.
   */
  tracing?: boolean
  /**
   * Jaeger tracing endpoint to send tracing information to.
   */
  tracing_endpoint_jaeger?: string
  /**
   * Number of DBSP worker threads.
   */
  workers?: number
} & {
  /**
   * Input endpoint configuration.
   */
  inputs: {
    [key: string]: InputEndpointConfig
  }
  /**
   * Pipeline name.
   */
  name?: string | null
  /**
   * Output endpoint configuration.
   */
  outputs?: {
    [key: string]: OutputEndpointConfig
  }
  storage_config?: StorageConfig | null
}

/**
 * Pipeline descriptor.
 */
export type PipelineDescr = {
  /**
   * Pipeline description.
   */
  description: string
  /**
   * Pipeline name.
   */
  name: string
  /**
   * Program SQL code.
   */
  program_code: string
  program_config: ProgramConfig
  runtime_config: RuntimeConfig
}

/**
 * Pipeline identifier.
 */
export type PipelineId = string

/**
 * Pipeline status.
 *
 * This type represents the state of the pipeline tracked by the pipeline
 * runner and observed by the API client via the `GET /pipeline` endpoint.
 *
 * ### The lifecycle of a pipeline
 *
 * The following automaton captures the lifecycle of the pipeline.  Individual
 * states and transitions of the automaton are described below.
 *
 * * In addition to the transitions shown in the diagram, all states have an
 * implicit "forced shutdown" transition to the `Shutdown` state.  This
 * transition is triggered when the pipeline runner is unable to communicate
 * with the pipeline and thereby forces a shutdown.
 *
 * * States labeled with the hourglass symbol (⌛) are **timed** states.  The
 * automaton stays in timed state until the corresponding operation completes
 * or until the runner performs a forced shutdown of the pipeline after a
 * pre-defined timeout period.
 *
 * * State transitions labeled with API endpoint names (`/deploy`, `/start`,
 * `/pause`, `/shutdown`) are triggered by invoking corresponding endpoint,
 * e.g., `POST /v0/pipelines/{pipeline_id}/start`.
 *
 * ```text
 * Shutdown◄────┐
 * │         │
 * /deploy│         │
 * │   ⌛ShuttingDown
 * ▼         ▲
 * ⌛Provisioning    │
 * │         │
 * Provisioned        │         │
 * ▼         │/shutdown
 * ⌛Initializing    │
 * │         │
 * ┌────────┴─────────┴─┐
 * │        ▼           │
 * │      Paused        │
 * │      │    ▲        │
 * │/start│    │/pause  │
 * │      ▼    │        │
 * │     Running        │
 * └──────────┬─────────┘
 * │
 * ▼
 * Failed
 * ```
 *
 * ### Desired and actual status
 *
 * We use the desired state model to manage the lifecycle of a pipeline.
 * In this model, the pipeline has two status attributes associated with
 * it at runtime: the **desired** status, which represents what the user
 * would like the pipeline to do, and the **current** status, which
 * represents the actual state of the pipeline.  The pipeline runner
 * service continuously monitors both fields and steers the pipeline
 * towards the desired state specified by the user.
 * Only three of the states in the pipeline automaton above can be
 * used as desired statuses: `Paused`, `Running`, and `Shutdown`.
 * These statuses are selected by invoking REST endpoints shown
 * in the diagram.
 *
 * The user can monitor the current state of the pipeline via the
 * `/status` endpoint, which returns an object of type `Pipeline`.
 * In a typical scenario, the user first sets
 * the desired state, e.g., by invoking the `/deploy` endpoint, and
 * then polls the `GET /pipeline` endpoint to monitor the actual status
 * of the pipeline until its `state.current_status` attribute changes
 * to "paused" indicating that the pipeline has been successfully
 * initialized, or "failed", indicating an error.
 */
export type PipelineStatus =
  | 'Shutdown'
  | 'Provisioning'
  | 'Initializing'
  | 'Paused'
  | 'Running'
  | 'ShuttingDown'
  | 'Failed'

/**
 * Program configuration.
 */
export type ProgramConfig = {
  profile?: CompilationProfile | null
}

/**
 * Program information which includes schema, input connectors and output connectors.
 */
export type ProgramInfo = {
  /**
   * Input connectors derived from the schema.
   */
  input_connectors: {
    [key: string]: InputEndpointConfig
  }
  /**
   * Output connectors derived from the schema.
   */
  output_connectors: {
    [key: string]: OutputEndpointConfig
  }
  schema: ProgramSchema
}

/**
 * A struct containing the tables (inputs) and views for a program.
 *
 * Parse from the JSON data-type of the DDL generated by the SQL compiler.
 */
export type ProgramSchema = {
  inputs: Array<Relation>
  outputs: Array<Relation>
}

/**
 * Program compilation status.
 */
export type ProgramStatus =
  | 'Pending'
  | 'CompilingSql'
  | 'CompilingRust'
  | 'Success'
  | {
      /**
       * SQL compiler returned an error.
       */
      SqlError: Array<SqlCompilerMessage>
    }
  | {
      /**
       * Rust compiler returned an error.
       */
      RustError: string
    }
  | {
      /**
       * System/OS returned an error when trying to invoke commands.
       */
      SystemError: string
    }

export type PropertyValue = {
  key_position: SourcePosition
  value: string
  value_position: SourcePosition
}

export type ProviderAwsCognito = {
  jwk_uri: string
  login_url: string
  logout_url: string
}

export type ProviderGoogleIdentity = {
  client_id: string
  jwk_uri: string
}

/**
 * Strategy that determines which objects to read from a given bucket
 */
export type ReadStrategy =
  | {
      key: string
      type: 'SingleKey'
    }
  | {
      prefix: string
      type: 'Prefix'
    }

export type type3 = 'SingleKey'

/**
 * A SQL table or view. It has a name and a list of fields.
 *
 * Matches the Calcite JSON format.
 */
export type Relation = {
  case_sensitive?: boolean
  fields: Array<Field>
  materialized?: boolean
  name: string
  properties?: {
    [key: string]: PropertyValue
  }
}

export type ResourceConfig = {
  /**
   * The maximum number of CPU cores to reserve
   * for an instance of this pipeline
   */
  cpu_cores_max?: number | null
  /**
   * The minimum number of CPU cores to reserve
   * for an instance of this pipeline
   */
  cpu_cores_min?: number | null
  /**
   * The maximum memory in Megabytes to reserve
   * for an instance of this pipeline
   */
  memory_mb_max?: number | null
  /**
   * The minimum memory in Megabytes to reserve
   * for an instance of this pipeline
   */
  memory_mb_min?: number | null
  /**
   * Storage class to use for an instance of this pipeline.
   * The class determines storage performance such as IOPS and throughput.
   */
  storage_class?: string | null
  /**
   * The total storage in Megabytes to reserve
   * for an instance of this pipeline
   */
  storage_mb_max?: number | null
}

/**
 * Configuration for generating random data for a field of a table.
 */
export type RngFieldSettings = {
  /**
   * The frequency rank exponent for the Zipf distribution.
   *
   * - This value is only used if the strategy is set to `Zipf`.
   * - The default value is 1.0.
   */
  e?: number
  /**
   * Specifies the values that the generator should produce in case the field is a struct type.
   */
  fields?: {
    [key: string]: RngFieldSettings
  } | null
  key?: RngFieldSettings | null
  /**
   * Percentage of records where this field should be set to NULL.
   *
   * If not set, the generator will produce only records with non-NULL values.
   * If set to `1..=100`, the generator will produce records with NULL values with the specified percentage.
   */
  null_percentage?: number | null
  /**
   * An optional, exclusive range [a, b) to limit the range of values the generator should produce.
   *
   * - For integer/floating point types specifies min/max values.
   * If not set, the generator will produce values for the entire range of the type for number types.
   * - For string/binary types specifies min/max length, values are required to be >=0.
   * If not set, a range of [0, 25) is used by default.
   * - For timestamp types specifies the min/max in milliseconds from the number of non-leap
   * milliseconds since January 1, 1970 0:00:00.000 UTC (aka “UNIX timestamp”).
   * If not set, a range of [0, 4102444800) is used by default (1970-01-01 -- 2100-01-01).
   * - For time types specifies the min/max in milliseconds.
   * If not set, the range is 24h. Range values are required to be >=0.
   * - For date types specifies the min/max in days from the number of days since January 1, 1970.
   * If not set, a range of [0, 54787) is used by default (1970-01-01 -- 2100-01-01).
   * - For array types specifies the min/max number of elements.
   * If not set, a range of [0, 5) is used by default. Range values are required to be >=0.
   * - For map types specifies the min/max number of key-value pairs.
   * If not set, a range of [0, 5) is used by default.
   * - For struct/boolean/null types `range` is ignored.
   */
  range?: Array<number> | null
  /**
   * A scale factor to apply a multiplier to the generated value.
   *
   * - For integer/floating point types, the value is multiplied by the scale factor.
   * - For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.
   * - For time types, the generated value (milliseconds) is multiplied by the scale factor.
   * - For date types, the generated value (days) is multiplied by the scale factor.
   * - For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.
   *
   * - If `values` is specified, the scale factor is ignored.
   * - If `range` is specified and the range is required to be positive (struct, map, array etc.)
   * the scale factor is required to be positive too.
   *
   * The default scale factor is 1.
   */
  scale?: number
  strategy?: DatagenStrategy
  value?: RngFieldSettings | null
  /**
   * An optional set of values the generator will pick from.
   *
   * If set, the generator will pick values from the specified set.
   * If not set, the generator will produce values according to the specified range.
   * If set to an empty set, the generator will produce NULL values.
   * If set to a single value, the generator will produce only that value.
   *
   * Note that `range` is ignored if `values` is set.
   */
  values?: Array<{
    [key: string]: unknown
  }> | null
}

/**
 * Global pipeline configuration settings. This is the publicly
 * exposed type for users to configure pipelines.
 */
export type RuntimeConfig = {
  /**
   * Enable CPU profiler.
   *
   * The default value is `true`.
   */
  cpu_profiler?: boolean
  /**
   * Maximal delay in microseconds to wait for `min_batch_size_records` to
   * get buffered by the controller, defaults to 0.
   */
  max_buffering_delay_usecs?: number
  /**
   * Minimal input batch size.
   *
   * The controller delays pushing input records to the circuit until at
   * least `min_batch_size_records` records have been received (total
   * across all endpoints) or `max_buffering_delay_usecs` microseconds
   * have passed since at least one input records has been buffered.
   * Defaults to 0.
   */
  min_batch_size_records?: number
  /**
   * The minimum estimated number of bytes in a batch of data to write it to
   * storage.  This is provided for debugging and fine-tuning and should
   * ordinarily be left unset. It only has an effect when `storage` is set to
   * true.
   *
   * A value of 0 will write even empty batches to storage, and nonzero
   * values provide a threshold.  `usize::MAX` would effectively disable
   * storage.
   */
  min_storage_bytes?: number | null
  resources?: ResourceConfig
  /**
   * Should persistent storage be enabled for this pipeline?
   *
   * - If `false` (default), the pipeline's state is kept in in-memory data-structures.
   * This is useful if the pipeline is ephemeral and does not need to be recovered
   * after a restart. The pipeline will most likely run faster since it does not
   * need to read from, or write to disk
   *
   * - If `true`, the pipeline state is stored in the specified location,
   * is persisted across restarts, and can be checkpointed and recovered.
   * This feature is currently experimental.
   */
  storage?: boolean
  /**
   * Enable pipeline tracing.
   */
  tracing?: boolean
  /**
   * Jaeger tracing endpoint to send tracing information to.
   */
  tracing_endpoint_jaeger?: string
  /**
   * Number of DBSP worker threads.
   */
  workers?: number
}

/**
 * Configuration for reading data from AWS S3.
 */
export type S3InputConfig = {
  /**
   * S3 bucket name to access
   */
  bucket_name: string
  consume_strategy?: ConsumeStrategy
  credentials: AwsCredentials
  read_strategy: ReadStrategy
  /**
   * AWS region
   */
  region: string
}

export type SourcePosition = {
  end_column: number
  end_line_number: number
  start_column: number
  start_line_number: number
}

/**
 * A SQL compiler error.
 *
 * The SQL compiler returns a list of errors in the following JSON format if
 * it's invoked with the `-je` option.
 *
 * ```ignore
 * [ {
 * "start_line_number" : 2,
 * "start_column" : 4,
 * "end_line_number" : 2,
 * "end_column" : 8,
 * "warning" : false,
 * "error_type" : "PRIMARY KEY cannot be nullable",
 * "message" : "PRIMARY KEY column 'C' has type INTEGER, which is nullable",
 * "snippet" : "    2|   c INT PRIMARY KEY\n         ^^^^^\n    3|);\n"
 * } ]
 * ```
 */
export type SqlCompilerMessage = {
  end_column: number
  end_line_number: number
  error_type: string
  message: string
  snippet?: string | null
  start_column: number
  start_line_number: number
  warning: boolean
}

/**
 * The available SQL types as specified in `CREATE` statements.
 */
export type SqlType =
  | 'BOOLEAN'
  | 'TINYINT'
  | 'SMALLINT'
  | 'INTEGER'
  | 'BIGINT'
  | 'REAL'
  | 'DOUBLE'
  | 'DECIMAL'
  | 'CHAR'
  | 'VARCHAR'
  | 'BINARY'
  | 'VARBINARY'
  | 'TIME'
  | 'DATE'
  | 'TIMESTAMP'
  | {
      Interval: IntervalUnit
    }
  | 'ARRAY'
  | 'STRUCT'
  | 'MAP'
  | 'NULL'

/**
 * How to cache access to storage within a Feldera pipeline.
 */
export type StorageCacheConfig = 'page_cache' | 'feldera_cache'

/**
 * Configuration for persistent storage in a [`PipelineConfig`].
 */
export type StorageConfig = {
  cache: StorageCacheConfig
  /**
   * The location where the pipeline state is stored or will be stored.
   *
   * It should point to a path on the file-system of the machine/container
   * where the pipeline will run. If that path doesn't exist yet, or if it
   * does not contain any checkpoints, then the pipeline creates it and
   * starts from an initial state in which no data has yet been received. If
   * it does exist, then the pipeline starts from the most recent checkpoint
   * that already exists there. In either case, (further) checkpoints will be
   * written there.
   */
  path: string
}

/**
 * Transport-specific endpoint configuration passed to
 * `crate::OutputTransport::new_endpoint`
 * and `crate::InputTransport::new_endpoint`.
 */
export type TransportConfig =
  | {
      config: FileInputConfig
      name: 'file_input'
    }
  | {
      config: FileOutputConfig
      name: 'file_output'
    }
  | {
      config: KafkaInputConfig
      name: 'kafka_input'
    }
  | {
      config: KafkaOutputConfig
      name: 'kafka_output'
    }
  | {
      config: UrlInputConfig
      name: 'url_input'
    }
  | {
      config: S3InputConfig
      name: 's3_input'
    }
  | {
      config: DeltaTableReaderConfig
      name: 'delta_table_input'
    }
  | {
      config: DeltaTableWriterConfig
      name: 'delta_table_output'
    }
  | {
      config: DatagenInputConfig
      name: 'datagen'
    }
  | {
      config: NexmarkInputConfig
      name: 'nexmark'
    }
  | {
      name: 'http_input'
    }
  | {
      name: 'http_output'
    }

export type name = 'file_input'

/**
 * Configuration for reading data from an HTTP or HTTPS URL with
 * `UrlInputTransport`.
 */
export type UrlInputConfig = {
  /**
   * URL.
   */
  path: string
  /**
   * Timeout before disconnection when paused.
   *
   * If the pipeline is paused, or if the input adapter reads data faster
   * than the pipeline can process it, then the controller will pause the
   * input adapter. If the input adapter stays paused longer than this
   * timeout, it will drop the network connection to the server. It will
   * automatically reconnect when the input adapter starts running again.
   */
  pause_timeout?: number
}

/**
 * Version number.
 */
export type Version = number

export type GetConfigAuthenticationResponse = AuthProvider

export type GetConfigAuthenticationError = ErrorResponse

export type ListApiKeysData = {
  query?: {
    /**
     * API key name
     */
    name?: string | null
  }
}

export type ListApiKeysResponse = Array<ApiKeyDescr>

export type ListApiKeysError = ErrorResponse

export type CreateApiKeyData = {
  body: NewApiKeyRequest
}

export type CreateApiKeyResponse = NewApiKeyResponse

export type CreateApiKeyError = ErrorResponse

export type GetApiKeyData = {
  path: {
    /**
     * Unique API key name
     */
    api_key_name: string
  }
}

export type GetApiKeyResponse = ApiKeyDescr

export type GetApiKeyError = ErrorResponse

export type DeleteApiKeyData = {
  path: {
    /**
     * Unique API key name
     */
    api_key_name: string
  }
}

export type DeleteApiKeyResponse = unknown

export type DeleteApiKeyError = ErrorResponse

export type GetConfigDemosResponse = Array<Demo>

export type GetConfigDemosError = ErrorResponse

export type ListPipelinesData = {
  query?: {
    /**
     * Whether to include program code in the response (default: `true`).
     * Passing `false` reduces the response size, which is particularly handy
     * when frequently monitoring the endpoint over low bandwidth connections.
     */
    code?: boolean
  }
}

export type ListPipelinesResponse = Array<ExtendedPipelineDescrOptionalCode>

export type ListPipelinesError = unknown

export type PostPipelineData = {
  body: PipelineDescr
}

export type PostPipelineResponse = ExtendedPipelineDescr

export type PostPipelineError = ErrorResponse

export type GetPipelineData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineResponse = ExtendedPipelineDescr

export type GetPipelineError = ErrorResponse

export type PutPipelineData = {
  body: PipelineDescr
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PutPipelineResponse = ExtendedPipelineDescr

export type PutPipelineError = ErrorResponse

export type DeletePipelineData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type DeletePipelineResponse = unknown

export type DeletePipelineError = ErrorResponse

export type PatchPipelineData = {
  body: PatchPipeline
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PatchPipelineResponse = ExtendedPipelineDescr

export type PatchPipelineError = ErrorResponse

export type GetPipelineCircuitProfileData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineCircuitProfileResponse = {
  [key: string]: unknown
}

export type GetPipelineCircuitProfileError = ErrorResponse

export type HttpOutputData = {
  /**
   * When the `query` parameter is set to 'neighborhood', the body of the request must contain a neighborhood specification.
   */
  body?: NeighborhoodQuery | null
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * SQL table name. Unquoted SQL names have to be capitalized. Quoted SQL names have to exactly match the case from the SQL program.
     */
    table_name: string
  }
  query: {
    /**
     * Set to `true` to group updates in this stream into JSON arrays (used in conjunction with `format=json`). The default value is `false`
     */
    array?: boolean | null
    /**
     * Apply backpressure on the pipeline when the HTTP client cannot receive data fast enough.
     * When this flag is set to false (the default), the HTTP connector drops data chunks if the client is not keeping up with its output.  This prevents a slow HTTP client from slowing down the entire pipeline.
     * When the flag is set to true, the connector waits for the client to receive each chunk and blocks the pipeline if the client cannot keep up.
     */
    backpressure?: boolean | null
    /**
     * Output data format, e.g., 'csv' or 'json'.
     */
    format: string
    /**
     * Output mode. Must be one of 'watch' or 'snapshot'. The default value is 'watch'
     */
    mode?: EgressMode | null
    /**
     * For 'quantiles' queries: the number of quantiles to output. The default value is 100.
     */
    quantiles?: number | null
    /**
     * Query to execute on the table. Must be one of 'table', 'neighborhood', or 'quantiles'. The default value is 'table'
     */
    query?: OutputQuery | null
  }
}

export type HttpOutputResponse = Chunk

export type HttpOutputError = ErrorResponse

export type GetPipelineHeapProfileData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineHeapProfileResponse = Blob | File

export type GetPipelineHeapProfileError = ErrorResponse

export type HttpInputData = {
  /**
   * Contains the new input data in CSV.
   */
  body: string
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * SQL table name. Unquoted SQL names have to be capitalized. Quoted SQL names have to exactly match the case from the SQL program.
     */
    table_name: string
  }
  query: {
    /**
     * Set to `true` if updates in this stream are packaged into JSON arrays (used in conjunction with `format=json`). The default values is `false`.
     */
    array?: boolean | null
    /**
     * When `true`, push data to the pipeline even if the pipeline is paused. The default value is `false`
     */
    force: boolean
    /**
     * Input data format, e.g., 'csv' or 'json'.
     */
    format: string
    /**
     * JSON data change event format (used in conjunction with `format=json`).  The default value is 'insert_delete'.
     */
    update_format?: JsonUpdateFormat | null
  }
}

export type HttpInputResponse = unknown

export type HttpInputError = ErrorResponse

export type InputEndpointActionData = {
  path: {
    /**
     * Endpoint action [start, pause]
     */
    action: string
    /**
     * Input endpoint name
     */
    endpoint_name: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type InputEndpointActionResponse = unknown

export type InputEndpointActionError = ErrorResponse

export type GetPipelineStatsData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineStatsResponse = {
  [key: string]: unknown
}

export type GetPipelineStatsError = ErrorResponse

export type PostPipelineActionData = {
  path: {
    /**
     * Pipeline action (one of: start, pause, shutdown)
     */
    action: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PostPipelineActionResponse = unknown

export type PostPipelineActionError = ErrorResponse

export type $OpenApiTs = {
  '/config/authentication': {
    get: {
      res: {
        /**
         * The response body contains Authentication Provider configuration, or is empty if no auth is configured.
         */
        '200': AuthProvider
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/api_keys': {
    get: {
      req: ListApiKeysData
      res: {
        /**
         * API keys retrieved successfully
         */
        '200': Array<ApiKeyDescr>
        /**
         * Specified API key name does not exist.
         */
        '404': ErrorResponse
      }
    }
    post: {
      req: CreateApiKeyData
      res: {
        /**
         * API key created successfully.
         */
        '200': NewApiKeyResponse
        /**
         * An API key with this name already exists.
         */
        '409': ErrorResponse
      }
    }
  }
  '/v0/api_keys/{api_key_name}': {
    get: {
      req: GetApiKeyData
      res: {
        /**
         * API key retrieved successfully
         */
        '200': ApiKeyDescr
        /**
         * Specified API key name does not exist.
         */
        '404': ErrorResponse
      }
    }
    delete: {
      req: DeleteApiKeyData
      res: {
        /**
         * API key deleted successfully
         */
        '200': unknown
        /**
         * Specified API key name does not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/config/demos': {
    get: {
      res: {
        /**
         * List of demos.
         */
        '200': Array<Demo>
        /**
         * Failed to read demos from the demos directory.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines': {
    get: {
      req: ListPipelinesData
      res: {
        /**
         * List of pipelines retrieved successfully
         */
        '200': Array<ExtendedPipelineDescrOptionalCode>
      }
    }
    post: {
      req: PostPipelineData
      res: {
        /**
         * Pipeline successfully created
         */
        '201': ExtendedPipelineDescr
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}': {
    get: {
      req: GetPipelineData
      res: {
        /**
         * Pipeline retrieved successfully
         */
        '200': ExtendedPipelineDescr
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
      }
    }
    put: {
      req: PutPipelineData
      res: {
        /**
         * Pipeline successfully updated
         */
        '200': ExtendedPipelineDescr
        /**
         * Pipeline needs to be shutdown to be modified
         */
        '400': ErrorResponse
        /**
         * Cannot rename pipeline as the name already exists
         */
        '409': ErrorResponse
      }
    }
    delete: {
      req: DeletePipelineData
      res: {
        /**
         * Pipeline successfully deleted
         */
        '200': unknown
        /**
         * Pipeline needs to be shutdown to be deleted
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
      }
    }
    patch: {
      req: PatchPipelineData
      res: {
        /**
         * Pipeline successfully updated
         */
        '200': ExtendedPipelineDescr
        /**
         * Pipeline needs to be shutdown to be modified
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        /**
         * Cannot rename pipeline as the name already exists
         */
        '409': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/circuit_profile': {
    get: {
      req: GetPipelineCircuitProfileData
      res: {
        /**
         * Obtains a circuit performance profile.
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline is not running or paused
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/egress/{table_name}': {
    post: {
      req: HttpOutputData
      res: {
        /**
         * Connection to the endpoint successfully established. The body of the response contains a stream of data chunks.
         */
        '200': Chunk
        /**
         * Unknown data format specified in the '?format=' argument.
         */
        '400': ErrorResponse
        /**
         * Specified table or view does not exist.
         */
        '404': ErrorResponse
        /**
         * Pipeline is not currently running because it has been shutdown or not yet started.
         */
        '410': ErrorResponse
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/heap_profile': {
    get: {
      req: GetPipelineHeapProfileData
      res: {
        /**
         * Pipeline's heap usage profile as a gzipped protobuf that can be inspected by the pprof tool
         */
        '200': Blob | File
        /**
         * Pipeline is not running or paused
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/ingress/{table_name}': {
    post: {
      req: HttpInputData
      res: {
        /**
         * Data successfully delivered to the pipeline.
         */
        '200': unknown
        /**
         * Error parsing input data.
         */
        '400': ErrorResponse
        /**
         * Pipeline is not currently running because it has been shutdown or not yet started.
         */
        '404': ErrorResponse
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/input_endpoints/{endpoint_name}/{action}': {
    post: {
      req: InputEndpointActionData
      res: {
        /**
         * Request accepted.
         */
        '202': unknown
        /**
         * Specified endpoint does not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/stats': {
    get: {
      req: GetPipelineStatsData
      res: {
        /**
         * Pipeline metrics retrieved successfully.
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline is not running or paused
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/{action}': {
    post: {
      req: PostPipelineActionData
      res: {
        /**
         * Action accepted and is being performed
         */
        '202': unknown
        /**
         * Unable to accept action
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
      }
    }
  }
}
