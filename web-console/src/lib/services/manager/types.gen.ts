// This file is auto-generated by @hey-api/openapi-ts

/**
 * Configuration for inserting data with ad-hoc queries
 *
 * An ad-hoc input adapters cannot be usefully configured as part of pipeline
 * configuration.  Instead, use ad-hoc queries through the UI, the REST API, or
 * the `fda` command-line tool.
 */
export type AdHocInputConfig = {
  /**
   * Autogenerated name.
   */
  name: string
}

/**
 * URL-encoded `format` argument to the `/query` endpoint.
 */
export type AdHocResultFormat = 'text' | 'json' | 'parquet' | 'arrow_ipc' | 'hash'

/**
 * Arguments to the `/query` endpoint.
 *
 * The arguments can be provided in two ways:
 *
 * - In case a normal HTTP connection is established to the endpoint,
 * these arguments are passed as URL-encoded parameters.
 * Note: this mode is deprecated and will be removed in the future.
 *
 * - If a Websocket connection is opened to `/query`, the arguments are passed
 * to the server over the websocket as a JSON encoded string.
 */
export type AdhocQueryArgs = {
  format?: AdHocResultFormat
  /**
   * The SQL query to run.
   */
  sql?: string
}

/**
 * API key descriptor.
 */
export type ApiKeyDescr = {
  id: ApiKeyId
  name: string
  scopes: Array<ApiPermission>
}

/**
 * API key identifier.
 */
export type ApiKeyId = string

/**
 * Permission types for invoking API endpoints.
 */
export type ApiPermission = 'Read' | 'Write'

export type Auth = {
  credentials?: Credentials | null
  jwt?: string | null
  nkey?: string | null
  token?: string | null
  user_and_password?: UserAndPassword | null
}

export type AuthProvider =
  | {
      AwsCognito: ProviderAwsCognito
    }
  | {
      GenericOidc: ProviderGenericOidc
    }

export type BootstrapPolicy = 'allow' | 'reject' | 'await_approval'

/**
 * Information about the build of the platform.
 */
export type BuildInformation = {
  /**
   * CPU of build machine.
   */
  build_cpu: string
  /**
   * OS of build machine.
   */
  build_os: string
  /**
   * Timestamp of the build.
   */
  build_timestamp: string
  /**
   * Whether the build is optimized for performance.
   */
  cargo_debug: string
  /**
   * Dependencies used during the build.
   */
  cargo_dependencies: string
  /**
   * Features enabled during the build.
   */
  cargo_features: string
  /**
   * Optimization level of the build.
   */
  cargo_opt_level: string
  /**
   * Target triple of the build.
   */
  cargo_target_triple: string
  /**
   * Rust version of the build used.
   */
  rustc_version: string
}

export type CalciteId =
  | {
      partial: number
    }
  | {
      final: number
    }
  | {
      and: Array<CalciteId>
    }
  | {
      seq: Array<CalciteId>
    }
  | {
      [key: string]: unknown
    }
  | null

/**
 * The Calcite plan representation of a dataflow graph.
 */
export type CalcitePlan = {
  rels: Array<Rel>
}

/**
 * Information about a failed checkpoint.
 */
export type CheckpointFailure = {
  /**
   * Error message associated with the failure.
   */
  error: string
  /**
   * Sequence number of the failed checkpoint.
   */
  sequence_number: number
}

/**
 * Response to a checkpoint request.
 */
export type CheckpointResponse = {
  checkpoint_sequence_number: number
}

/**
 * Checkpoint status returned by the `/checkpoint_status` endpoint.
 */
export type CheckpointStatus = {
  failure?: CheckpointFailure | null
  /**
   * Most recently successful checkpoint.
   */
  success?: number | null
}

/**
 * A set of updates to a SQL table or view.
 *
 * The `sequence_number` field stores the offset of the chunk relative to the
 * start of the stream and can be used to implement reliable delivery.
 * The payload is stored in the `bin_data`, `text_data`, or `json_data` field
 * depending on the data format used.
 */
export type Chunk = {
  /**
   * Base64 encoded binary payload, e.g., bincode.
   */
  bin_data?: (Blob | File) | null
  /**
   * JSON payload.
   */
  json_data?: {
    [key: string]: unknown
  } | null
  sequence_number: number
  /**
   * Text payload, e.g., CSV.
   */
  text_data?: string | null
}

export type ClockConfig = {
  clock_resolution_usecs: number
}

/**
 * A SQL column type description.
 *
 * Matches the Calcite JSON format.
 */
export type ColumnType = {
  component?: ColumnType | null
  /**
   * The fields of the type (if available).
   *
   * For example this would specify the fields of a `CREATE TYPE` construct.
   *
   * ```sql
   * CREATE TYPE person_typ AS (
   * firstname       VARCHAR(30),
   * lastname        VARCHAR(30),
   * address         ADDRESS_TYP
   * );
   * ```
   *
   * Would lead to the following `fields` value:
   *
   * ```sql
   * [
   * ColumnType { name: "firstname, ... },
   * ColumnType { name: "lastname", ... },
   * ColumnType { name: "address", fields: [ ... ] }
   * ]
   * ```
   */
  fields?: Array<Field> | null
  key?: ColumnType | null
  /**
   * Does the type accept NULL values?
   */
  nullable: boolean
  /**
   * Precision of the type.
   *
   * # Examples
   * - `VARCHAR` sets precision to `-1`.
   * - `VARCHAR(255)` sets precision to `255`.
   * - `BIGINT`, `DATE`, `FLOAT`, `DOUBLE`, `GEOMETRY`, etc. sets precision
   * to None
   * - `TIME`, `TIMESTAMP` set precision to `0`.
   */
  precision?: number | null
  /**
   * The scale of the type.
   *
   * # Example
   * - `DECIMAL(1,2)` sets scale to `2`.
   */
  scale?: number | null
  type?: SqlType
  value?: ColumnType | null
}

export type CombinedDesiredStatus =
  | 'Stopped'
  | 'Unavailable'
  | 'Standby'
  | 'Paused'
  | 'Running'
  | 'Suspended'

export type CombinedStatus =
  | 'Stopped'
  | 'Provisioning'
  | 'Unavailable'
  | 'Standby'
  | 'AwaitingApproval'
  | 'Initializing'
  | 'Bootstrapping'
  | 'Replaying'
  | 'Paused'
  | 'Running'
  | 'Suspended'
  | 'Stopping'

/**
 * Enumeration of possible compilation profiles that can be passed to the Rust compiler
 * as an argument via `cargo build --profile <>`. A compilation profile affects among
 * other things the compilation speed (how long till the program is ready to be run)
 * and runtime speed (the performance while running).
 */
export type CompilationProfile = 'dev' | 'unoptimized' | 'optimized' | 'optimized_symbols'

/**
 * Completion token status returned by the `/completion_status` endpoint.
 */
export type CompletionStatus = 'complete' | 'inprogress'

/**
 * URL-encoded arguments to the `/completion_status` endpoint.
 */
export type CompletionStatusArgs = {
  /**
   * Completion token returned by the `/completion_token` or `/ingress`
   * endpoint.
   */
  token: string
}

/**
 * Response to a completion token status request.
 */
export type CompletionStatusResponse = {
  status: CompletionStatus
}

/**
 * Response to a completion token creation request.
 */
export type CompletionTokenResponse = {
  /**
   * Completion token.
   *
   * An opaque string associated with the current position in the input stream
   * generated by an input connector.
   * Pass this string to the `/completion_status` endpoint to check whether all
   * inputs associated with the token have been fully processed by the pipeline.
   */
  token: string
}

export type Condition = {
  literal?: boolean
  op?: Op | null
  operands?: Array<Operand> | null
  '[key: string]': (unknown | boolean | Operand) | undefined
}

export type Configuration = {
  build_info: BuildInformation
  /**
   * URL that navigates to the changelog of the current version
   */
  changelog_url: string
  /**
   * Feldera edition: "Open source" or "Enterprise"
   */
  edition: string
  license_validity?: LicenseValidity | null
  /**
   * Specific revision corresponding to the edition `version` (e.g., git commit hash).
   */
  revision: string
  /**
   * Specific revision corresponding to the default runtime version of the platform (e.g., git commit hash).
   */
  runtime_revision: string
  /**
   * Telemetry key.
   */
  telemetry: string
  /**
   * List of unstable features that are enabled.
   */
  unstable_features?: string | null
  update_info?: UpdateInformation | null
  /**
   * The version corresponding to the type of `edition`.
   * Format is `x.y.z`.
   */
  version: string
}

export type ConnectOptions = {
  auth?: Auth
  server_url: string
}

/**
 * A data connector's configuration
 */
export type ConnectorConfig = OutputBufferConfig & {
  format?: FormatConfig | null
  /**
   * Name of the index that the connector is attached to.
   *
   * This property is valid for output connectors only.  It is used with data
   * transports and formats that expect output updates in the form of key/value
   * pairs, where the key typically represents a unique id associated with the
   * table or view.
   *
   * To support such output formats, an output connector can be attached to an
   * index created using the SQL CREATE INDEX statement.  An index of a table
   * or view contains the same updates as the table or view itself, indexed by
   * one or more key columns.
   *
   * See individual connector documentation for details on how they work
   * with indexes.
   */
  index?: string | null
  /**
   * Arbitrary user-defined text labels associated with the connector.
   *
   * These labels can be used in conjunction with the `start_after` property
   * to control the start order of connectors.
   */
  labels?: Array<string>
  /**
   * Maximum batch size, in records.
   *
   * This is the maximum number of records to process in one batch through
   * the circuit.  The time and space cost of processing a batch is
   * asymptotically superlinear in the size of the batch, but very small
   * batches are less efficient due to constant factors.
   *
   * This should usually be less than `max_queued_records`, to give the
   * connector a round-trip time to restart and refill the buffer while
   * batches are being processed.
   *
   * Some input adapters might not honor this setting.
   *
   * The default is 10,000.
   */
  max_batch_size?: number
  /**
   * Backpressure threshold.
   *
   * Maximal number of records queued by the endpoint before the endpoint
   * is paused by the backpressure mechanism.
   *
   * For input endpoints, this setting bounds the number of records that have
   * been received from the input transport but haven't yet been consumed by
   * the circuit since the circuit, since the circuit is still busy processing
   * previous inputs.
   *
   * For output endpoints, this setting bounds the number of records that have
   * been produced by the circuit but not yet sent via the output transport endpoint
   * nor stored in the output buffer (see `enable_output_buffer`).
   *
   * Note that this is not a hard bound: there can be a small delay between
   * the backpressure mechanism is triggered and the endpoint is paused, during
   * which more data may be queued.
   *
   * The default is 1 million.
   */
  max_queued_records?: number
  /**
   * Create connector in paused state.
   *
   * The default is `false`.
   */
  paused?: boolean
  /**
   * Start the connector after all connectors with specified labels.
   *
   * This property is used to control the start order of connectors.
   * The connector will not start until all connectors with the specified
   * labels have finished processing all inputs.
   */
  start_after?: Array<string> | null
  transport: TransportConfig
}

/**
 * Aggregated connector error statistics.
 *
 * This structure contains the sum of all error counts across all input and output connectors
 * for a pipeline.
 */
export type ConnectorStats = {
  /**
   * Total number of errors across all connectors.
   *
   * This is the sum of:
   * - `num_transport_errors` from all input connectors
   * - `num_parse_errors` from all input connectors
   * - `num_encode_errors` from all output connectors
   * - `num_transport_errors` from all output connectors
   */
  num_errors: number
}

export type ConsumerConfig = {
  deliver_policy: DeliverPolicy
  description?: string | null
  filter_subjects?: Array<string>
  max_batch?: number | null
  max_bytes?: number | null
  max_expires?: string | null
  max_waiting?: number
  metadata?: {
    [key: string]: string
  }
  name?: string | null
  rate_limit?: number
  replay_policy?: ReplayPolicy
}

export type Credentials =
  | {
      FromString: string
    }
  | {
      FromFile: string
    }

/**
 * The JSON representation of a dataflow graph.
 */
export type Dataflow = {
  calcite_plan: {
    [key: string]: CalcitePlan
  }
  mir: {
    [key: string]: MirNode
  }
}

/**
 * Configuration for generating random data for a table.
 */
export type DatagenInputConfig = {
  /**
   * The sequence of generations to perform.
   *
   * If not set, the generator will produce a single sequence with default settings.
   * If set, the generator will produce the specified sequences in sequential order.
   *
   * Note that if one of the sequences before the last one generates an unlimited number of rows
   * the following sequences will not be executed.
   */
  plan?: Array<GenerationPlan>
  /**
   * Optional seed for the random generator.
   *
   * Setting this to a fixed value will make the generator produce the same sequence of records
   * every time the pipeline is run.
   *
   * # Notes
   * - To ensure the set of generated input records is deterministic across multiple runs,
   * apart from setting a seed, `workers` also needs to remain unchanged.
   * - The input will arrive in non-deterministic order if `workers > 1`.
   */
  seed?: number | null
  /**
   * Number of workers to use for generating data.
   */
  workers?: number
}

/**
 * Strategy used to generate values.
 */
export type DatagenStrategy =
  | 'increment'
  | 'uniform'
  | 'zipf'
  | 'word'
  | 'words'
  | 'sentence'
  | 'sentences'
  | 'paragraph'
  | 'paragraphs'
  | 'first_name'
  | 'last_name'
  | 'title'
  | 'suffix'
  | 'name'
  | 'name_with_title'
  | 'domain_suffix'
  | 'email'
  | 'username'
  | 'password'
  | 'field'
  | 'position'
  | 'seniority'
  | 'job_title'
  | 'ipv4'
  | 'ipv6'
  | 'ip'
  | 'mac_address'
  | 'user_agent'
  | 'rfc_status_code'
  | 'valid_status_code'
  | 'company_suffix'
  | 'company_name'
  | 'buzzword'
  | 'buzzword_middle'
  | 'buzzword_tail'
  | 'catch_phrase'
  | 'bs_verb'
  | 'bs_adj'
  | 'bs_noun'
  | 'bs'
  | 'profession'
  | 'industry'
  | 'currency_code'
  | 'currency_name'
  | 'currency_symbol'
  | 'credit_card_number'
  | 'city_prefix'
  | 'city_suffix'
  | 'city_name'
  | 'country_name'
  | 'country_code'
  | 'street_suffix'
  | 'street_name'
  | 'time_zone'
  | 'state_name'
  | 'state_abbr'
  | 'secondary_address_type'
  | 'secondary_address'
  | 'zip_code'
  | 'post_code'
  | 'building_number'
  | 'latitude'
  | 'longitude'
  | 'isbn'
  | 'isbn13'
  | 'isbn10'
  | 'phone_number'
  | 'cell_number'
  | 'file_path'
  | 'file_name'
  | 'file_extension'
  | 'dir_path'

export type DeliverPolicy =
  | 'All'
  | 'Last'
  | 'New'
  | {
      ByStartSequence: {
        start_sequence: number
      }
    }
  | {
      ByStartTime: {
        start_time: string
      }
    }
  | 'LastPerSubject'

/**
 * Delta table read mode.
 *
 * Three options are available:
 *
 * * `snapshot` - read a snapshot of the table and stop.
 *
 * * `follow` - continuously ingest changes to the table, starting from a specified version
 * or timestamp.
 *
 * * `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion
 * mode.
 */
export type DeltaTableIngestMode = 'snapshot' | 'follow' | 'snapshot_and_follow' | 'cdc'

/**
 * Delta table input connector configuration.
 */
export type DeltaTableReaderConfig = {
  /**
   * A predicate that determines whether the record represents a deletion.
   *
   * This setting is only valid in the `cdc` mode. It specifies a predicate applied to
   * each row in the Delta table to determine whether the row represents a deletion event.
   * Its value must be a valid Boolean SQL expression that can be used in a query of the
   * form `SELECT * from <table> WHERE <cdc_delete_filter>`.
   */
  cdc_delete_filter?: string | null
  /**
   * An expression that determines the ordering of updates in the Delta table.
   *
   * This setting is only valid in the `cdc` mode. It specifies a predicate applied to
   * each row in the Delta table to determine the order in which updates in the table should
   * be applied. Its value must be a valid SQL expression that can be used in a query of the
   * form `SELECT * from <table> ORDER BY <cdc_order_by>`.
   */
  cdc_order_by?: string | null
  /**
   * Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
   * "2024-12-09T16:09:53+00:00".
   *
   * When this option is set, the connector finds and opens the version of the table as of the
   * specified point in time (based on the server time recorded in the transaction log, not the
   * event time encoded in the data).  In `snapshot` and `snapshot_and_follow` modes, it
   * retrieves the snapshot of this version of the table.  In `follow`, `snapshot_and_follow`, and
   * `cdc` modes, it follows transaction log records **after** this version.
   *
   * Note: at most one of `version` and `datetime` options can be specified.
   * When neither of the two options is specified, the latest committed version of the table
   * is used.
   */
  datetime?: string | null
  /**
   * Optional final table version.
   *
   * Valid only when the connector is configured in `follow`, `snapshot_and_follow`, or `cdc` mode.
   *
   * When set, the connector will stop scanning the table’s transaction log after reaching this version or any greater version.
   * This bound is inclusive: if the specified version appears in the log, it will be processed before signaling end-of-input.
   */
  end_version?: number | null
  /**
   * Optional row filter.
   *
   * When specified, only rows that satisfy the filter condition are read from the delta table.
   * The condition must be a valid SQL Boolean expression that can be used in
   * the `where` clause of the `select * from my_table where ...` query.
   */
  filter?: string | null
  /**
   * Maximum number of concurrent object store reads performed by all Delta Lake connectors.
   *
   * This setting is used to limit the number of concurrent reads of the object store in a
   * pipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously
   * reading from the object store, this can lead to transport timeouts.
   *
   * When enabled, this setting limits the number of concurrent reads across all connectors.
   * This is a global setting that affects all Delta Lake connectors, and not just the connector
   * where it is specified. It should therefore be used at most once in a pipeline.  If multiple
   * connectors specify this setting, they must all use the same value.
   *
   * The default value is 6.
   */
  max_concurrent_readers?: number | null
  mode: DeltaTableIngestMode
  /**
   * The number of parallel parsing tasks the connector uses to process data read from the
   * table. Increasing this value can enhance performance by allowing more concurrent processing.
   * Recommended range: 1–10. The default is 4.
   */
  num_parsers?: number
  /**
   * Don't read unused columns from the Delta table.
   *
   * When set to `true`, this option instructs the connector to avoid reading
   * columns from the Delta table that are not used in any view definitions.
   * To be skipped, the columns must be either nullable or have default
   * values. This can improve ingestion performance, especially for wide
   * tables.
   *
   * Note: The simplest way to exclude unused columns is to omit them from the Feldera SQL table
   * declaration. The connector never reads columns that aren't declared in the SQL schema.
   * Additionally, the SQL compiler emits warnings for declared but unused columns—use these as
   * a guide to optimize your schema.
   */
  skip_unused_columns?: boolean
  /**
   * Optional snapshot filter.
   *
   * This option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.
   *
   * When specified, only rows that satisfy the filter condition are included in the
   * snapshot.  The condition must be a valid SQL Boolean expression that can be used in
   * the `where` clause of the `select * from snapshot where ...` query.
   *
   * Unlike the `filter` option, which applies to all records retrieved from the table, this
   * filter only applies to rows in the initial snapshot of the table.
   * For instance, it can be used to specify the range of event times to include in the snapshot,
   * e.g.: `ts BETWEEN TIMESTAMP '2005-01-01 00:00:00' AND TIMESTAMP '2010-12-31 23:59:59'`.
   *
   * This option can be used together with the `filter` option. During the initial snapshot,
   * only rows that satisfy both `filter` and `snapshot_filter` are retrieved from the Delta table.
   * When subsequently following changes in the the transaction log (`mode = snapshot_and_follow`),
   * all rows that meet the `filter` condition are ingested, regardless of `snapshot_filter`.
   */
  snapshot_filter?: string | null
  /**
   * Table column that serves as an event timestamp.
   *
   * When this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,
   * table rows are ingested in the timestamp order, respecting the
   * [`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)
   * property of the column: each ingested row has a timestamp no more than `LATENESS`
   * time units earlier than the most recent timestamp of any previously ingested row.
   * The ingestion is performed by partitioning the table into timestamp ranges of width
   * `LATENESS`. Each range is processed sequentially, in increasing timestamp order.
   *
   * # Example
   *
   * Consider a table with timestamp column of type `TIMESTAMP` and lateness attribute
   * `INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is
   * `2024-01-01T00:00:00``, the connector will fetch all records with timestamps
   * from `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records
   * in the table have been ingested.
   *
   * # Requirements
   *
   * * The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.
   * * The timestamp column must be declared with non-zero `LATENESS`.
   * * For efficient ingest, the table must be optimized for timestamp-based
   * queries using partitioning, Z-ordering, or liquid clustering.
   */
  timestamp_column?: string | null
  /**
   * Table URI.
   *
   * Example: "s3://feldera-fraud-detection-data/demographics_train"
   */
  uri: string
  /**
   * Optional table version.
   *
   * When this option is set, the connector finds and opens the specified version of the table.
   * In `snapshot` and `snapshot_and_follow` modes, it retrieves the snapshot of this version of
   * the table.  In `follow`, `snapshot_and_follow`, and `cdc` modes, it follows transaction log records
   * **after** this version.
   *
   * Note: at most one of `version` and `datetime` options can be specified.
   * When neither of the two options is specified, the latest committed version of the table
   * is used.
   */
  version?: number | null
  /**
   * Storage options for configuring backend object store.
   *
   * For specific options available for different storage backends, see:
   * * [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
   * * [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
   * * [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
   */
  '[key: string]': (string | number | DeltaTableIngestMode | boolean) | undefined
}

/**
 * Delta table write mode.
 *
 * Determines how the Delta table connector handles an existing table at the target location.
 */
export type DeltaTableWriteMode = 'append' | 'truncate' | 'error_if_exists'

/**
 * Delta table output connector configuration.
 */
export type DeltaTableWriterConfig = {
  mode?: DeltaTableWriteMode
  /**
   * Table URI.
   */
  uri: string
  /**
   * Storage options for configuring backend object store.
   *
   * For specific options available for different storage backends, see:
   * * [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
   * * [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
   * * [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
   */
  '[key: string]': (string | DeltaTableWriteMode) | undefined
}

export type Demo = {
  /**
   * Description of the demo (parsed from SQL preamble).
   */
  description: string
  /**
   * Name of the demo (parsed from SQL preamble).
   */
  name: string
  /**
   * Program SQL code.
   */
  program_code: string
  /**
   * Title of the demo (parsed from SQL preamble).
   */
  title: string
  /**
   * User defined function (UDF) Rust code.
   */
  udf_rust: string
  /**
   * User defined function (UDF) TOML dependencies.
   */
  udf_toml: string
}

export type DisplaySchedule =
  | 'Once'
  | 'Session'
  | {
      /**
       * Display it again after a certain period of time after it is dismissed
       */
      Every: {
        seconds: number
      }
    }
  | 'Always'

/**
 * Information returned by REST API endpoints on error.
 */
export type ErrorResponse = {
  /**
   * Detailed error metadata.
   * The contents of this field is determined by `error_code`.
   */
  details: {
    [key: string]: unknown
  }
  /**
   * Error code is a string that specifies this error type.
   */
  error_code: string
  /**
   * Human-readable error message.
   */
  message: string
}

/**
 * A SQL field.
 *
 * Matches the SQL compiler JSON format.
 */
export type Field = SqlIdentifier & {
  columntype: ColumnType
  default?: string | null
  lateness?: string | null
  unused: boolean
  watermark?: string | null
}

/**
 * Configuration for local file system access.
 */
export type FileBackendConfig = {
  /**
   * Whether to use background threads for file I/O.
   *
   * Background threads should improve performance, but they can reduce
   * performance if too few cores are available. This is provided for
   * debugging and fine-tuning and should ordinarily be left unset.
   */
  async_threads?: boolean | null
  /**
   * Per-I/O operation sleep duration, in milliseconds.
   *
   * This is for simulating slow storage devices.  Do not use this in
   * production.
   */
  ioop_delay?: number | null
  sync?: SyncConfig | null
}

/**
 * Configuration for reading data from a file with `FileInputTransport`
 */
export type FileInputConfig = {
  /**
   * Read buffer size.
   *
   * Default: when this parameter is not specified, a platform-specific
   * default is used.
   */
  buffer_size_bytes?: number | null
  /**
   * Enable file following.
   *
   * When `false`, the endpoint outputs an `InputConsumer::eoi`
   * message and stops upon reaching the end of file.  When `true`, the
   * endpoint will keep watching the file and outputting any new content
   * appended to it.
   */
  follow?: boolean
  /**
   * File path.
   *
   * This may be a file name or a `file://` URL with an absolute path.
   */
  path: string
}

/**
 * Configuration for writing data to a file with `FileOutputTransport`.
 */
export type FileOutputConfig = {
  /**
   * File path.
   */
  path: string
}

/**
 * Data format specification used to parse raw data received from the
 * endpoint or to encode data sent to the endpoint.
 */
export type FormatConfig = {
  /**
   * Format-specific parser or encoder configuration.
   */
  config?: {
    [key: string]: unknown
  }
  /**
   * Format name, e.g., "csv", "json", "bincode", etc.
   */
  name: string
}

/**
 * Fault-tolerance configuration.
 *
 * The default [FtConfig] (via [FtConfig::default]) disables fault tolerance,
 * which is the configuration that one gets if [RuntimeConfig] omits fault
 * tolerance configuration.
 *
 * The default value for [FtConfig::model] enables fault tolerance, as
 * `Some(FtModel::default())`.  This is the configuration that one gets if
 * [RuntimeConfig] includes a fault tolerance configuration but does not
 * specify a particular model.
 */
export type FtConfig = {
  /**
   * Interval between automatic checkpoints, in seconds.
   *
   * The default is 60 seconds.  Values less than 1 or greater than 3600 will
   * be forced into that range.
   */
  checkpoint_interval_secs?: number | null
  model?: FtModel | 'none'
}

/**
 * Fault tolerance model.
 *
 * The ordering is significant: we consider [Self::ExactlyOnce] to be a "higher
 * level" of fault tolerance than [Self::AtLeastOnce].
 */
export type FtModel = 'at_least_once' | 'exactly_once'

/**
 * A random generation plan for a table that generates either a limited amount of rows or runs continuously.
 */
export type GenerationPlan = {
  /**
   * Specifies the values that the generator should produce.
   */
  fields?: {
    [key: string]: RngFieldSettings
  }
  /**
   * Total number of new rows to generate.
   *
   * If not set, the generator will produce new/unique records as long as the pipeline is running.
   * If set to 0, the table will always remain empty.
   * If set, the generator will produce new records until the specified limit is reached.
   *
   * Note that if the table has one or more primary keys that don't use the `increment` strategy to
   * generate the key there is a potential that an update is generated instead of an insert. In
   * this case it's possible the total number of records is less than the specified limit.
   */
  limit?: number | null
  /**
   * Non-zero number of rows to generate per second.
   *
   * If not set, the generator will produce rows as fast as possible.
   */
  rate?: number | null
  /**
   * When multiple workers are used, each worker will pick a consecutive "chunk" of
   * records to generate.
   *
   * By default, if not specified, the generator will use the formula `min(rate, 10_000)`
   * to determine it. This works well in most situations. However, if you're
   * running tests with lateness and many workers you can e.g., reduce the
   * chunk size to make sure a smaller range of records is being ingested in parallel.
   *
   * This also controls the sizes of input batches.  If, for example, `rate`
   * and `worker_chunk_size` are both 1000, with a single worker, the
   * generator will output 1000 records once a second.  But if we reduce
   * `worker_chunk_size` to 100 without changing `rate`, the generator will
   * instead output 100 records 10 times per second.
   *
   * # Example
   * Assume you generate a total of 125 records with 4 workers and a chunk size of 25.
   * In this case, worker A will generate records 0..25, worker B will generate records 25..50,
   * etc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk
   * will pick up the last chunk of records (100..125) to generate.
   */
  worker_chunk_size?: number | null
}

/**
 * Query parameters to GET a pipeline or a list of pipelines.
 */
export type GetPipelineParameters = {
  selector?: PipelineFieldSelector
}

/**
 * AWS Glue catalog config.
 */
export type GlueCatalogConfig = {
  /**
   * Access key id used to access the Glue catalog.
   */
  'glue.access-key-id'?: string | null
  /**
   * Configure an alternative endpoint of the Glue service for Glue catalog to access.
   *
   * Example: `"https://glue.us-east-1.amazonaws.com"`
   */
  'glue.endpoint'?: string | null
  /**
   * The 12-digit ID of the Glue catalog.
   */
  'glue.id'?: string | null
  /**
   * Profile used to access the Glue catalog.
   */
  'glue.profile-name'?: string | null
  /**
   * Region of the Glue catalog.
   */
  'glue.region'?: string | null
  /**
   * Secret access key used to access the Glue catalog.
   */
  'glue.secret-access-key'?: string | null
  'glue.session-token'?: string | null
  /**
   * Location for table metadata.
   *
   * Example: `"s3://my-data-warehouse/tables/"`
   */
  'glue.warehouse'?: string | null
}

export type HealthStatus = {
  compiler: ServiceStatus
  runner: ServiceStatus
}

/**
 * Configuration for reading data via HTTP.
 *
 * HTTP input adapters cannot be usefully configured as part of pipeline
 * configuration.  Instead, instantiate them through the REST API as
 * `/pipelines/{pipeline_name}/ingress/{table_name}`.
 */
export type HttpInputConfig = {
  /**
   * Autogenerated name.
   */
  name: string
}

export type IcebergCatalogType = 'rest' | 'glue'

/**
 * Iceberg table read mode.
 *
 * Three options are available:
 *
 * * `snapshot` - read a snapshot of the table and stop.
 *
 * * `follow` - continuously ingest changes to the table, starting from a specified snapshot
 * or timestamp.
 *
 * * `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion
 * mode.
 */
export type IcebergIngestMode = 'snapshot' | 'follow' | 'snapshot_and_follow'

/**
 * Iceberg input connector configuration.
 */
export type IcebergReaderConfig = GlueCatalogConfig &
  RestCatalogConfig & {
    catalog_type?: IcebergCatalogType | null
    /**
     * Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
     * "2024-12-09T16:09:53+00:00".
     *
     * When this option is set, the connector finds and opens the snapshot of the table as of the
     * specified point in time (based on the server time recorded in the transaction
     * log, not the event time encoded in the data).  In `snapshot` and `snapshot_and_follow`
     * modes, it retrieves this snapshot.  In `follow` and `snapshot_and_follow` modes, it
     * follows transaction log records **after** this snapshot.
     *
     * Note: at most one of `snapshot_id` and `datetime` options can be specified.
     * When neither of the two options is specified, the latest committed version of the table
     * is used.
     */
    datetime?: string | null
    /**
     * Location of the table metadata JSON file.
     *
     * This propery is used to access an Iceberg table without a catalog. It is mutually
     * exclusive with the `catalog_type` property.
     */
    metadata_location?: string | null
    mode: IcebergIngestMode
    /**
     * Optional row filter.
     *
     * This option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.
     *
     * When specified, only rows that satisfy the filter condition are included in the
     * snapshot.  The condition must be a valid SQL Boolean expression that can be used in
     * the `where` clause of the `select * from snapshot where ...` query.
     *
     * This option can be used to specify the range of event times to include in the snapshot,
     * e.g.: `ts BETWEEN '2005-01-01 00:00:00' AND '2010-12-31 23:59:59'`.
     */
    snapshot_filter?: string | null
    /**
     * Optional snapshot id.
     *
     * When this option is set, the connector finds the specified snapshot of the table.
     * In `snapshot` and `snapshot_and_follow` modes, it loads this snapshot.
     * In `follow` and `snapshot_and_follow` modes, it follows table updates
     * **after** this snapshot.
     *
     * Note: at most one of `snapshot_id` and `datetime` options can be specified.
     * When neither of the two options is specified, the latest committed version of the table
     * is used.
     */
    snapshot_id?: number | null
    /**
     * Specifies the Iceberg table name in the "namespace.table" format.
     *
     * This option is applicable when an Iceberg catalog is configured using the `catalog_type` property.
     */
    table_name?: string | null
    /**
     * Table column that serves as an event timestamp.
     *
     * When this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,
     * table rows are ingested in the timestamp order, respecting the
     * [`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)
     * property of the column: each ingested row has a timestamp no more than `LATENESS`
     * time units earlier than the most recent timestamp of any previously ingested row.
     * The ingestion is performed by partitioning the table into timestamp ranges of width
     * `LATENESS`. Each range is processed sequentially, in increasing timestamp order.
     *
     * # Example
     *
     * Consider a table with timestamp column of type `TIMESTAMP` and lateness attribute
     * `INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is
     * `2024-01-01T00:00:00``, the connector will fetch all records with timestamps
     * from `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records
     * in the table have been ingested.
     *
     * # Requirements
     *
     * * The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.
     * * The timestamp column must be declared with non-zero `LATENESS`.
     * * For efficient ingest, the table must be optimized for timestamp-based
     * queries using partitioning, Z-ordering, or liquid clustering.
     */
    timestamp_column?: string | null
    /**
     * Storage options for configuring backend object store.
     *
     * See the [list of available options in PyIceberg documentation](https://py.iceberg.apache.org/configuration/#fileio).
     */
    '[key: string]': (string | unknown | IcebergIngestMode | number) | undefined
  }

/**
 * Describes an input connector configuration
 */
export type InputEndpointConfig = ConnectorConfig & {
  /**
   * The name of the input stream of the circuit that this endpoint is
   * connected to.
   */
  stream: string
}

/**
 * The specified units for SQL Interval types.
 *
 * `INTERVAL 1 DAY`, `INTERVAL 1 DAY TO HOUR`, `INTERVAL 1 DAY TO MINUTE`,
 * would yield `Day`, `DayToHour`, `DayToMinute`, as the `IntervalUnit` respectively.
 */
export type IntervalUnit =
  | 'Day'
  | 'DayToHour'
  | 'DayToMinute'
  | 'DayToSecond'
  | 'Hour'
  | 'HourToMinute'
  | 'HourToSecond'
  | 'Minute'
  | 'MinuteToSecond'
  | 'Month'
  | 'Second'
  | 'Year'
  | 'YearToMonth'

/**
 * Whether JSON values can span multiple lines.
 */
export type JsonLines = 'multiple' | 'single'

/**
 * Supported JSON data change event formats.
 *
 * Each element in a JSON-formatted input stream specifies
 * an update to one or more records in an input table.  We support
 * several different ways to represent such updates.
 *
 * ### `InsertDelete`
 *
 * Each element in the input stream consists of an "insert" or "delete"
 * command and a record to be inserted to or deleted from the input table.
 *
 * ```json
 * {"insert": {"column1": "hello, world!", "column2": 100}}
 * ```
 *
 * ### `Weighted`
 *
 * Each element in the input stream consists of a record and a weight
 * which indicates how many times the row appears.
 *
 * ```json
 * {"weight": 2, "data": {"column1": "hello, world!", "column2": 100}}
 * ```
 *
 * Note that the line above would be equivalent to the following input in the `InsertDelete` format:
 *
 * ```json
 * {"insert": {"column1": "hello, world!", "column2": 100}}
 * {"insert": {"column1": "hello, world!", "column2": 100}}
 * ```
 *
 * Similarly, negative weights are equivalent to deletions:
 *
 * ```json
 * {"weight": -1, "data": {"column1": "hello, world!", "column2": 100}}
 * ```
 *
 * is equivalent to in the `InsertDelete` format:
 *
 * ```json
 * {"delete": {"column1": "hello, world!", "column2": 100}}
 * ```
 *
 * ### `Debezium`
 *
 * Debezium CDC format.  Refer to [Debezium input connector documentation](https://docs.feldera.com/connectors/sources/debezium) for details.
 *
 * ### `Snowflake`
 *
 * Uses flat structure so that fields can get parsed directly into SQL
 * columns.  Defines three metadata fields:
 *
 * * `__action` - "insert" or "delete"
 * * `__stream_id` - unique 64-bit ID of the output stream (records within
 * a stream are totally ordered)
 * * `__seq_number` - monotonically increasing sequence number relative to
 * the start of the stream.
 *
 * ```json
 * {"PART":1,"VENDOR":2,"EFFECTIVE_SINCE":"2019-05-21","PRICE":"10000","__action":"insert","__stream_id":4523666124030717756,"__seq_number":1}
 * ```
 *
 * ### `Raw`
 *
 * This format is suitable for insert-only streams (no deletions).
 * Each element in the input stream contains a record without any
 * additional envelope that gets inserted in the input table.
 */
export type JsonUpdateFormat =
  | 'insert_delete'
  | 'weighted'
  | 'debezium'
  | 'snowflake'
  | 'raw'
  | 'redis'

/**
 * Kafka message header.
 */
export type KafkaHeader = {
  key: string
  value?: KafkaHeaderValue | null
}

/**
 * Kafka header value encoded as a UTF-8 string or a byte array.
 */
export type KafkaHeaderValue = Blob | File

/**
 * Configuration for reading data from Kafka topics with `InputTransport`.
 */
export type KafkaInputConfig = {
  /**
   * Maximum timeout in seconds to wait for the endpoint to join the Kafka
   * consumer group during initialization.
   */
  group_join_timeout_secs?: number
  log_level?: KafkaLogLevel | null
  /**
   * The list of Kafka partitions to read from.
   *
   * Only the specified partitions will be consumed. If this field is not set,
   * the connector will consume from all available partitions.
   *
   * If `start_from` is set to `offsets` and this field is provided, the
   * number of partitions must exactly match the number of offsets, and the
   * order of partitions must correspond to the order of offsets.
   *
   * If offsets are provided for all partitions, this field can be omitted.
   */
  partitions?: Array<number> | null
  /**
   * Set to 1 or more to fix the number of threads used to poll
   * `rdkafka`. Multiple threads can increase performance with small Kafka
   * messages; for large messages, one thread is enough. In either case, too
   * many threads can harm performance. If unset, the default is 3, which
   * helps with small messages but will not harm performance with large
   * messagee
   */
  poller_threads?: number | null
  /**
   * The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).
   */
  region?: string | null
  /**
   * By default, if the input connector resumes from a checkpoint and the
   * data where it needs to resume has expired from the Kafka topic, the
   * input connector fails initialization and the pipeline will fail to start.
   *
   * Set this to true to change the behavior so that, if data is not
   * available on resume, the input connector starts from the earliest
   * offsets that are now available.
   */
  resume_earliest_if_data_expires: boolean
  start_from?: KafkaStartFromConfig
  /**
   * Topic to subscribe to.
   */
  topic: string
  /**
   * Options passed directly to `rdkafka`.
   *
   * [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
   * used to configure the Kafka consumer.
   *
   * This input connector does not use consumer groups, so options related to
   * consumer groups are rejected, including:
   *
   * * `group.id`, if present, is ignored.
   * * `auto.offset.reset` (use `start_from` instead).
   * * "enable.auto.commit", if present, must be set to "false".
   * * "enable.auto.offset.store", if present, must be set to "false".
   */
  '[key: string]': (string | number | unknown | boolean | KafkaStartFromConfig) | undefined
}

/**
 * Kafka logging levels.
 */
export type KafkaLogLevel =
  | 'emerg'
  | 'alert'
  | 'critical'
  | 'error'
  | 'warning'
  | 'notice'
  | 'info'
  | 'debug'

/**
 * Configuration for writing data to a Kafka topic with `OutputTransport`.
 */
export type KafkaOutputConfig = {
  fault_tolerance?: KafkaOutputFtConfig | null
  /**
   * Kafka headers to be added to each message produced by this connector.
   */
  headers?: Array<KafkaHeader>
  /**
   * Maximum timeout in seconds to wait for the endpoint to connect to
   * a Kafka broker.
   *
   * Defaults to 60.
   */
  initialization_timeout_secs?: number
  /**
   * If specified, this service is used to provide defaults for the Kafka options.
   */
  kafka_service?: string | null
  log_level?: KafkaLogLevel | null
  /**
   * The AWS region to use while connecting to AWS Managed Streaming for Kafka (MSK).
   */
  region?: string | null
  /**
   * Topic to write to.
   */
  topic: string
  /**
   * Options passed directly to `rdkafka`.
   *
   * See [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
   * used to configure the Kafka producer.
   */
  '[key: string]': (string | unknown | KafkaHeader | number) | undefined
}

/**
 * Fault tolerance configuration for Kafka output connector.
 */
export type KafkaOutputFtConfig = {
  /**
   * Options passed to `rdkafka` for consumers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for consumers, and may be empty.
   */
  consumer_options?: {
    [key: string]: string
  }
  /**
   * Options passed to `rdkafka` for producers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for producers, and may be empty.
   */
  producer_options?: {
    [key: string]: string
  }
}

/**
 * Where to begin reading a Kafka topic.
 */
export type KafkaStartFromConfig =
  | 'earliest'
  | 'latest'
  | {
      /**
       * Start from particular offsets in the topic.
       *
       * The number of offsets must match the number of partitions in the topic.
       */
      offsets: Array<number>
    }
  | {
      /**
       * Start from a particular timestamp in the topic.
       *
       * Kafka timestamps are in milliseconds since the epoch.
       */
      timestamp: number
    }

export type LicenseInformation = {
  /**
   * Timestamp when the server responded.
   */
  current: string
  /**
   * Optional description of the advantages of extending the license / upgrading from a trial
   */
  description_html: string
  /**
   * URL that navigates the user to extend / upgrade their license
   */
  extension_url?: string | null
  /**
   * Whether the license is a trial
   */
  is_trial: boolean
  remind_schedule: DisplaySchedule
  /**
   * Timestamp from which the user should be reminded of the license expiring soon
   */
  remind_starting_at?: string | null
  /**
   * Timestamp at which point the license expires
   */
  valid_until?: string | null
}

export type LicenseValidity =
  | {
      Exists: LicenseInformation
    }
  | {
      /**
       * Either the license key is invalid according to the server, or the request that checks with
       * the server failed (e.g., if it could not reach the server).
       */
      DoesNotExistOrNotConfirmed: string
    }

/**
 * Circuit metrics output format.
 * - `prometheus`: [format](https://github.com/prometheus/docs/blob/4b1b80f5f660a2f8dc25a54f52a65a502f31879a/docs/instrumenting/exposition_formats.md) expected by Prometheus
 * - `json`: JSON format
 */
export type MetricsFormat = 'prometheus' | 'json'

/**
 * Query parameters to retrieve pipeline circuit metrics.
 */
export type MetricsParameters = {
  format?: MetricsFormat
}

export type MirInput = {
  node: string
  output: number
  '[key: string]': (unknown | string | number) | undefined
}

export type MirNode = {
  calcite?: CalciteId | null
  inputs?: Array<MirInput>
  operation: string
  outputs?: Array<MirInput | null> | null
  persistent_id?: string | null
  positions?: Array<SourcePosition>
  table?: string | null
  view?: string | null
  '[key: string]': (unknown | MirInput | string | SourcePosition) | undefined
}

export type NatsInputConfig = {
  connection_config: ConnectOptions
  consumer_config: ConsumerConfig
  stream_name: string
}

/**
 * Request to create a new API key.
 */
export type NewApiKeyRequest = {
  /**
   * Key name.
   */
  name: string
}

/**
 * Response to a successful API key creation.
 */
export type NewApiKeyResponse = {
  /**
   * Generated secret API key. There is no way to retrieve this
   * key again through the API, so store it securely.
   */
  api_key: string
  id: ApiKeyId
  /**
   * API key name provided by the user.
   */
  name: string
}

/**
 * Configuration for generating Nexmark input data.
 *
 * This connector must be used exactly three times in a pipeline if it is used
 * at all, once for each [`NexmarkTable`].
 */
export type NexmarkInputConfig = {
  options?: NexmarkInputOptions | null
  table: NexmarkTable
}

/**
 * Configuration for generating Nexmark input data.
 */
export type NexmarkInputOptions = {
  /**
   * Number of events to generate and submit together, per thread.
   *
   * Each thread generates this many records, which are then combined with
   * the records generated by the other threads, to form combined input
   * batches of size `threads × batch_size_per_thread`.
   */
  batch_size_per_thread?: number
  /**
   * Number of events to generate.
   */
  events?: number
  /**
   * Maximum number of events to submit in a single step, per thread.
   *
   * This should really be per worker thread, not per generator thread, but
   * the connector does not know how many worker threads there are.
   *
   * This stands in for `max_batch_size` from the connector configuration
   * because it must be a constant across all three of the nexmark tables.
   */
  max_step_size_per_thread?: number
  /**
   * Number of event generator threads.
   *
   * It's reasonable to choose the same number of generator threads as worker
   * threads.
   */
  threads?: number
}

/**
 * Table in Nexmark.
 */
export type NexmarkTable = 'bid' | 'auction' | 'person'

export type ObjectStorageConfig = {
  /**
   * URL.
   *
   * The following URL schemes are supported:
   *
   * * S3:
   * - `s3://<bucket>/<path>`
   * - `s3a://<bucket>/<path>`
   * - `https://s3.<region>.amazonaws.com/<bucket>`
   * - `https://<bucket>.s3.<region>.amazonaws.com`
   * - `https://ACCOUNT_ID.r2.cloudflarestorage.com/bucket`
   * * Google Cloud Storage:
   * - `gs://<bucket>/<path>`
   * * Microsoft Azure Blob Storage:
   * - `abfs[s]://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))
   * - `abfs[s]://<file_system>@<account_name>.dfs.core.windows.net/<path>`
   * - `abfs[s]://<file_system>@<account_name>.dfs.fabric.microsoft.com/<path>`
   * - `az://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))
   * - `adl://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))
   * - `azure://<container>/<path>` (custom)
   * - `https://<account>.dfs.core.windows.net`
   * - `https://<account>.blob.core.windows.net`
   * - `https://<account>.blob.core.windows.net/<container>`
   * - `https://<account>.dfs.fabric.microsoft.com`
   * - `https://<account>.dfs.fabric.microsoft.com/<container>`
   * - `https://<account>.blob.fabric.microsoft.com`
   * - `https://<account>.blob.fabric.microsoft.com/<container>`
   *
   * Settings derived from the URL will override other settings.
   */
  url: string
  /**
   * Additional options as key-value pairs.
   *
   * The following keys are supported:
   *
   * * S3:
   * - `access_key_id`: AWS Access Key.
   * - `secret_access_key`: AWS Secret Access Key.
   * - `region`: Region.
   * - `default_region`: Default region.
   * - `endpoint`: Custom endpoint for communicating with S3,
   * e.g. `https://localhost:4566` for testing against a localstack
   * instance.
   * - `token`: Token to use for requests (passed to underlying provider).
   * - [Other keys](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html#variants).
   * * Google Cloud Storage:
   * - `service_account`: Path to the service account file.
   * - `service_account_key`: The serialized service account key.
   * - `google_application_credentials`: Application credentials path.
   * - [Other keys](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html).
   * * Microsoft Azure Blob Storage:
   * - `access_key`: Azure Access Key.
   * - `container_name`: Azure Container Name.
   * - `account`: Azure Account.
   * - `bearer_token_authorization`: Static bearer token for authorizing requests.
   * - `client_id`: Client ID for use in client secret or Kubernetes federated credential flow.
   * - `client_secret`: Client secret for use in client secret flow.
   * - `tenant_id`: Tenant ID for use in client secret or Kubernetes federated credential flow.
   * - `endpoint`: Override the endpoint for communicating with blob storage.
   * - [Other keys](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html#variants).
   *
   * Options set through the URL take precedence over those set with these
   * options.
   */
  '[key: string]': string | undefined
}

export type Op = {
  kind: string
  name: string
  syntax: string
  '[key: string]': (unknown | string) | undefined
}

export type Operand = {
  input?: number | null
  name?: string | null
  '[key: string]': (unknown | number | string) | undefined
}

export type OutputBufferConfig = {
  /**
   * Enable output buffering.
   *
   * The output buffering mechanism allows decoupling the rate at which the pipeline
   * pushes changes to the output transport from the rate of input changes.
   *
   * By default, output updates produced by the pipeline are pushed directly to
   * the output transport. Some destinations may prefer to receive updates in fewer
   * bigger batches. For instance, when writing Parquet files, producing
   * one bigger file every few minutes is usually better than creating
   * small files every few milliseconds.
   *
   * To achieve such input/output decoupling, users can enable output buffering by
   * setting the `enable_output_buffer` flag to `true`.  When buffering is enabled, output
   * updates produced by the pipeline are consolidated in an internal buffer and are
   * pushed to the output transport when one of several conditions is satisfied:
   *
   * * data has been accumulated in the buffer for more than `max_output_buffer_time_millis`
   * milliseconds.
   * * buffer size exceeds `max_output_buffer_size_records` records.
   *
   * This flag is `false` by default.
   */
  enable_output_buffer?: boolean
  /**
   * Maximum number of updates to be kept in the output buffer.
   *
   * This parameter bounds the maximal size of the buffer.
   * Note that the size of the buffer is not always equal to the
   * total number of updates output by the pipeline. Updates to the
   * same record can overwrite or cancel previous updates.
   *
   * By default, the buffer can grow indefinitely until one of
   * the other output conditions is satisfied.
   *
   * NOTE: this configuration option requires the `enable_output_buffer` flag
   * to be set.
   */
  max_output_buffer_size_records?: number
  /**
   * Maximum time in milliseconds data is kept in the output buffer.
   *
   * By default, data is kept in the buffer indefinitely until one of
   * the other output conditions is satisfied.  When this option is
   * set the buffer will be flushed at most every
   * `max_output_buffer_time_millis` milliseconds.
   *
   * NOTE: this configuration option requires the `enable_output_buffer` flag
   * to be set.
   */
  max_output_buffer_time_millis?: number
}

/**
 * Describes an output connector configuration
 */
export type OutputEndpointConfig = ConnectorConfig & {
  /**
   * The name of the output stream of the circuit that this endpoint is
   * connected to.
   */
  stream: string
}

/**
 * Program information is the result of the SQL compilation.
 */
export type PartialProgramInfo = {
  /**
   * Input connectors derived from the schema.
   */
  input_connectors: {
    [key: string]: InputEndpointConfig
  }
  /**
   * Output connectors derived from the schema.
   */
  output_connectors: {
    [key: string]: OutputEndpointConfig
  }
  schema: ProgramSchema
  /**
   * Generated user defined function (UDF) stubs Rust code: stubs.rs
   */
  udf_stubs: string
}

/**
 * Partially update the pipeline (PATCH).
 *
 * Note that the patching only applies to the main fields, not subfields.
 * For instance, it is not possible to update only the number of workers;
 * it is required to again pass the whole runtime configuration with the
 * change.
 */
export type PatchPipeline = {
  description?: string | null
  name?: string | null
  program_code?: string | null
  program_config?: ProgramConfig | null
  runtime_config?: RuntimeConfig | null
  udf_rust?: string | null
  udf_toml?: string | null
}

/**
 * Pipeline deployment configuration.
 * It represents configuration entries directly provided by the user
 * (e.g., runtime configuration) and entries derived from the schema
 * of the compiled program (e.g., connectors). Storage configuration,
 * if applicable, is set by the runner.
 */
export type PipelineConfig = {
  /**
   * Deprecated: setting this true or false does not have an effect anymore.
   */
  checkpoint_during_suspend?: boolean
  /**
   * Real-time clock resolution in microseconds.
   *
   * This parameter controls the execution of queries that use the `NOW()` function.  The output of such
   * queries depends on the real-time clock and can change over time without any external
   * inputs.  If the query uses `NOW()`, the pipeline will update the clock value and trigger incremental
   * recomputation at most each `clock_resolution_usecs` microseconds.  If the query does not use
   * `NOW()`, then clock value updates are suppressed and the pipeline ignores this setting.
   *
   * It is set to 1 second (1,000,000 microseconds) by default.
   */
  clock_resolution_usecs?: number | null
  /**
   * Enable CPU profiler.
   *
   * The default value is `true`.
   */
  cpu_profiler?: boolean
  /**
   * Optional settings for tweaking Feldera internals.
   *
   * The available key-value pairs change from one version of Feldera to
   * another, so users should not depend on particular settings being
   * available, or on their behavior.
   */
  dev_tweaks?: {
    [key: string]: unknown
  }
  fault_tolerance?: FtConfig
  /**
   * Sets the number of available runtime threads for the http server.
   *
   * In most cases, this does not need to be set explicitly and
   * the default is sufficient. Can be increased in case the
   * pipeline HTTP API operations are a bottleneck.
   *
   * If not specified, the default is set to `workers`.
   */
  http_workers?: number | null
  /**
   * Specification of additional (sidecar) containers.
   */
  init_containers?: unknown
  /**
   * Sets the number of available runtime threads for async IO tasks.
   *
   * This affects some networking and file I/O operations
   * especially adapters and ad-hoc queries.
   *
   * In most cases, this does not need to be set explicitly and
   * the default is sufficient. Can be increased in case
   * ingress, egress or ad-hoc queries are a bottleneck.
   *
   * If not specified, the default is set to `workers`.
   */
  io_workers?: number | null
  /**
   * Log filtering directives.
   *
   * If set to a valid [tracing-subscriber] filter, this controls the log
   * messages emitted by the pipeline process.  Otherwise, or if the filter
   * has invalid syntax, messages at "info" severity and higher are written
   * to the log and all others are discarded.
   *
   * [tracing-subscriber]: https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#directives
   */
  logging?: string | null
  /**
   * Maximal delay in microseconds to wait for `min_batch_size_records` to
   * get buffered by the controller, defaults to 0.
   */
  max_buffering_delay_usecs?: number
  /**
   * The maximum number of connectors initialized in parallel during pipeline
   * startup.
   *
   * At startup, the pipeline must initialize all of its input and output connectors.
   * Depending on the number and types of connectors, this can take a long time.
   * To accelerate the process, multiple connectors are initialized concurrently.
   * This option controls the maximum number of connectors that can be initialized
   * in parallel.
   *
   * The default is 10.
   */
  max_parallel_connector_init?: number | null
  /**
   * Minimal input batch size.
   *
   * The controller delays pushing input records to the circuit until at
   * least `min_batch_size_records` records have been received (total
   * across all endpoints) or `max_buffering_delay_usecs` microseconds
   * have passed since at least one input records has been buffered.
   * Defaults to 0.
   */
  min_batch_size_records?: number
  /**
   * Optionally, a list of CPU numbers for CPUs to which the pipeline may pin
   * its worker threads.  Specify at least twice as many CPU numbers as
   * workers.  CPUs are generally numbered starting from 0.  The pipeline
   * might not be able to honor CPU pinning requests.
   *
   * CPU pinning can make pipelines run faster and perform more consistently,
   * as long as different pipelines running on the same machine are pinned to
   * different CPUs.
   */
  pin_cpus?: Array<number>
  /**
   * Timeout in seconds for the `Provisioning` phase of the pipeline.
   * Setting this value will override the default of the runner.
   */
  provisioning_timeout_secs?: number | null
  resources?: ResourceConfig
  storage?: StorageOptions | null
  /**
   * Enable pipeline tracing.
   */
  tracing?: boolean
  /**
   * Jaeger tracing endpoint to send tracing information to.
   */
  tracing_endpoint_jaeger?: string
  /**
   * Number of DBSP worker threads.
   *
   * Each DBSP "foreground" worker thread is paired with a "background"
   * thread for LSM merging, making the total number of threads twice the
   * specified number.
   *
   * The typical sweet spot for the number of workers is between 4 and 16.
   * Each worker increases overall memory consumption for data structures
   * used during a step.
   */
  workers?: number
} & {
  /**
   * Input endpoint configuration.
   */
  inputs: {
    [key: string]: InputEndpointConfig
  }
  /**
   * Pipeline name.
   */
  name?: string | null
  /**
   * Output endpoint configuration.
   */
  outputs?: {
    [key: string]: OutputEndpointConfig
  }
  program_ir?: ProgramIr | null
  /**
   * Directory containing values of secrets.
   *
   * If this is not set, a default directory is used.
   */
  secrets_dir?: string | null
  storage_config?: StorageConfig | null
}

/**
 * Summary of changes in the pipeline between checkpointed and new versions.
 */
export type PipelineDiff = {
  added_input_connectors: Array<string>
  added_output_connectors: Array<string>
  modified_input_connectors: Array<string>
  modified_output_connectors: Array<string>
  program_diff?: ProgramDiff | null
  program_diff_error?: string | null
  removed_input_connectors: Array<string>
  removed_output_connectors: Array<string>
}

export type PipelineFieldSelector = 'all' | 'status' | 'status_with_connectors'

/**
 * Pipeline identifier.
 */
export type PipelineId = string

/**
 * Pipeline information.
 * It both includes fields which are user-provided and system-generated.
 */
export type PipelineInfo = {
  created_at: string
  deployment_desired_status: CombinedDesiredStatus
  deployment_desired_status_since: string
  deployment_error?: ErrorResponse | null
  deployment_id?: string | null
  deployment_initial?: RuntimeDesiredStatus | null
  deployment_resources_desired_status: ResourcesDesiredStatus
  deployment_resources_desired_status_since: string
  deployment_resources_status: ResourcesStatus
  deployment_resources_status_since: string
  deployment_runtime_desired_status?: RuntimeDesiredStatus | null
  deployment_runtime_desired_status_since?: string | null
  deployment_runtime_status?: RuntimeStatus | null
  deployment_runtime_status_since?: string | null
  deployment_status: CombinedStatus
  deployment_status_since: string
  description: string
  id: PipelineId
  name: string
  platform_version: string
  program_code: string
  program_config: ProgramConfig
  program_error: ProgramError
  program_info?: PartialProgramInfo | null
  program_status: ProgramStatus
  program_status_since: string
  program_version: Version
  refresh_version: Version
  runtime_config: RuntimeConfig
  storage_status: StorageStatus
  udf_rust: string
  udf_toml: string
  version: Version
}

/**
 * Pipeline information which has a selected subset of optional fields.
 * It both includes fields which are user-provided and system-generated.
 * If an optional field is not selected (i.e., is `None`), it will not be serialized.
 */
export type PipelineSelectedInfo = {
  connectors?: ConnectorStats | null
  created_at: string
  deployment_desired_status: CombinedDesiredStatus
  deployment_desired_status_since: string
  deployment_error?: ErrorResponse | null
  deployment_id?: string | null
  deployment_initial?: RuntimeDesiredStatus | null
  deployment_resources_desired_status: ResourcesDesiredStatus
  deployment_resources_desired_status_since: string
  deployment_resources_status: ResourcesStatus
  deployment_resources_status_since: string
  deployment_runtime_desired_status?: RuntimeDesiredStatus | null
  deployment_runtime_desired_status_since?: string | null
  deployment_runtime_status?: RuntimeStatus | null
  deployment_runtime_status_details?: unknown
  deployment_runtime_status_since?: string | null
  deployment_status: CombinedStatus
  deployment_status_since: string
  description: string
  id: PipelineId
  name: string
  platform_version: string
  program_code?: string | null
  program_config?: ProgramConfig | null
  program_error?: ProgramError | null
  program_info?: PartialProgramInfo | null
  program_status: ProgramStatus
  program_status_since: string
  program_version: Version
  refresh_version: Version
  runtime_config?: RuntimeConfig | null
  storage_status: StorageStatus
  udf_rust?: string | null
  udf_toml?: string | null
  version: Version
}

/**
 * Create a new pipeline (POST), or fully update an existing pipeline (PUT).
 * Fields which are optional and not provided will be set to their empty type value
 * (for strings: an empty string `""`, for objects: an empty dictionary `{}`).
 */
export type PostPutPipeline = {
  description?: string | null
  name: string
  program_code: string
  program_config?: ProgramConfig | null
  runtime_config?: RuntimeConfig | null
  udf_rust?: string | null
  udf_toml?: string | null
}

/**
 * Query parameters to POST a pipeline stop.
 */
export type PostStopPipelineParameters = {
  /**
   * The `force` parameter determines whether to immediately deprovision the pipeline compute
   * resources (`force=true`) or first attempt to atomically checkpoint before doing so
   * (`force=false`, which is the default).
   */
  force?: boolean
}

/**
 * Postgres input connector configuration.
 */
export type PostgresReaderConfig = {
  /**
   * Query that specifies what data to fetch from postgres.
   */
  query: string
  /**
   * Postgres URI.
   * See: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>
   */
  uri: string
}

/**
 * Postgres output connector configuration.
 */
export type PostgresWriterConfig = {
  /**
   * The maximum buffer size in for a single operation.
   * Note that the buffers of `INSERT`, `UPDATE` and `DELETE` queries are
   * separate.
   * Default: 1 MiB
   */
  max_buffer_size_bytes?: number
  /**
   * The maximum number of records in a single buffer.
   */
  max_records_in_buffer?: number | null
  /**
   * Specifies how the connector handles conflicts when executing an `INSERT`
   * into a table with a primary key. By default, an existing row with the same
   * key is overwritten. Setting this flag to `true` preserves the existing row
   * and ignores the new insert.
   *
   * This setting does not affect `UPDATE` statements, which always replace the
   * value associated with the key.
   *
   * Default: `false`
   */
  on_conflict_do_nothing?: boolean
  /**
   * Path to a file containing a sequence of CA certificates in PEM format.
   */
  ssl_ca_location?: string | null
  /**
   * A sequence of CA certificates in PEM format.
   */
  ssl_ca_pem?: string | null
  /**
   * The path to the certificate chain file.
   * The file must contain a sequence of PEM-formatted certificates,
   * the first being the leaf certificate, and the remainder forming
   * the chain of certificates up to and including the trusted root certificate.
   */
  ssl_certificate_chain_location?: string | null
  /**
   * The client certificate key in PEM format.
   */
  ssl_client_key?: string | null
  /**
   * Path to the client certificate key.
   */
  ssl_client_key_location?: string | null
  /**
   * Path to the client certificate.
   */
  ssl_client_location?: string | null
  /**
   * The client certificate in PEM format.
   */
  ssl_client_pem?: string | null
  /**
   * The table to write the output to.
   */
  table: string
  /**
   * Postgres URI.
   * See: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>
   */
  uri: string
  /**
   * True to enable hostname verification when using TLS. True by default.
   */
  verify_hostname?: boolean | null
}

/**
 * Program configuration.
 */
export type ProgramConfig = {
  /**
   * If `true` (default), when a prior compilation with the same checksum
   * already exists, the output of that (i.e., binary) is used.
   * Set `false` to always trigger a new compilation, which might take longer
   * and as well can result in overriding an existing binary.
   */
  cache?: boolean
  profile?: CompilationProfile | null
  /**
   * Override runtime version of the pipeline being executed.
   *
   * Warning: This setting is experimental and may change in the future.
   * Requires the platform to run with the unstable feature `runtime_version`
   * enabled. Should only be used for testing purposes, and requires
   * network access.
   *
   * A runtime version can be specified in the form of a version
   * or SHA taken from the `feldera/feldera` repository main branch.
   *
   * Examples: `v0.96.0` or `f4dcac0989ca0fda7d2eb93602a49d007cb3b0ae`
   *
   * A platform of version `0.x.y` may be capable of running future and past
   * runtimes with versions `>=0.x.y` and `<=0.x.y` until breaking API changes happen,
   * the exact bounds for each platform version are unspecified until we reach a
   * stable version. Compatibility is only guaranteed if platform and runtime version
   * are exact matches.
   *
   * Note that any enterprise features are currently considered to be part of
   * the platform.
   *
   * If not set (null), the runtime version will be the same as the platform version.
   */
  runtime_version?: string | null
}

/**
 * Summary of changes in the program between checkpointed and new versions.
 */
export type ProgramDiff = {
  added_tables: Array<string>
  added_views: Array<string>
  modified_tables: Array<string>
  modified_views: Array<string>
  removed_tables: Array<string>
  removed_views: Array<string>
}

/**
 * Log, warning and error information about the program compilation.
 */
export type ProgramError = {
  rust_compilation?: RustCompilationInfo | null
  sql_compilation?: SqlCompilationInfo | null
  /**
   * System error that occurred.
   * - Set `Some(...)` upon transition to `SystemError`
   * - Set `None` upon transition to `Pending`
   */
  system_error?: string | null
}

/**
 * Program information is the output of the SQL compiler.
 *
 * It includes information needed for Rust compilation (e.g., generated Rust code)
 * as well as only for runtime (e.g., schema, input/output connectors).
 */
export type ProgramInfo = {
  dataflow?: Dataflow | null
  /**
   * Input connectors derived from the schema.
   */
  input_connectors: {
    [key: string]: InputEndpointConfig
  }
  /**
   * Generated main program Rust code: main.rs
   */
  main_rust?: string
  /**
   * Output connectors derived from the schema.
   */
  output_connectors: {
    [key: string]: OutputEndpointConfig
  }
  schema: ProgramSchema
  /**
   * Generated user defined function (UDF) stubs Rust code: stubs.rs
   */
  udf_stubs?: string
}

/**
 * Program information included in the pipeline configuration.
 */
export type ProgramIr = {
  /**
   * The MIR of the program.
   */
  mir: {
    [key: string]: MirNode
  }
  program_schema: ProgramSchema
}

/**
 * A struct containing the tables (inputs) and views for a program.
 *
 * Parse from the JSON data-type of the DDL generated by the SQL compiler.
 */
export type ProgramSchema = {
  inputs: Array<Relation>
  outputs: Array<Relation>
}

/**
 * Program compilation status.
 */
export type ProgramStatus =
  | 'Pending'
  | 'CompilingSql'
  | 'SqlCompiled'
  | 'CompilingRust'
  | 'Success'
  | 'SqlError'
  | 'RustError'
  | 'SystemError'

export type PropertyValue = {
  key_position: SourcePosition
  value: string
  value_position: SourcePosition
}

export type ProviderAwsCognito = {
  issuer: string
  login_url: string
  logout_url: string
}

export type ProviderGenericOidc = {
  client_id: string
  extra_oidc_scopes: Array<string>
  issuer: string
}

/**
 * Google Pub/Sub input connector configuration.
 */
export type PubSubInputConfig = {
  /**
   * gRPC connection timeout.
   */
  connect_timeout_seconds?: number | null
  /**
   * The content of a Google Cloud credentials JSON file.
   *
   * When this option is specified, the connector will use the provided credentials for
   * authentication.  Otherwise, it will use Application Default Credentials (ADC) configured
   * in the environment where the Feldera service is running.  See
   * [Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)
   * for information on configuring application default credentials.
   *
   * When running Feldera in an environment where ADC are not configured,
   * e.g., a Docker container, use this option to ship Google Cloud credentials from another environment.
   * For example, if you use the
   * [`gcloud auth application-default login`](https://cloud.google.com/pubsub/docs/authentication#client-libs)
   * command for authentication in your local development environment, ADC are stored in the
   * `.config/gcloud/application_default_credentials.json` file in your home directory.
   */
  credentials?: string | null
  /**
   * Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)
   * instead of the production service, e.g., 'localhost:8681'.
   */
  emulator?: string | null
  /**
   * Override the default service endpoint 'pubsub.googleapis.com'
   */
  endpoint?: string | null
  /**
   * gRPC channel pool size.
   */
  pool_size?: number | null
  /**
   * Google Cloud project_id.
   *
   * When not specified, the connector will use the project id associated
   * with the authenticated account.
   */
  project_id?: string | null
  /**
   * Reset subscription's backlog to a given snapshot on startup,
   * using the Pub/Sub `Seek` API.
   *
   * This option is mutually exclusive with the `timestamp` option.
   */
  snapshot?: string | null
  /**
   * Subscription name.
   */
  subscription: string
  /**
   * gRPC request timeout.
   */
  timeout_seconds?: number | null
  /**
   * Reset subscription's backlog to a given timestamp on startup,
   * using the Pub/Sub `Seek` API.
   *
   * The value of this option is an ISO 8601-encoded UTC time, e.g., "2024-08-17T16:39:57-08:00".
   *
   * This option is mutually exclusive with the `snapshot` option.
   */
  timestamp?: string | null
}

/**
 * Redis output connector configuration.
 */
export type RedisOutputConfig = {
  /**
   * The URL format: `redis://[<username>][:<password>@]<hostname>[:port][/[<db>][?protocol=<protocol>]]`
   * This is parsed by the [redis](https://docs.rs/redis/latest/redis/#connection-parameters) crate.
   */
  connection_string: string
  /**
   * Separator used to join multiple components into a single key.
   * ":" by default.
   */
  key_separator?: string
}

export type Rel = {
  aggs?: Array<unknown> | null
  all?: boolean | null
  condition?: Condition | null
  exprs?: Array<Operand> | null
  fields?: Array<string> | null
  group?: Array<number> | null
  id: number
  inputs?: Array<number>
  joinType?: string | null
  relOp: string
  /**
   * This is a vector where the elements concatenated form a fully qualified table name.
   *
   * e.g., usually is of the form `[$namespace, $table] / [schema, table]`
   */
  table?: Array<string> | null
  '[key: string]': (unknown | boolean | Operand | string | number) | undefined
}

/**
 * A SQL table or view. It has a name and a list of fields.
 *
 * Matches the Calcite JSON format.
 */
export type Relation = SqlIdentifier & {
  fields: Array<Field>
  materialized?: boolean
  properties?: {
    [key: string]: PropertyValue
  }
}

export type ReplayPolicy = 'Instant' | 'Original'

export type ResourceConfig = {
  /**
   * The maximum number of CPU cores to reserve
   * for an instance of this pipeline
   */
  cpu_cores_max?: number | null
  /**
   * The minimum number of CPU cores to reserve
   * for an instance of this pipeline
   */
  cpu_cores_min?: number | null
  /**
   * The maximum memory in Megabytes to reserve
   * for an instance of this pipeline
   */
  memory_mb_max?: number | null
  /**
   * The minimum memory in Megabytes to reserve
   * for an instance of this pipeline
   */
  memory_mb_min?: number | null
  /**
   * Kubernetes namespace to use for an instance of this pipeline.
   * The namespace determines the scope of names for resources created
   * for the pipeline.
   * If not set, the pipeline will be deployed in the same namespace
   * as the control-plane.
   */
  namespace?: string | null
  /**
   * Kubernetes service account name to use for an instance of this pipeline.
   * The account determines permissions and access controls.
   */
  service_account_name?: string | null
  /**
   * Storage class to use for an instance of this pipeline.
   * The class determines storage performance such as IOPS and throughput.
   */
  storage_class?: string | null
  /**
   * The total storage in Megabytes to reserve
   * for an instance of this pipeline
   */
  storage_mb_max?: number | null
}

export type ResourcesDesiredStatus = 'Stopped' | 'Provisioned'

/**
 * Pipeline resources status.
 *
 * ```text
 * /start (early start failed)
 * ┌───────────────────┐
 * │                   ▼
 * Stopped ◄────────── Stopping
 * /start │                   ▲
 * │                   │ /stop?force=true
 * │                   │ OR: timeout (from Provisioning)
 * ▼                   │ OR: fatal runtime or resource error
 * ⌛Provisioning ────────────│ OR: runtime status is Suspended
 * │                   │
 * │                   │
 * ▼                   │
 * Provisioned ─────────────┘
 * ```
 *
 * ### Desired and actual status
 *
 * We use the desired state model to manage the lifecycle of a pipeline. In this model, the
 * pipeline has two status attributes associated with it: the **desired** status, which represents
 * what the user would like the pipeline to do, and the **current** status, which represents the
 * actual (last observed) status of the pipeline. The pipeline runner service continuously monitors
 * the desired status field to decide where to steer the pipeline towards.
 *
 * There are two desired statuses:
 * - `Provisioned` (set by invoking `/start`)
 * - `Stopped` (set by invoking `/stop?force=true`)
 *
 * The user can monitor the current status of the pipeline via the `GET /v0/pipelines/{name}`
 * endpoint. In a typical scenario, the user first sets the desired status, e.g., by invoking the
 * `/start` endpoint, and then polls the `GET /v0/pipelines/{name}` endpoint to monitor the actual
 * status of the pipeline until its `deployment_resources_status` attribute changes to
 * `Provisioned` indicating that the pipeline has been successfully provisioned, or `Stopped` with
 * `deployment_error` being set.
 */
export type ResourcesStatus = 'Stopped' | 'Provisioning' | 'Provisioned' | 'Stopping'

/**
 * Iceberg REST catalog config.
 */
export type RestCatalogConfig = {
  /**
   * Logical name of target resource or service.
   */
  'rest.audience'?: string | null
  /**
   * Credential to use for OAuth2 credential flow when initializing the catalog.
   *
   * A key and secret pair separated by ":" (key is optional).
   */
  'rest.credential'?: string | null
  /**
   * Additional HTTP request headers added to each catalog REST API call.
   */
  'rest.headers'?: Array<Array<string>> | null
  /**
   * Authentication URL to use for client credentials authentication (default: uri + 'v1/oauth/tokens')
   */
  'rest.oauth2-server-uri'?: string | null
  /**
   * Customize table storage paths.
   *
   * When combined with the `warehouse` property, the prefix determines
   * how table data is organized within the storage.
   */
  'rest.prefix'?: string | null
  /**
   * URI for the target resource or service.
   */
  'rest.resource'?: string | null
  'rest.scope'?: string | null
  /**
   * Bearer token value to use for `Authorization` header.
   */
  'rest.token'?: string | null
  /**
   * URI identifying the REST catalog server.
   */
  'rest.uri'?: string | null
  /**
   * The default location for managed tables created by the catalog.
   */
  'rest.warehouse'?: string | null
}

/**
 * Configuration for generating random data for a field of a table.
 */
export type RngFieldSettings = {
  /**
   * The frequency rank exponent for the Zipf distribution.
   *
   * - This value is only used if the strategy is set to `Zipf`.
   * - The default value is 1.0.
   */
  e?: number
  /**
   * Specifies the values that the generator should produce in case the field is a struct type.
   */
  fields?: {
    [key: string]: RngFieldSettings
  } | null
  key?: RngFieldSettings | null
  /**
   * Percentage of records where this field should be set to NULL.
   *
   * If not set, the generator will produce only records with non-NULL values.
   * If set to `1..=100`, the generator will produce records with NULL values with the specified percentage.
   */
  null_percentage?: number | null
  /**
   * An optional, exclusive range [a, b) to limit the range of values the generator should produce.
   *
   * - For integer/floating point types specifies min/max values as an integer.
   * If not set, the generator will produce values for the entire range of the type for number types.
   * - For string/binary types specifies min/max length as an integer, values are required to be >=0.
   * If not set, a range of [0, 25) is used by default.
   * - For timestamp types specifies the min/max as two strings in the RFC 3339 format
   * (e.g., ["2021-01-01T00:00:00Z", "2022-01-02T00:00:00Z"]).
   * Alternatively, the range values can be specified as a number of non-leap
   * milliseconds since January 1, 1970 0:00:00.000 UTC (aka “UNIX timestamp”).
   * If not set, a range of ["1970-01-01T00:00:00Z", "2100-01-01T00:00:00Z") or [0, 4102444800000)
   * is used by default.
   * - For time types specifies the min/max as two strings in the "HH:MM:SS" format.
   * Alternatively, the range values can be specified in milliseconds as two positive integers.
   * If not set, the range is 24h.
   * - For date types, the min/max range is specified as two strings in the "YYYY-MM-DD" format.
   * Alternatively, two integers that represent number of days since January 1, 1970 can be used.
   * If not set, a range of ["1970-01-01", "2100-01-01") or [0, 54787) is used by default.
   * - For array types specifies the min/max number of elements as an integer.
   * If not set, a range of [0, 5) is used by default. Range values are required to be >=0.
   * - For map types specifies the min/max number of key-value pairs as an integer.
   * If not set, a range of [0, 5) is used by default.
   * - For struct/boolean/null types `range` is ignored.
   */
  range?: {
    [key: string]: unknown
  }
  /**
   * A scale factor to apply a multiplier to the generated value.
   *
   * - For integer/floating point types, the value is multiplied by the scale factor.
   * - For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.
   * - For time types, the generated value (milliseconds) is multiplied by the scale factor.
   * - For date types, the generated value (days) is multiplied by the scale factor.
   * - For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.
   *
   * - If `values` is specified, the scale factor is ignored.
   * - If `range` is specified and the range is required to be positive (struct, map, array etc.)
   * the scale factor is required to be positive too.
   *
   * The default scale factor is 1.
   */
  scale?: number
  strategy?: DatagenStrategy
  value?: RngFieldSettings | null
  /**
   * An optional set of values the generator will pick from.
   *
   * If set, the generator will pick values from the specified set.
   * If not set, the generator will produce values according to the specified range.
   * If set to an empty set, the generator will produce NULL values.
   * If set to a single value, the generator will produce only that value.
   *
   * Note that `range` is ignored if `values` is set.
   */
  values?: Array<{
    [key: string]: unknown
  }> | null
}

/**
 * Global pipeline configuration settings. This is the publicly
 * exposed type for users to configure pipelines.
 */
export type RuntimeConfig = {
  /**
   * Deprecated: setting this true or false does not have an effect anymore.
   */
  checkpoint_during_suspend?: boolean
  /**
   * Real-time clock resolution in microseconds.
   *
   * This parameter controls the execution of queries that use the `NOW()` function.  The output of such
   * queries depends on the real-time clock and can change over time without any external
   * inputs.  If the query uses `NOW()`, the pipeline will update the clock value and trigger incremental
   * recomputation at most each `clock_resolution_usecs` microseconds.  If the query does not use
   * `NOW()`, then clock value updates are suppressed and the pipeline ignores this setting.
   *
   * It is set to 1 second (1,000,000 microseconds) by default.
   */
  clock_resolution_usecs?: number | null
  /**
   * Enable CPU profiler.
   *
   * The default value is `true`.
   */
  cpu_profiler?: boolean
  /**
   * Optional settings for tweaking Feldera internals.
   *
   * The available key-value pairs change from one version of Feldera to
   * another, so users should not depend on particular settings being
   * available, or on their behavior.
   */
  dev_tweaks?: {
    [key: string]: unknown
  }
  fault_tolerance?: FtConfig
  /**
   * Sets the number of available runtime threads for the http server.
   *
   * In most cases, this does not need to be set explicitly and
   * the default is sufficient. Can be increased in case the
   * pipeline HTTP API operations are a bottleneck.
   *
   * If not specified, the default is set to `workers`.
   */
  http_workers?: number | null
  /**
   * Specification of additional (sidecar) containers.
   */
  init_containers?: unknown
  /**
   * Sets the number of available runtime threads for async IO tasks.
   *
   * This affects some networking and file I/O operations
   * especially adapters and ad-hoc queries.
   *
   * In most cases, this does not need to be set explicitly and
   * the default is sufficient. Can be increased in case
   * ingress, egress or ad-hoc queries are a bottleneck.
   *
   * If not specified, the default is set to `workers`.
   */
  io_workers?: number | null
  /**
   * Log filtering directives.
   *
   * If set to a valid [tracing-subscriber] filter, this controls the log
   * messages emitted by the pipeline process.  Otherwise, or if the filter
   * has invalid syntax, messages at "info" severity and higher are written
   * to the log and all others are discarded.
   *
   * [tracing-subscriber]: https://docs.rs/tracing-subscriber/latest/tracing_subscriber/filter/struct.EnvFilter.html#directives
   */
  logging?: string | null
  /**
   * Maximal delay in microseconds to wait for `min_batch_size_records` to
   * get buffered by the controller, defaults to 0.
   */
  max_buffering_delay_usecs?: number
  /**
   * The maximum number of connectors initialized in parallel during pipeline
   * startup.
   *
   * At startup, the pipeline must initialize all of its input and output connectors.
   * Depending on the number and types of connectors, this can take a long time.
   * To accelerate the process, multiple connectors are initialized concurrently.
   * This option controls the maximum number of connectors that can be initialized
   * in parallel.
   *
   * The default is 10.
   */
  max_parallel_connector_init?: number | null
  /**
   * Minimal input batch size.
   *
   * The controller delays pushing input records to the circuit until at
   * least `min_batch_size_records` records have been received (total
   * across all endpoints) or `max_buffering_delay_usecs` microseconds
   * have passed since at least one input records has been buffered.
   * Defaults to 0.
   */
  min_batch_size_records?: number
  /**
   * Optionally, a list of CPU numbers for CPUs to which the pipeline may pin
   * its worker threads.  Specify at least twice as many CPU numbers as
   * workers.  CPUs are generally numbered starting from 0.  The pipeline
   * might not be able to honor CPU pinning requests.
   *
   * CPU pinning can make pipelines run faster and perform more consistently,
   * as long as different pipelines running on the same machine are pinned to
   * different CPUs.
   */
  pin_cpus?: Array<number>
  /**
   * Timeout in seconds for the `Provisioning` phase of the pipeline.
   * Setting this value will override the default of the runner.
   */
  provisioning_timeout_secs?: number | null
  resources?: ResourceConfig
  storage?: StorageOptions | null
  /**
   * Enable pipeline tracing.
   */
  tracing?: boolean
  /**
   * Jaeger tracing endpoint to send tracing information to.
   */
  tracing_endpoint_jaeger?: string
  /**
   * Number of DBSP worker threads.
   *
   * Each DBSP "foreground" worker thread is paired with a "background"
   * thread for LSM merging, making the total number of threads twice the
   * specified number.
   *
   * The typical sweet spot for the number of workers is between 4 and 16.
   * Each worker increases overall memory consumption for data structures
   * used during a step.
   */
  workers?: number
}

export type RuntimeDesiredStatus = 'Unavailable' | 'Standby' | 'Paused' | 'Running' | 'Suspended'

/**
 * Runtime status of the pipeline.
 *
 * Of the statuses, only `Unavailable` is determined by the runner. All other statuses are
 * determined by the pipeline and taken over by the runner.
 */
export type RuntimeStatus =
  | 'Unavailable'
  | 'Standby'
  | 'Initializing'
  | 'AwaitingApproval'
  | 'Bootstrapping'
  | 'Replaying'
  | 'Paused'
  | 'Running'
  | 'Suspended'

/**
 * Rust compilation information.
 */
export type RustCompilationInfo = {
  /**
   * Exit code of the `cargo` compilation command.
   */
  exit_code: number
  /**
   * Output printed to stderr by the `cargo` compilation command.
   */
  stderr: string
  /**
   * Output printed to stdout by the `cargo` compilation command.
   */
  stdout: string
}

/**
 * Configuration for reading data from AWS S3.
 */
export type S3InputConfig = {
  /**
   * AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.
   */
  aws_access_key_id?: string | null
  /**
   * Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.
   */
  aws_secret_access_key?: string | null
  /**
   * S3 bucket name to access.
   */
  bucket_name: string
  /**
   * The endpoint URL used to communicate with this service. Can be used to make this connector
   * talk to non-AWS services with an S3 API.
   */
  endpoint_url?: string | null
  /**
   * Read a single object specified by a key.
   */
  key?: string | null
  /**
   * Controls the number of S3 objects fetched in parallel.
   *
   * Increasing this value can improve throughput by enabling greater concurrency.
   * However, higher concurrency may lead to timeouts or increased memory usage due to in-memory buffering.
   *
   * Recommended range: 1–10. Default: 8.
   */
  max_concurrent_fetches?: number
  /**
   * Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI.
   */
  no_sign_request?: boolean
  /**
   * Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.
   */
  prefix?: string | null
  /**
   * AWS region.
   */
  region: string
}

/**
 * One sample of time-series data.
 */
export type SampleStatistics = {
  /**
   * Memory usage in bytes.
   */
  m: number
  /**
   * Records processed.
   */
  r: number
  /**
   * Storage usage in bytes.
   */
  s: number
  /**
   * Sample time.
   */
  t: string
}

export type ServiceStatus = {
  checked_at: string
  healthy: boolean
  message: string
  unchanged_since: string
}

export type SessionInfo = {
  tenant_id: TenantId
  /**
   * Current user's tenant name
   */
  tenant_name: string
}

export type SourcePosition = {
  end_column: number
  end_line_number: number
  start_column: number
  start_line_number: number
}

/**
 * SQL compilation information.
 */
export type SqlCompilationInfo = {
  /**
   * Exit code of the SQL compiler.
   */
  exit_code: number
  /**
   * Messages (warnings and errors) generated by the SQL compiler.
   */
  messages: Array<SqlCompilerMessage>
}

/**
 * A SQL compiler error.
 *
 * The SQL compiler returns a list of errors in the following JSON format if
 * it's invoked with the `-je` option.
 *
 * ```ignore
 * [ {
 * "start_line_number" : 2,
 * "start_column" : 4,
 * "end_line_number" : 2,
 * "end_column" : 8,
 * "warning" : false,
 * "error_type" : "PRIMARY KEY cannot be nullable",
 * "message" : "PRIMARY KEY column 'C' has type INTEGER, which is nullable",
 * "snippet" : "    2|   c INT PRIMARY KEY\n         ^^^^^\n    3|);\n"
 * } ]
 * ```
 */
export type SqlCompilerMessage = {
  end_column: number
  end_line_number: number
  error_type: string
  message: string
  snippet?: string | null
  start_column: number
  start_line_number: number
  warning: boolean
}

/**
 * An SQL identifier.
 *
 * This struct is used to represent SQL identifiers in a canonical form.
 * We store table names or field names as identifiers in the schema.
 */
export type SqlIdentifier = {
  case_sensitive: boolean
  name: string
}

/**
 * The available SQL types as specified in `CREATE` statements.
 */
export type SqlType =
  | 'Boolean'
  | 'TinyInt'
  | 'SmallInt'
  | 'Int'
  | 'BigInt'
  | 'UTinyInt'
  | 'USmallInt'
  | 'UInt'
  | 'UBigInt'
  | 'Real'
  | 'Double'
  | 'Decimal'
  | 'Char'
  | 'Varchar'
  | 'Binary'
  | 'Varbinary'
  | 'Time'
  | 'Date'
  | 'Timestamp'
  | {
      Interval: IntervalUnit
    }
  | 'Array'
  | 'Struct'
  | 'Map'
  | 'Null'
  | 'Uuid'
  | 'Variant'

export type StartFromCheckpoint = 'latest' | string | null

/**
 * Response to a `/start_transaction` request.
 */
export type StartTransactionResponse = {
  transaction_id: number
}

/**
 * Backend storage configuration.
 */
export type StorageBackendConfig =
  | {
      name: 'default'
    }
  | {
      config: FileBackendConfig
      name: 'file'
    }
  | {
      config: ObjectStorageConfig
      name: 'object'
    }

export type name = 'default'

/**
 * How to cache access to storage within a Feldera pipeline.
 */
export type StorageCacheConfig = 'page_cache' | 'feldera_cache'

/**
 * Storage compression algorithm.
 */
export type StorageCompression = 'default' | 'none' | 'snappy'

/**
 * Configuration for persistent storage in a [`PipelineConfig`].
 */
export type StorageConfig = {
  cache?: StorageCacheConfig
  /**
   * A directory to keep pipeline state, as a path on the filesystem of the
   * machine or container where the pipeline will run.
   *
   * When storage is enabled, this directory stores the data for
   * [StorageBackendConfig::Default].
   *
   * When fault tolerance is enabled, this directory stores checkpoints and
   * the log.
   */
  path: string
}

/**
 * Storage configuration for a pipeline.
 */
export type StorageOptions = {
  backend?: StorageBackendConfig
  /**
   * The maximum size of the in-memory storage cache, in MiB.
   *
   * If set, the specified cache size is spread across all the foreground and
   * background threads. If unset, each foreground or background thread cache
   * is limited to 256 MiB.
   */
  cache_mib?: number | null
  compression?: StorageCompression
  /**
   * For a batch of data passed through the pipeline during a single step,
   * the minimum estimated number of bytes to write it to storage.
   *
   * This is provided for debugging and fine-tuning and should ordinarily be
   * left unset.  A value of 0 will write even empty batches to storage, and
   * nonzero values provide a threshold.  `usize::MAX`, the default,
   * effectively disables storage for such batches.  If it is set to another
   * value, it should ordinarily be greater than or equal to
   * `min_storage_bytes`.
   */
  min_step_storage_bytes?: number | null
  /**
   * For a batch of data maintained as part of a persistent index during a
   * pipeline run, the minimum estimated number of bytes to write it to
   * storage.
   *
   * This is provided for debugging and fine-tuning and should ordinarily be
   * left unset.
   *
   * A value of 0 will write even empty batches to storage, and nonzero
   * values provide a threshold.  `usize::MAX` would effectively disable
   * storage for such batches.  The default is 1,048,576 (1 MiB).
   */
  min_storage_bytes?: number | null
}

/**
 * Storage status.
 *
 * The storage status can only transition when the resources status is `Stopped`.
 *
 * ```text
 * Cleared ───┐
 * ▲       │
 * /clear │       │
 * │       │
 * Clearing   │
 * ▲       │
 * │       │
 * InUse ◄───┘
 * ```
 */
export type StorageStatus = 'Cleared' | 'InUse' | 'Clearing'

export type SyncConfig = {
  /**
   * The access key used to authenticate with the storage provider.
   *
   * If not provided, rclone will fall back to environment-based credentials, such as
   * `RCLONE_S3_ACCESS_KEY_ID`. In Kubernetes environments using IRSA (IAM Roles for Service Accounts),
   * this can be left empty to allow automatic authentication via the pod's service account.
   */
  access_key?: string | null
  /**
   * The name of the storage bucket.
   *
   * This may include a path to a folder inside the bucket (e.g., `my-bucket/data`).
   */
  bucket: string
  /**
   * The number of checkers to run in parallel.
   * Default: 20
   */
  checkers?: number | null
  /**
   * The endpoint URL for the storage service.
   *
   * This is typically required for custom or local S3-compatible storage providers like MinIO.
   * Example: `http://localhost:9000`
   *
   * Relevant rclone config key: [`endpoint`](https://rclone.org/s3/#s3-endpoint)
   */
  endpoint?: string | null
  /**
   * When true, the pipeline will fail to initialize if fetching the
   * specified checkpoint fails (missing, download error).
   * When false, the pipeline will start from scratch instead.
   *
   * False by default.
   */
  fail_if_no_checkpoint?: boolean
  /**
   * Extra flags to pass to `rclone`.
   *
   * WARNING: Supplying incorrect or conflicting flags can break `rclone`.
   * Use with caution.
   *
   * Refer to the docs to see the supported flags:
   * - [Global flags](https://rclone.org/flags/)
   * - [S3 specific flags](https://rclone.org/s3/)
   */
  flags?: Array<string> | null
  /**
   * Set to skip post copy check of checksums, and only check the file sizes.
   * This can significantly improve the throughput.
   * Defualt: false
   */
  ignore_checksum?: boolean | null
  /**
   * Use multi-thread download for files above this size.
   * Format: `[size][Suffix]` (Example: 1G, 500M)
   * Supported suffixes: k|M|G|T
   * Default: 100M
   */
  multi_thread_cutoff?: string | null
  /**
   * Number of streams to use for multi-thread downloads.
   * Default: 10
   */
  multi_thread_streams?: number | null
  /**
   * The name of the cloud storage provider (e.g., `"AWS"`, `"Minio"`).
   *
   * Used for provider-specific behavior in rclone.
   * If omitted, defaults to `"Other"`.
   *
   * See [rclone S3 provider documentation](https://rclone.org/s3/#s3-provider)
   */
  provider?: string | null
  /**
   * The interval (in seconds) between each attempt to fetch the latest
   * checkpoint from object store while in standby mode.
   *
   * Applies only when `start_from_checkpoint` is set to `latest`.
   *
   * Default: 10 seconds
   */
  pull_interval?: number
  /**
   * The region that this bucket is in.
   *
   * Leave empty for Minio or the default region (`us-east-1` for AWS).
   */
  region?: string | null
  /**
   * The minimum age (in days) a checkpoint must reach before it becomes
   * eligible for deletion. All younger checkpoints will be preserved.
   *
   * Default: 30
   */
  retention_min_age?: number
  /**
   * The minimum number of checkpoints to retain in object store.
   * No checkpoints will be deleted if the total count is below this threshold.
   *
   * Default: 10
   */
  retention_min_count?: number
  /**
   * The secret key used together with the access key for authentication.
   *
   * If not provided, rclone will fall back to environment-based credentials, such as
   * `RCLONE_S3_SECRET_ACCESS_KEY`. In Kubernetes environments using IRSA (IAM Roles for Service Accounts),
   * this can be left empty to allow automatic authentication via the pod's service account.
   */
  secret_key?: string | null
  /**
   * When `true`, the pipeline starts in **standby** mode; processing doesn't
   * start until activation (`POST /activate`).
   * If this pipeline was previously activated and the storage has not been
   * cleared, the pipeline will auto activate, no newer checkpoints will be
   * fetched.
   *
   * Standby behavior depends on `start_from_checkpoint`:
   * - If `latest`, pipeline continuously fetches the latest available
   * checkpoint until activated.
   * - If checkpoint UUID, pipeline fetches this checkpoint once and waits
   * in standby until activated.
   *
   * Default: `false`
   */
  standby?: boolean
  start_from_checkpoint?: StartFromCheckpoint | null
  /**
   * The number of file transfers to run in parallel.
   * Default: 20
   */
  transfers?: number | null
  /**
   * The number of chunks of the same file that are uploaded for multipart uploads.
   * Default: 10
   */
  upload_concurrency?: number | null
}

export type TenantId = string

/**
 * Time series to make graphs in the web console easier.
 */
export type TimeSeries = {
  /**
   * Current time as of the creation of the structure.
   */
  now: string
  /**
   * Time series.
   *
   * These report 60 seconds of samples, one per second.
   */
  samples: Array<SampleStatistics>
}

/**
 * Transport-specific endpoint configuration passed to
 * `crate::OutputTransport::new_endpoint`
 * and `crate::InputTransport::new_endpoint`.
 */
export type TransportConfig =
  | {
      config: FileInputConfig
      name: 'file_input'
    }
  | {
      config: FileOutputConfig
      name: 'file_output'
    }
  | {
      config: NatsInputConfig
      name: 'nats_input'
    }
  | {
      config: KafkaInputConfig
      name: 'kafka_input'
    }
  | {
      config: KafkaOutputConfig
      name: 'kafka_output'
    }
  | {
      config: PubSubInputConfig
      name: 'pub_sub_input'
    }
  | {
      config: UrlInputConfig
      name: 'url_input'
    }
  | {
      config: S3InputConfig
      name: 's3_input'
    }
  | {
      config: DeltaTableReaderConfig
      name: 'delta_table_input'
    }
  | {
      config: DeltaTableWriterConfig
      name: 'delta_table_output'
    }
  | {
      config: RedisOutputConfig
      name: 'redis_output'
    }
  | {
      config: IcebergReaderConfig
      name: 'iceberg_input'
    }
  | {
      config: PostgresReaderConfig
      name: 'postgres_input'
    }
  | {
      config: PostgresWriterConfig
      name: 'postgres_output'
    }
  | {
      config: DatagenInputConfig
      name: 'datagen'
    }
  | {
      config: NexmarkInputConfig
      name: 'nexmark'
    }
  | {
      config: HttpInputConfig
      name: 'http_input'
    }
  | {
      name: 'http_output'
    }
  | {
      config: AdHocInputConfig
      name: 'ad_hoc_input'
    }
  | {
      config: ClockConfig
      name: 'clock_input'
    }

export type name2 = 'file_input'

export type UpdateInformation = {
  /**
   * URL that navigates the user to instructions on how to update their deployment's version
   */
  instructions_url: string
  /**
   * Whether the current version matches the latest version
   */
  is_latest_version: boolean
  /**
   * Latest version corresponding to the edition
   */
  latest_version: string
  remind_schedule: DisplaySchedule
}

/**
 * Configuration for reading data from an HTTP or HTTPS URL with
 * `UrlInputTransport`.
 */
export type UrlInputConfig = {
  /**
   * URL.
   */
  path: string
  /**
   * Timeout before disconnection when paused, in seconds.
   *
   * If the pipeline is paused, or if the input adapter reads data faster
   * than the pipeline can process it, then the controller will pause the
   * input adapter. If the input adapter stays paused longer than this
   * timeout, it will drop the network connection to the server. It will
   * automatically reconnect when the input adapter starts running again.
   */
  pause_timeout?: number
}

export type UserAndPassword = {
  password: string
  user: string
}

/**
 * Version number.
 */
export type Version = number

export type GetConfigAuthenticationResponse = AuthProvider

export type GetConfigAuthenticationError = ErrorResponse

export type ListApiKeysResponse = Array<ApiKeyDescr>

export type ListApiKeysError = ErrorResponse

export type PostApiKeyData = {
  body: NewApiKeyRequest
}

export type PostApiKeyResponse = NewApiKeyResponse

export type PostApiKeyError = ErrorResponse

export type GetApiKeyData = {
  path: {
    /**
     * Unique API key name
     */
    api_key_name: string
  }
}

export type GetApiKeyResponse = ApiKeyDescr

export type GetApiKeyError = ErrorResponse

export type DeleteApiKeyData = {
  path: {
    /**
     * Unique API key name
     */
    api_key_name: string
  }
}

export type DeleteApiKeyResponse = unknown

export type DeleteApiKeyError = ErrorResponse

export type GetHealthResponse = HealthStatus

export type GetHealthError = HealthStatus

export type GetConfigResponse = Configuration

export type GetConfigError = ErrorResponse

export type GetConfigDemosResponse = Array<Demo>

export type GetConfigDemosError = ErrorResponse

export type GetConfigSessionResponse = SessionInfo

export type GetConfigSessionError = ErrorResponse

export type GetMetricsResponse = Blob | File

export type GetMetricsError = unknown

export type ListPipelinesData = {
  query?: {
    /**
     * The `selector` parameter limits which fields are returned for a pipeline.
     * Limiting which fields is particularly handy for instance when frequently
     * monitoring over low bandwidth connections while being only interested
     * in pipeline status.
     */
    selector?: PipelineFieldSelector
  }
}

export type ListPipelinesResponse = Array<PipelineSelectedInfo>

export type ListPipelinesError = ErrorResponse

export type PostPipelineData = {
  body: PostPutPipeline
}

export type PostPipelineResponse = PipelineInfo

export type PostPipelineError = ErrorResponse

export type GetPipelineData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query?: {
    /**
     * The `selector` parameter limits which fields are returned for a pipeline.
     * Limiting which fields is particularly handy for instance when frequently
     * monitoring over low bandwidth connections while being only interested
     * in pipeline status.
     */
    selector?: PipelineFieldSelector
  }
}

export type GetPipelineResponse = PipelineSelectedInfo

export type GetPipelineError = ErrorResponse

export type PutPipelineData = {
  body: PostPutPipeline
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PutPipelineResponse = PipelineInfo

export type PutPipelineError = ErrorResponse

export type DeletePipelineData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type DeletePipelineResponse = unknown

export type DeletePipelineError = ErrorResponse

export type PatchPipelineData = {
  body: PatchPipeline
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PatchPipelineResponse = PipelineInfo

export type PatchPipelineError = ErrorResponse

export type PostPipelineActivateData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query?: {
    initial?: string
  }
}

export type PostPipelineActivateResponse = CheckpointResponse

export type PostPipelineActivateError = ErrorResponse

export type PostPipelineApproveData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PostPipelineApproveResponse = CheckpointResponse

export type PostPipelineApproveError = ErrorResponse

export type CheckpointPipelineData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type CheckpointPipelineResponse = CheckpointResponse

export type CheckpointPipelineError = ErrorResponse

export type SyncCheckpointData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type SyncCheckpointResponse = CheckpointResponse

export type SyncCheckpointError = ErrorResponse

export type GetCheckpointSyncStatusData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetCheckpointSyncStatusResponse = CheckpointStatus

export type GetCheckpointSyncStatusError = ErrorResponse

export type GetCheckpointStatusData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetCheckpointStatusResponse = CheckpointStatus

export type GetCheckpointStatusError = ErrorResponse

export type GetPipelineCircuitProfileData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineCircuitProfileResponse = {
  [key: string]: unknown
}

export type GetPipelineCircuitProfileError = ErrorResponse

export type PostPipelineClearData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PostPipelineClearResponse = unknown

export type PostPipelineClearError = ErrorResponse

export type CommitTransactionData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type CommitTransactionResponse = unknown

export type CommitTransactionError = ErrorResponse

export type CompletionStatusData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query: {
    /**
     * Completion token returned by the '/ingress' or '/completion_status' endpoint.
     */
    token: string
  }
}

export type CompletionStatusResponse2 = CompletionStatusResponse

export type CompletionStatusError = ErrorResponse

export type HttpOutputData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * SQL table name. Unquoted SQL names have to be capitalized. Quoted SQL names have to exactly match the case from the SQL program.
     */
    table_name: string
  }
  query: {
    /**
     * Set to `true` to group updates in this stream into JSON arrays (used in conjunction with `format=json`). The default value is `false`
     */
    array?: boolean | null
    /**
     * Apply backpressure on the pipeline when the HTTP client cannot receive data fast enough.
     * When this flag is set to false (the default), the HTTP connector drops data chunks if the client is not keeping up with its output.  This prevents a slow HTTP client from slowing down the entire pipeline.
     * When the flag is set to true, the connector waits for the client to receive each chunk and blocks the pipeline if the client cannot keep up.
     */
    backpressure?: boolean | null
    /**
     * Output data format, e.g., 'csv' or 'json'.
     */
    format: string
  }
}

export type HttpOutputResponse = Chunk

export type HttpOutputError = ErrorResponse

export type GetPipelineHeapProfileData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineHeapProfileResponse = Blob | File

export type GetPipelineHeapProfileError = ErrorResponse

export type HttpInputData = {
  /**
   * Input data in the specified format
   */
  body: string
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * SQL table name. Unquoted SQL names have to be capitalized. Quoted SQL names have to exactly match the case from the SQL program.
     */
    table_name: string
  }
  query: {
    /**
     * Set to `true` if updates in this stream are packaged into JSON arrays (used in conjunction with `format=json`). The default values is `false`.
     */
    array?: boolean | null
    /**
     * When `true`, push data to the pipeline even if the pipeline is paused. The default value is `false`
     */
    force: boolean
    /**
     * Input data format, e.g., 'csv' or 'json'.
     */
    format: string
    /**
     * JSON data change event format (used in conjunction with `format=json`).  The default value is 'insert_delete'.
     */
    update_format?: JsonUpdateFormat | null
  }
}

export type HttpInputResponse = CompletionTokenResponse

export type HttpInputError = ErrorResponse

export type GetPipelineLogsData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineLogsResponse = Blob | File

export type GetPipelineLogsError = ErrorResponse

export type GetPipelineMetricsData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query?: {
    format?: MetricsFormat
  }
}

export type GetPipelineMetricsResponse = Blob | File

export type GetPipelineMetricsError = ErrorResponse

export type PostPipelinePauseData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PostPipelinePauseResponse = unknown

export type PostPipelinePauseError = ErrorResponse

export type PipelineAdhocSqlData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query: {
    /**
     * Input data format, e.g., 'text', 'json' or 'parquet'
     */
    format: AdHocResultFormat
    /**
     * SQL query to execute
     */
    sql: string
  }
}

export type PipelineAdhocSqlResponse = Blob | File

export type PipelineAdhocSqlError = ErrorResponse

export type PostPipelineResumeData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PostPipelineResumeResponse = unknown

export type PostPipelineResumeError = ErrorResponse

export type PostPipelineStartData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query?: {
    bootstrap_policy?: BootstrapPolicy
    /**
     * The `initial` parameter determines whether to after provisioning the pipeline make it
     * become `standby`, `paused` or `running` (only valid values).
     */
    initial?: string
  }
}

export type PostPipelineStartResponse = unknown

export type PostPipelineStartError = ErrorResponse

export type StartTransactionData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type StartTransactionResponse2 = StartTransactionResponse

export type StartTransactionError = ErrorResponse

export type GetPipelineStatsData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineStatsResponse = {
  [key: string]: unknown
}

export type GetPipelineStatsError = ErrorResponse

export type PostPipelineStopData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query?: {
    /**
     * The `force` parameter determines whether to immediately deprovision the pipeline compute
     * resources (`force=true`) or first attempt to atomically checkpoint before doing so
     * (`force=false`, which is the default).
     */
    force?: boolean
  }
}

export type PostPipelineStopResponse = unknown

export type PostPipelineStopError = ErrorResponse

export type GetPipelineSupportBundleData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query?: {
    /**
     * Whether to collect circuit profile data (default: true)
     */
    circuit_profile?: boolean
    /**
     * Whether to collect heap profile data (default: true)
     */
    heap_profile?: boolean
    /**
     * Whether to collect logs data (default: true)
     */
    logs?: boolean
    /**
     * Whether to collect metrics data (default: true)
     */
    metrics?: boolean
    /**
     * Whether to collect pipeline configuration data (default: true)
     */
    pipeline_config?: boolean
    /**
     * Whether to collect stats data (default: true)
     */
    stats?: boolean
    /**
     * Whether to collect system configuration data (default: true)
     */
    system_config?: boolean
  }
}

export type GetPipelineSupportBundleResponse = Blob | File

export type GetPipelineSupportBundleError = ErrorResponse

export type CompletionTokenData = {
  path: {
    /**
     * Unique input connector name
     */
    connector_name: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * SQL table name. Unquoted SQL names have to be capitalized. Quoted SQL names have to exactly match the case from the SQL program.
     */
    table_name: string
  }
}

export type CompletionTokenResponse2 = CompletionTokenResponse

export type CompletionTokenError = ErrorResponse

export type GetPipelineInputConnectorStatusData = {
  path: {
    /**
     * Unique input connector name
     */
    connector_name: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * Unique table name
     */
    table_name: string
  }
}

export type GetPipelineInputConnectorStatusResponse = {
  [key: string]: unknown
}

export type GetPipelineInputConnectorStatusError = ErrorResponse

export type PostPipelineInputConnectorActionData = {
  path: {
    /**
     * Input connector action (one of: start, pause)
     */
    action: string
    /**
     * Unique input connector name
     */
    connector_name: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * Unique table name
     */
    table_name: string
  }
}

export type PostPipelineInputConnectorActionResponse = unknown

export type PostPipelineInputConnectorActionError = ErrorResponse

export type PostPipelineTestingData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query?: {
    set_platform_version?: string | null
  }
}

export type PostPipelineTestingResponse = unknown

export type PostPipelineTestingError = ErrorResponse

export type GetPipelineTimeSeriesData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineTimeSeriesResponse = TimeSeries

export type GetPipelineTimeSeriesError = ErrorResponse

export type GetPipelineTimeSeriesStreamData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineTimeSeriesStreamResponse = string

export type GetPipelineTimeSeriesStreamError = ErrorResponse

export type PostUpdateRuntimeData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PostUpdateRuntimeResponse = PipelineInfo

export type PostUpdateRuntimeError = ErrorResponse

export type GetPipelineOutputConnectorStatusData = {
  path: {
    /**
     * Unique output connector name
     */
    connector_name: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * Unique SQL view name
     */
    view_name: string
  }
}

export type GetPipelineOutputConnectorStatusResponse = {
  [key: string]: unknown
}

export type GetPipelineOutputConnectorStatusError = ErrorResponse

export type $OpenApiTs = {
  '/config/authentication': {
    get: {
      res: {
        /**
         * The response body contains Authentication Provider configuration, or is empty if no auth is configured.
         */
        '200': AuthProvider
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/api_keys': {
    get: {
      res: {
        /**
         * API keys retrieved successfully
         */
        '200': Array<ApiKeyDescr>
        '500': ErrorResponse
      }
    }
    post: {
      req: PostApiKeyData
      res: {
        /**
         * API key created successfully
         */
        '201': NewApiKeyResponse
        /**
         * API key with that name already exists
         */
        '409': ErrorResponse
      }
    }
  }
  '/v0/api_keys/{api_key_name}': {
    get: {
      req: GetApiKeyData
      res: {
        /**
         * API key retrieved successfully
         */
        '200': ApiKeyDescr
        /**
         * API key with that name does not exist
         */
        '404': ErrorResponse
      }
    }
    delete: {
      req: DeleteApiKeyData
      res: {
        /**
         * API key deleted successfully
         */
        '200': unknown
        /**
         * API key with that name does not exist
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/cluster_healthz': {
    get: {
      res: {
        /**
         * All services healthy
         */
        '200': HealthStatus
        /**
         * One or more services unhealthy
         */
        '503': HealthStatus
      }
    }
  }
  '/v0/config': {
    get: {
      res: {
        /**
         * The response body contains basic configuration information about this host.
         */
        '200': Configuration
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/config/demos': {
    get: {
      res: {
        /**
         * List of demos
         */
        '200': Array<Demo>
        /**
         * Failed to read demos from the demos directories
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/config/session': {
    get: {
      res: {
        /**
         * The response body contains current session information including tenant details.
         */
        '200': SessionInfo
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/metrics': {
    get: {
      res: {
        /**
         * Metrics of all running pipelines belonging to this tenant in Prometheus format
         */
        '200': Blob | File
      }
    }
  }
  '/v0/pipelines': {
    get: {
      req: ListPipelinesData
      res: {
        /**
         * List of pipelines retrieved successfully
         */
        '200': Array<PipelineSelectedInfo>
        '500': ErrorResponse
      }
    }
    post: {
      req: PostPipelineData
      res: {
        /**
         * Pipeline successfully created
         */
        '201': PipelineInfo
        '400': ErrorResponse
        /**
         * Cannot create pipeline as the name already exists
         */
        '409': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}': {
    get: {
      req: GetPipelineData
      res: {
        /**
         * Pipeline retrieved successfully
         */
        '200': PipelineSelectedInfo
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
    put: {
      req: PutPipelineData
      res: {
        /**
         * Pipeline successfully updated
         */
        '200': PipelineInfo
        '400': ErrorResponse
        /**
         * Cannot rename pipeline as the new name already exists
         */
        '409': ErrorResponse
        '500': ErrorResponse
      }
    }
    delete: {
      req: DeletePipelineData
      res: {
        /**
         * Pipeline successfully deleted
         */
        '200': unknown
        /**
         * Pipeline must be fully stopped and cleared to be deleted
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
    patch: {
      req: PatchPipelineData
      res: {
        /**
         * Pipeline successfully updated
         */
        '200': PipelineInfo
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        /**
         * Cannot rename pipeline as the name already exists
         */
        '409': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/activate': {
    post: {
      req: PostPipelineActivateData
      res: {
        /**
         * Pipeline activation initiated
         */
        '202': CheckpointResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/approve': {
    post: {
      req: PostPipelineApproveData
      res: {
        /**
         * Pipeline activation initiated
         */
        '202': CheckpointResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/checkpoint': {
    post: {
      req: CheckpointPipelineData
      res: {
        /**
         * Checkpoint initiated
         */
        '200': CheckpointResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/checkpoint/sync': {
    post: {
      req: SyncCheckpointData
      res: {
        /**
         * Checkpoint synced to object store
         */
        '200': CheckpointResponse
        /**
         * No checkpoints found
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/checkpoint/sync_status': {
    get: {
      req: GetCheckpointSyncStatusData
      res: {
        /**
         * Checkpoint sync status retrieved successfully
         */
        '200': CheckpointStatus
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/checkpoint_status': {
    get: {
      req: GetCheckpointStatusData
      res: {
        /**
         * Checkpoint status retrieved successfully
         */
        '200': CheckpointStatus
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/circuit_profile': {
    get: {
      req: GetPipelineCircuitProfileData
      res: {
        /**
         * Circuit performance profile
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/clear': {
    post: {
      req: PostPipelineClearData
      res: {
        /**
         * Action is accepted and is being performed
         */
        '202': unknown
        /**
         * Action could not be performed
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/commit_transaction': {
    post: {
      req: CommitTransactionData
      res: {
        /**
         * Commit operation initiated.
         */
        '200': unknown
        /**
         * Another transaction is already in progress.
         */
        '409': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/completion_status': {
    get: {
      req: CompletionStatusData
      res: {
        /**
         * The pipeline has finished processing inputs associated with the provided completion token.
         */
        '200': CompletionStatusResponse
        /**
         * An invalid completion token was provided
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        /**
         * Completion token was created by a previous incarnation of the pipeline and is not valid for the current incarnation. This indicates that the pipeline was suspended and resumed from a checkpoint or restarted after a failure.
         */
        '410': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/egress/{table_name}': {
    post: {
      req: HttpOutputData
      res: {
        /**
         * Connection to the endpoint successfully established. The body of the response contains a stream of data chunks.
         */
        '200': Chunk
        '400': ErrorResponse
        /**
         * Pipeline and/or table/view with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/heap_profile': {
    get: {
      req: GetPipelineHeapProfileData
      res: {
        /**
         * Heap usage profile as a gzipped protobuf that can be inspected by the pprof tool
         */
        '200': Blob | File
        /**
         * Getting a heap profile is not supported on this platform
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/ingress/{table_name}': {
    post: {
      req: HttpInputData
      res: {
        /**
         * Data successfully delivered to the pipeline. The body of the response contains a completion token that can be passed to the '/completion_status' endpoint to check whether the pipeline has fully processed the data.
         */
        '200': CompletionTokenResponse
        '400': ErrorResponse
        /**
         * Pipeline and/or table with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/logs': {
    get: {
      req: GetPipelineLogsData
      res: {
        /**
         * Pipeline logs retrieved successfully
         */
        '200': Blob | File
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/metrics': {
    get: {
      req: GetPipelineMetricsData
      res: {
        /**
         * Pipeline circuit metrics retrieved successfully
         */
        '200': Blob | File
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/pause': {
    post: {
      req: PostPipelinePauseData
      res: {
        /**
         * Action is accepted and is being performed
         */
        '202': unknown
        /**
         * Action could not be performed
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/query': {
    get: {
      req: PipelineAdhocSqlData
      res: {
        /**
         * Ad-hoc SQL query result
         */
        '200': Blob | File
        /**
         * Invalid SQL query
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/resume': {
    post: {
      req: PostPipelineResumeData
      res: {
        /**
         * Action is accepted and is being performed
         */
        '202': unknown
        /**
         * Action could not be performed
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/start': {
    post: {
      req: PostPipelineStartData
      res: {
        /**
         * Action is accepted and is being performed
         */
        '202': unknown
        /**
         * Action could not be performed
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/start_transaction': {
    post: {
      req: StartTransactionData
      res: {
        /**
         * Transaction successfully started.
         */
        '200': StartTransactionResponse
        /**
         * Another transaction is already in progress.
         */
        '409': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/stats': {
    get: {
      req: GetPipelineStatsData
      res: {
        /**
         * Pipeline statistics retrieved successfully
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/stop': {
    post: {
      req: PostPipelineStopData
      res: {
        /**
         * Action is accepted and is being performed
         */
        '202': unknown
        /**
         * Action could not be performed
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        /**
         * Action is not supported
         */
        '405': ErrorResponse
        '500': ErrorResponse
        /**
         * Action is not implemented because it is only available in the Enterprise edition
         */
        '501': ErrorResponse
        /**
         * Action can not be performed (maybe because the pipeline is already suspended)
         */
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/support_bundle': {
    get: {
      req: GetPipelineSupportBundleData
      res: {
        /**
         * Support bundle containing diagnostic information
         */
        '200': Blob | File
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/tables/{table_name}/connectors/{connector_name}/completion_token': {
    get: {
      req: CompletionTokenData
      res: {
        /**
         * Completion token that can be passed to the '/completion_status' endpoint.
         */
        '200': CompletionTokenResponse
        /**
         * Specified pipeline, table, or connector does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/tables/{table_name}/connectors/{connector_name}/stats': {
    get: {
      req: GetPipelineInputConnectorStatusData
      res: {
        /**
         * Input connector status retrieved successfully
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline, table and/or input connector with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/tables/{table_name}/connectors/{connector_name}/{action}': {
    post: {
      req: PostPipelineInputConnectorActionData
      res: {
        /**
         * Action has been processed
         */
        '200': unknown
        /**
         * Pipeline, table and/or input connector with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/testing': {
    post: {
      req: PostPipelineTestingData
      res: {
        /**
         * Request successfully processed
         */
        '200': unknown
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        /**
         * Endpoint is disabled. Set FELDERA_UNSTABLE_FEATURES="testing" to enable.
         */
        '405': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/time_series': {
    get: {
      req: GetPipelineTimeSeriesData
      res: {
        /**
         * Pipeline time series retrieved successfully
         */
        '200': TimeSeries
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/time_series_stream': {
    get: {
      req: GetPipelineTimeSeriesStreamData
      res: {
        /**
         * Pipeline time series stream established successfully
         */
        '200': string
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/update_runtime': {
    post: {
      req: PostUpdateRuntimeData
      res: {
        /**
         * Pipeline successfully updated
         */
        '200': PipelineInfo
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/views/{view_name}/connectors/{connector_name}/stats': {
    get: {
      req: GetPipelineOutputConnectorStatusData
      res: {
        /**
         * Output connector status retrieved successfully
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline, view and/or output connector with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
}
