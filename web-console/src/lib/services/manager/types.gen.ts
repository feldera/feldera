// This file is auto-generated by @hey-api/openapi-ts

/**
 * Configuration for inserting data with ad-hoc queries
 *
 * An ad-hoc input adapters cannot be usefully configured as part of pipeline
 * configuration.  Instead, use ad-hoc queries through the UI, the REST API, or
 * the `fda` command-line tool.
 */
export type AdHocInputConfig = {
  /**
   * Autogenerated name.
   */
  name: string
}

/**
 * URL-encoded `format` argument to the `/query` endpoint.
 */
export type AdHocResultFormat = 'text' | 'json' | 'parquet' | 'arrow_ipc'

/**
 * URL-encoded arguments to the `/query` endpoint.
 */
export type AdhocQueryArgs = {
  format?: AdHocResultFormat
  /**
   * The SQL query to run.
   */
  sql: string
}

/**
 * API key descriptor.
 */
export type ApiKeyDescr = {
  id: ApiKeyId
  name: string
  scopes: Array<ApiPermission>
}

/**
 * API key identifier.
 */
export type ApiKeyId = string

/**
 * Permission types for invoking API endpoints.
 */
export type ApiPermission = 'Read' | 'Write'

export type AuthProvider =
  | {
      AwsCognito: ProviderAwsCognito
    }
  | {
      GoogleIdentity: ProviderGoogleIdentity
    }

/**
 * A set of updates to a SQL table or view.
 *
 * The `sequence_number` field stores the offset of the chunk relative to the
 * start of the stream and can be used to implement reliable delivery.
 * The payload is stored in the `bin_data`, `text_data`, or `json_data` field
 * depending on the data format used.
 */
export type Chunk = {
  /**
   * Base64 encoded binary payload, e.g., bincode.
   */
  bin_data?: (Blob | File) | null
  /**
   * JSON payload.
   */
  json_data?: {
    [key: string]: unknown
  } | null
  sequence_number: number
  /**
   * Text payload, e.g., CSV.
   */
  text_data?: string | null
}

/**
 * A SQL column type description.
 *
 * Matches the Calcite JSON format.
 */
export type ColumnType = {
  component?: ColumnType | null
  /**
   * The fields of the type (if available).
   *
   * For example this would specify the fields of a `CREATE TYPE` construct.
   *
   * ```sql
   * CREATE TYPE person_typ AS (
   * firstname       VARCHAR(30),
   * lastname        VARCHAR(30),
   * address         ADDRESS_TYP
   * );
   * ```
   *
   * Would lead to the following `fields` value:
   *
   * ```sql
   * [
   * ColumnType { name: "firstname, ... },
   * ColumnType { name: "lastname", ... },
   * ColumnType { name: "address", fields: [ ... ] }
   * ]
   * ```
   */
  fields?: Array<Field> | null
  key?: ColumnType | null
  /**
   * Does the type accept NULL values?
   */
  nullable: boolean
  /**
   * Precision of the type.
   *
   * # Examples
   * - `VARCHAR` sets precision to `-1`.
   * - `VARCHAR(255)` sets precision to `255`.
   * - `BIGINT`, `DATE`, `FLOAT`, `DOUBLE`, `GEOMETRY`, etc. sets precision
   * to None
   * - `TIME`, `TIMESTAMP` set precision to `0`.
   */
  precision?: number | null
  /**
   * The scale of the type.
   *
   * # Example
   * - `DECIMAL(1,2)` sets scale to `2`.
   */
  scale?: number | null
  type?: SqlType
  value?: ColumnType | null
}

/**
 * Enumeration of possible compilation profiles that can be passed to the Rust compiler
 * as an argument via `cargo build --profile <>`. A compilation profile affects among
 * other things the compilation speed (how long till the program is ready to be run)
 * and runtime speed (the performance while running).
 */
export type CompilationProfile = 'dev' | 'unoptimized' | 'optimized'

/**
 * Completion token status returned by the `/completion_status` endpoint.
 */
export type CompletionStatus = 'complete' | 'inprogress'

/**
 * URL-encoded arguments to the `/completion_status` endpoint.
 */
export type CompletionStatusArgs = {
  /**
   * Completion token returned by the `/completion_token` or `/ingress`
   * endpoint.
   */
  token: string
}

/**
 * Response to a completion token status request.
 */
export type CompletionStatusResponse = {
  status: CompletionStatus
}

/**
 * Response to a completion token creation request.
 */
export type CompletionTokenResponse = {
  /**
   * Completion token.
   *
   * An opaque string associated with the current position in the input stream
   * generated by an input connector.
   * Pass this string to the `/completion_status` endpoint to check whether all
   * inputs associated with the token have been fully processed by the pipeline.
   */
  token: string
}

export type Configuration = {
  /**
   * URL that navigates to the changelog of the current version
   */
  changelog_url: string
  /**
   * Feldera edition: "Open source" or "Enterprise"
   */
  edition: string
  license_info?: LicenseInformation | null
  /**
   * Specific revision corresponding to the edition `version` (e.g., git commit hash).
   * This is an empty string if it is unspecified.
   */
  revision: string
  /**
   * Telemetry key.
   */
  telemetry: string
  update_info?: UpdateInformation | null
  /**
   * The version corresponding to the type of `edition`.
   * Format is `x.y.z`.
   */
  version: string
}

/**
 * A data connector's configuration
 */
export type ConnectorConfig = OutputBufferConfig & {
  format?: FormatConfig | null
  /**
   * Name of the index that the connector is attached to.
   *
   * This property is valid for output connectors only.  It is used with data
   * transports and formats that expect output updates in the form of key/value
   * pairs, where the key typically represents a unique id associated with the
   * table or view.
   *
   * To support such output formats, an output connector can be attached to an
   * index created using the SQL CREATE INDEX statement.  An index of a table
   * or view contains the same updates as the table or view itself, indexed by
   * one or more key columns.
   *
   * See individual connector documentation for details on how they work
   * with indexes.
   */
  index?: string | null
  /**
   * Arbitrary user-defined text labels associated with the connector.
   *
   * These labels can be used in conjunction with the `start_after` property
   * to control the start order of connectors.
   */
  labels?: Array<string>
  /**
   * Maximum batch size, in records.
   *
   * This is the maximum number of records to process in one batch through
   * the circuit.  The time and space cost of processing a batch is
   * asymptotically superlinear in the size of the batch, but very small
   * batches are less efficient due to constant factors.
   *
   * This should usually be less than `max_queued_records`, to give the
   * connector a round-trip time to restart and refill the buffer while
   * batches are being processed.
   *
   * Some input adapters might not honor this setting.
   *
   * The default is 10,000.
   */
  max_batch_size?: number
  /**
   * Backpressure threshold.
   *
   * Maximal number of records queued by the endpoint before the endpoint
   * is paused by the backpressure mechanism.
   *
   * For input endpoints, this setting bounds the number of records that have
   * been received from the input transport but haven't yet been consumed by
   * the circuit since the circuit, since the circuit is still busy processing
   * previous inputs.
   *
   * For output endpoints, this setting bounds the number of records that have
   * been produced by the circuit but not yet sent via the output transport endpoint
   * nor stored in the output buffer (see `enable_output_buffer`).
   *
   * Note that this is not a hard bound: there can be a small delay between
   * the backpressure mechanism is triggered and the endpoint is paused, during
   * which more data may be queued.
   *
   * The default is 1 million.
   */
  max_queued_records?: number
  /**
   * Create connector in paused state.
   *
   * The default is `false`.
   */
  paused?: boolean
  /**
   * Start the connector after all connectors with specified labels.
   *
   * This property is used to control the start order of connectors.
   * The connector will not start until all connectors with the specified
   * labels have finished processing all inputs.
   */
  start_after?: Array<string> | null
  transport: TransportConfig
}

/**
 * Configuration for generating random data for a table.
 */
export type DatagenInputConfig = {
  /**
   * The sequence of generations to perform.
   *
   * If not set, the generator will produce a single sequence with default settings.
   * If set, the generator will produce the specified sequences in sequential order.
   *
   * Note that if one of the sequences before the last one generates an unlimited number of rows
   * the following sequences will not be executed.
   */
  plan?: Array<GenerationPlan>
  /**
   * Optional seed for the random generator.
   *
   * Setting this to a fixed value will make the generator produce the same sequence of records
   * every time the pipeline is run.
   *
   * # Notes
   * - To ensure the set of generated input records is deterministic across multiple runs,
   * apart from setting a seed, `workers` also needs to remain unchanged.
   * - The input will arrive in non-deterministic order if `workers > 1`.
   */
  seed?: number | null
  /**
   * Number of workers to use for generating data.
   */
  workers?: number
}

/**
 * Strategy used to generate values.
 */
export type DatagenStrategy =
  | 'increment'
  | 'uniform'
  | 'zipf'
  | 'word'
  | 'words'
  | 'sentence'
  | 'sentences'
  | 'paragraph'
  | 'paragraphs'
  | 'first_name'
  | 'last_name'
  | 'title'
  | 'suffix'
  | 'name'
  | 'name_with_title'
  | 'domain_suffix'
  | 'email'
  | 'username'
  | 'password'
  | 'field'
  | 'position'
  | 'seniority'
  | 'job_title'
  | 'ipv4'
  | 'ipv6'
  | 'ip'
  | 'mac_address'
  | 'user_agent'
  | 'rfc_status_code'
  | 'valid_status_code'
  | 'company_suffix'
  | 'company_name'
  | 'buzzword'
  | 'buzzword_middle'
  | 'buzzword_tail'
  | 'catch_phrase'
  | 'bs_verb'
  | 'bs_adj'
  | 'bs_noun'
  | 'bs'
  | 'profession'
  | 'industry'
  | 'currency_code'
  | 'currency_name'
  | 'currency_symbol'
  | 'credit_card_number'
  | 'city_prefix'
  | 'city_suffix'
  | 'city_name'
  | 'country_name'
  | 'country_code'
  | 'street_suffix'
  | 'street_name'
  | 'time_zone'
  | 'state_name'
  | 'state_abbr'
  | 'secondary_address_type'
  | 'secondary_address'
  | 'zip_code'
  | 'post_code'
  | 'building_number'
  | 'latitude'
  | 'longitude'
  | 'isbn'
  | 'isbn13'
  | 'isbn10'
  | 'phone_number'
  | 'cell_number'
  | 'file_path'
  | 'file_name'
  | 'file_extension'
  | 'dir_path'

/**
 * Delta table read mode.
 *
 * Three options are available:
 *
 * * `snapshot` - read a snapshot of the table and stop.
 *
 * * `follow` - continuously ingest changes to the table, starting from a specified version
 * or timestamp.
 *
 * * `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion
 * mode.
 */
export type DeltaTableIngestMode = 'snapshot' | 'follow' | 'snapshot_and_follow' | 'cdc'

/**
 * Delta table input connector configuration.
 */
export type DeltaTableReaderConfig = {
  /**
   * A predicate that determines whether the record represents a deletion.
   *
   * This setting is only valid in the 'cdc' mode. It specifies a predicate applied to
   * each row in the Delta table to determine whether the row represents a deletion event.
   * Its value must be a valid Boolean SQL expression that can be used in a query of the
   * form `SELECT * from <table> WHERE <cdc_delete_filter>`.
   */
  cdc_delete_filter?: string | null
  /**
   * An expression that determines the ordering of updates in the Delta table.
   *
   * This setting is only valid in the 'cdc' mode. It specifies a predicate applied to
   * each row in the Delta table to determine the order in which updates in the table should
   * be applied. Its value must be a valid SQL expression that can be used in a query of the
   * form `SELECT * from <table> ORDER BY <cdc_order_by>`.
   */
  cdc_order_by?: string | null
  /**
   * Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
   * "2024-12-09T16:09:53+00:00".
   *
   * When this option is set, the connector finds and opens the version of the table as of the
   * specified point in time (based on the server time recorded in the transaction log, not the
   * event time encoded in the data).  In `snapshot` and `snapshot_and_follow` modes, it
   * retrieves the snapshot of this version of the table.  In `follow` and `snapshot_and_follow`
   * modes, it follows transaction log records **after** this version.
   *
   * Note: at most one of `version` and `datetime` options can be specified.
   * When neither of the two options is specified, the latest committed version of the table
   * is used.
   */
  datetime?: string | null
  /**
   * Optional row filter.
   *
   * When specified, only rows that satisfy the filter condition are read from the delta table.
   * The condition must be a valid SQL Boolean expression that can be used in
   * the `where` clause of the `select * from my_table where ...` query.
   */
  filter?: string | null
  /**
   * Maximum number of concurrent object store reads performed by all Delta Lake connectors.
   *
   * This setting is used to limit the number of concurrent reads of the object store in a
   * pipeline with a large number of Delta Lake connectors. When multiple connectors are simultaneously
   * reading from the object store, this can lead to transport timeouts.
   *
   * When enabled, this setting limits the number of concurrent reads across all connectors.
   * This is a global setting that affects all Delta Lake connectors, and not just the connector
   * where it is specified. It should therefore be used at most once in a pipeline.  If multiple
   * connectors specify this setting, they must all use the same value.
   *
   * The default value is 6.
   */
  max_concurrent_readers?: number | null
  mode: DeltaTableIngestMode
  /**
   * The number of parallel parsing tasks the connector uses to process data read from the
   * table. Increasing this value can enhance performance by allowing more concurrent processing.
   * Recommended range: 1–10. The default is 4.
   */
  num_parsers?: number
  /**
   * Don't read unused columns from the Delta table.
   *
   * When set to `true`, this option instructs the connector to avoid reading
   * columns from the Delta table that are not used in any view definitions.
   * To be skipped, the columns must be either nullable or have default
   * values. This can improve ingestion performance, especially for wide
   * tables.
   *
   * Note: The simplest way to exclude unused columns is to omit them from the Feldera SQL table
   * declaration. The connector never reads columns that aren't declared in the SQL schema.
   * Additionally, the SQL compiler emits warnings for declared but unused columns—use these as
   * a guide to optimize your schema.
   */
  skip_unused_columns?: boolean
  /**
   * Optional snapshot filter.
   *
   * This option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.
   *
   * When specified, only rows that satisfy the filter condition are included in the
   * snapshot.  The condition must be a valid SQL Boolean expression that can be used in
   * the `where` clause of the `select * from snapshot where ...` query.
   *
   * Unlike the `filter` option, which applies to all records retrieved from the table, this
   * filter only applies to rows in the initial snapshot of the table.
   * For instance, it can be used to specify the range of event times to include in the snapshot,
   * e.g.: `ts BETWEEN TIMESTAMP '2005-01-01 00:00:00' AND TIMESTAMP '2010-12-31 23:59:59'`.
   *
   * This option can be used together with the `filter` option. During the initial snapshot,
   * only rows that satisfy both `filter` and `snapshot_filter` are retrieved from the Delta table.
   * When subsequently following changes in the the transaction log (`mode = snapshot_and_follow`),
   * all rows that meet the `filter` condition are ingested, regardless of `snapshot_filter`.
   */
  snapshot_filter?: string | null
  /**
   * Table column that serves as an event timestamp.
   *
   * When this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,
   * table rows are ingested in the timestamp order, respecting the
   * [`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)
   * property of the column: each ingested row has a timestamp no more than `LATENESS`
   * time units earlier than the most recent timestamp of any previously ingested row.
   * The ingestion is performed by partitioning the table into timestamp ranges of width
   * `LATENESS`. Each range is processed sequentially, in increasing timestamp order.
   *
   * # Example
   *
   * Consider a table with timestamp column of type `TIMESTAMP` and lateness attribute
   * `INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is
   * `2024-01-01T00:00:00``, the connector will fetch all records with timestamps
   * from `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records
   * in the table have been ingested.
   *
   * # Requirements
   *
   * * The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.
   * * The timestamp column must be declared with non-zero `LATENESS`.
   * * For efficient ingest, the table must be optimized for timestamp-based
   * queries using partitioning, Z-ordering, or liquid clustering.
   */
  timestamp_column?: string | null
  /**
   * Table URI.
   *
   * Example: "s3://feldera-fraud-detection-data/demographics_train"
   */
  uri: string
  /**
   * Optional table version.
   *
   * When this option is set, the connector finds and opens the specified version of the table.
   * In `snapshot` and `snapshot_and_follow` modes, it retrieves the snapshot of this version of
   * the table.  In `follow` and `snapshot_and_follow` modes, it follows transaction log records
   * **after** this version.
   *
   * Note: at most one of `version` and `datetime` options can be specified.
   * When neither of the two options is specified, the latest committed version of the table
   * is used.
   */
  version?: number | null
  /**
   * Storage options for configuring backend object store.
   *
   * For specific options available for different storage backends, see:
   * * [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
   * * [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
   * * [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
   */
  '[key: string]': (string | number | DeltaTableIngestMode | boolean) | undefined
}

/**
 * Delta table write mode.
 *
 * Determines how the Delta table connector handles an existing table at the target location.
 */
export type DeltaTableWriteMode = 'append' | 'truncate' | 'error_if_exists'

/**
 * Delta table output connector configuration.
 */
export type DeltaTableWriterConfig = {
  mode?: DeltaTableWriteMode
  /**
   * Table URI.
   */
  uri: string
  /**
   * Storage options for configuring backend object store.
   *
   * For specific options available for different storage backends, see:
   * * [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
   * * [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
   * * [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
   */
  '[key: string]': (string | DeltaTableWriteMode) | undefined
}

export type Demo = {
  /**
   * Description of the demo (parsed from SQL preamble).
   */
  description: string
  /**
   * Name of the demo (parsed from SQL preamble).
   */
  name: string
  /**
   * Program SQL code.
   */
  program_code: string
  /**
   * Title of the demo (parsed from SQL preamble).
   */
  title: string
  /**
   * User defined function (UDF) Rust code.
   */
  udf_rust: string
  /**
   * User defined function (UDF) TOML dependencies.
   */
  udf_toml: string
}

export type DisplaySchedule =
  | 'Once'
  | 'Session'
  | {
      /**
       * Display it again after a certain period of time after it is dismissed
       */
      Every: {
        seconds: number
      }
    }
  | 'Always'

/**
 * Information returned by REST API endpoints on error.
 */
export type ErrorResponse = {
  /**
   * Detailed error metadata.
   * The contents of this field is determined by `error_code`.
   */
  details: {
    [key: string]: unknown
  }
  /**
   * Error code is a string that specifies this error type.
   */
  error_code: string
  /**
   * Human-readable error message.
   */
  message: string
}

/**
 * A SQL field.
 *
 * Matches the SQL compiler JSON format.
 */
export type Field = SqlIdentifier & {
  columntype: ColumnType
  default?: string | null
  lateness?: string | null
  unused: boolean
  watermark?: string | null
}

/**
 * Configuration for reading data from a file with `FileInputTransport`
 */
export type FileInputConfig = {
  /**
   * Read buffer size.
   *
   * Default: when this parameter is not specified, a platform-specific
   * default is used.
   */
  buffer_size_bytes?: number | null
  /**
   * Enable file following.
   *
   * When `false`, the endpoint outputs an `InputConsumer::eoi`
   * message and stops upon reaching the end of file.  When `true`, the
   * endpoint will keep watching the file and outputting any new content
   * appended to it.
   */
  follow?: boolean
  /**
   * File path.
   */
  path: string
}

/**
 * Configuration for writing data to a file with `FileOutputTransport`.
 */
export type FileOutputConfig = {
  /**
   * File path.
   */
  path: string
}

/**
 * Data format specification used to parse raw data received from the
 * endpoint or to encode data sent to the endpoint.
 */
export type FormatConfig = {
  /**
   * Format-specific parser or encoder configuration.
   */
  config?: {
    [key: string]: unknown
  }
  /**
   * Format name, e.g., "csv", "json", "bincode", etc.
   */
  name: string
}

/**
 * Fault-tolerance configuration.
 *
 * The default [FtConfig] (via [FtConfig::default]) disables fault tolerance,
 * which is the configuration that one gets if [RuntimeConfig] omits fault
 * tolerance configuration.
 *
 * The default value for [FtConfig::model] enables fault tolerance, as
 * `Some(FtModel::default())`.  This is the configuration that one gets if
 * [RuntimeConfig] includes a fault tolerance configuration but does not
 * specify a particular model.
 */
export type FtConfig = {
  /**
   * Interval between automatic checkpoints, in seconds.
   *
   * The default is 60 seconds.  Values less than 1 or greater than 3600 will
   * be forced into that range.
   */
  checkpoint_interval_secs?: number
  model?: FtModel | 'none'
}

/**
 * Fault tolerance model.
 *
 * The ordering is significant: we consider [Self::ExactlyOnce] to be a "higher
 * level" of fault tolerance than [Self::AtLeastOnce].
 */
export type FtModel = 'at_least_once' | 'exactly_once'

/**
 * A random generation plan for a table that generates either a limited amount of rows or runs continuously.
 */
export type GenerationPlan = {
  /**
   * Specifies the values that the generator should produce.
   */
  fields?: {
    [key: string]: RngFieldSettings
  }
  /**
   * Total number of new rows to generate.
   *
   * If not set, the generator will produce new/unique records as long as the pipeline is running.
   * If set to 0, the table will always remain empty.
   * If set, the generator will produce new records until the specified limit is reached.
   *
   * Note that if the table has one or more primary keys that don't use the `increment` strategy to
   * generate the key there is a potential that an update is generated instead of an insert. In
   * this case it's possible the total number of records is less than the specified limit.
   */
  limit?: number | null
  /**
   * Non-zero number of rows to generate per second.
   *
   * If not set, the generator will produce rows as fast as possible.
   */
  rate?: number | null
  /**
   * When multiple workers are used, each worker will pick a consecutive "chunk" of
   * records to generate.
   *
   * By default, if not specified, the generator will use the formula `min(rate, 10_000)`
   * to determine it. This works well in most situations. However, if you're
   * running tests with lateness and many workers you can e.g., reduce the
   * chunk size to make sure a smaller range of records is being ingested in parallel.
   *
   * # Example
   * Assume you generate a total of 125 records with 4 workers and a chunk size of 25.
   * In this case, worker A will generate records 0..25, worker B will generate records 25..50,
   * etc. A, B, C, and D will generate records in parallel. The first worker to finish its chunk
   * will pick up the last chunk of records (100..125) to generate.
   */
  worker_chunk_size?: number | null
}

/**
 * Query parameters to GET a pipeline or a list of pipelines.
 */
export type GetPipelineParameters = {
  selector?: PipelineFieldSelector
}

/**
 * AWS Glue catalog config.
 */
export type GlueCatalogConfig = {
  /**
   * Access key id used to access the Glue catalog.
   */
  'glue.access-key-id'?: string | null
  /**
   * Configure an alternative endpoint of the Glue service for Glue catalog to access.
   *
   * Example: `"https://glue.us-east-1.amazonaws.com"`
   */
  'glue.endpoint'?: string | null
  /**
   * The 12-digit ID of the Glue catalog.
   */
  'glue.id'?: string | null
  /**
   * Profile used to access the Glue catalog.
   */
  'glue.profile-name'?: string | null
  /**
   * Region of the Glue catalog.
   */
  'glue.region'?: string | null
  /**
   * Secret access key used to access the Glue catalog.
   */
  'glue.secret-access-key'?: string | null
  'glue.session-token'?: string | null
  /**
   * Location for table metadata.
   *
   * Example: `"s3://my-data-warehouse/tables/"`
   */
  'glue.warehouse'?: string | null
}

/**
 * Configuration for reading data via HTTP.
 *
 * HTTP input adapters cannot be usefully configured as part of pipeline
 * configuration.  Instead, instantiate them through the REST API as
 * `/pipelines/{pipeline_name}/ingress/{table_name}`.
 */
export type HttpInputConfig = {
  /**
   * Autogenerated name.
   */
  name: string
}

export type IcebergCatalogType = 'rest' | 'glue'

/**
 * Iceberg table read mode.
 *
 * Three options are available:
 *
 * * `snapshot` - read a snapshot of the table and stop.
 *
 * * `follow` - continuously ingest changes to the table, starting from a specified snapshot
 * or timestamp.
 *
 * * `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion
 * mode.
 */
export type IcebergIngestMode = 'snapshot' | 'follow' | 'snapshot_and_follow'

/**
 * Iceberg input connector configuration.
 */
export type IcebergReaderConfig = GlueCatalogConfig &
  RestCatalogConfig & {
    catalog_type?: IcebergCatalogType | null
    /**
     * Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
     * "2024-12-09T16:09:53+00:00".
     *
     * When this option is set, the connector finds and opens the snapshot of the table as of the
     * specified point in time (based on the server time recorded in the transaction
     * log, not the event time encoded in the data).  In `snapshot` and `snapshot_and_follow`
     * modes, it retrieves this snapshot.  In `follow` and `snapshot_and_follow` modes, it
     * follows transaction log records **after** this snapshot.
     *
     * Note: at most one of `snapshot_id` and `datetime` options can be specified.
     * When neither of the two options is specified, the latest committed version of the table
     * is used.
     */
    datetime?: string | null
    /**
     * Location of the table metadata JSON file.
     *
     * This propery is used to access an Iceberg table without a catalog. It is mutually
     * exclusive with the `catalog_type` property.
     */
    metadata_location?: string | null
    mode: IcebergIngestMode
    /**
     * Optional row filter.
     *
     * This option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.
     *
     * When specified, only rows that satisfy the filter condition are included in the
     * snapshot.  The condition must be a valid SQL Boolean expression that can be used in
     * the `where` clause of the `select * from snapshot where ...` query.
     *
     * This option can be used to specify the range of event times to include in the snapshot,
     * e.g.: `ts BETWEEN '2005-01-01 00:00:00' AND '2010-12-31 23:59:59'`.
     */
    snapshot_filter?: string | null
    /**
     * Optional snapshot id.
     *
     * When this option is set, the connector finds the specified snapshot of the table.
     * In `snapshot` and `snapshot_and_follow` modes, it loads this snapshot.
     * In `follow` and `snapshot_and_follow` modes, it follows table updates
     * **after** this snapshot.
     *
     * Note: at most one of `snapshot_id` and `datetime` options can be specified.
     * When neither of the two options is specified, the latest committed version of the table
     * is used.
     */
    snapshot_id?: number | null
    /**
     * Specifies the Iceberg table name in the "namespace.table" format.
     *
     * This option is applicable when an Iceberg catalog is configured using the `catalog_type` property.
     */
    table_name?: string | null
    /**
     * Table column that serves as an event timestamp.
     *
     * When this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,
     * table rows are ingested in the timestamp order, respecting the
     * [`LATENESS`](https://docs.feldera.com/sql/streaming#lateness-expressions)
     * property of the column: each ingested row has a timestamp no more than `LATENESS`
     * time units earlier than the most recent timestamp of any previously ingested row.
     * The ingestion is performed by partitioning the table into timestamp ranges of width
     * `LATENESS`. Each range is processed sequentially, in increasing timestamp order.
     *
     * # Example
     *
     * Consider a table with timestamp column of type `TIMESTAMP` and lateness attribute
     * `INTERVAL 1 DAY`. Assuming that the oldest timestamp in the table is
     * `2024-01-01T00:00:00``, the connector will fetch all records with timestamps
     * from `2024-01-01`, then all records for `2024-01-02`, `2024-01-03`, etc., until all records
     * in the table have been ingested.
     *
     * # Requirements
     *
     * * The timestamp column must be of a supported type: integer, `DATE`, or `TIMESTAMP`.
     * * The timestamp column must be declared with non-zero `LATENESS`.
     * * For efficient ingest, the table must be optimized for timestamp-based
     * queries using partitioning, Z-ordering, or liquid clustering.
     */
    timestamp_column?: string | null
    /**
     * Storage options for configuring backend object store.
     *
     * See the [list of available options in PyIceberg documentation](https://py.iceberg.apache.org/configuration/#fileio).
     */
    '[key: string]': (string | unknown | IcebergIngestMode | number) | undefined
  }

/**
 * Describes an input connector configuration
 */
export type InputEndpointConfig = ConnectorConfig & {
  /**
   * The name of the input stream of the circuit that this endpoint is
   * connected to.
   */
  stream: string
}

/**
 * The specified units for SQL Interval types.
 *
 * `INTERVAL 1 DAY`, `INTERVAL 1 DAY TO HOUR`, `INTERVAL 1 DAY TO MINUTE`,
 * would yield `Day`, `DayToHour`, `DayToMinute`, as the `IntervalUnit` respectively.
 */
export type IntervalUnit =
  | 'Day'
  | 'DayToHour'
  | 'DayToMinute'
  | 'DayToSecond'
  | 'Hour'
  | 'HourToMinute'
  | 'HourToSecond'
  | 'Minute'
  | 'MinuteToSecond'
  | 'Month'
  | 'Second'
  | 'Year'
  | 'YearToMonth'

/**
 * Whether JSON values can span multiple lines.
 */
export type JsonLines = 'multiple' | 'single'

/**
 * Supported JSON data change event formats.
 *
 * Each element in a JSON-formatted input stream specifies
 * an update to one or more records in an input table.  We support
 * several different ways to represent such updates.
 *
 * ### `InsertDelete`
 *
 * Each element in the input stream consists of an "insert" or "delete"
 * command and a record to be inserted to or deleted from the input table.
 *
 * ```json
 * {"insert": {"column1": "hello, world!", "column2": 100}}
 * ```
 *
 * ### `Weighted`
 *
 * Each element in the input stream consists of a record and a weight
 * which indicates how many times the row appears.
 *
 * ```json
 * {"weight": 2, "data": {"column1": "hello, world!", "column2": 100}}
 * ```
 *
 * Note that the line above would be equivalent to the following input in the `InsertDelete` format:
 *
 * ```json
 * {"insert": {"column1": "hello, world!", "column2": 100}}
 * {"insert": {"column1": "hello, world!", "column2": 100}}
 * ```
 *
 * Similarly, negative weights are equivalent to deletions:
 *
 * ```json
 * {"weight": -1, "data": {"column1": "hello, world!", "column2": 100}}
 * ```
 *
 * is equivalent to in the `InsertDelete` format:
 *
 * ```json
 * {"delete": {"column1": "hello, world!", "column2": 100}}
 * ```
 *
 * ### `Debezium`
 *
 * Debezium CDC format.  Refer to [Debezium input connector documentation](https://docs.feldera.com/connectors/sources/debezium) for details.
 *
 * ### `Snowflake`
 *
 * Uses flat structure so that fields can get parsed directly into SQL
 * columns.  Defines three metadata fields:
 *
 * * `__action` - "insert" or "delete"
 * * `__stream_id` - unique 64-bit ID of the output stream (records within
 * a stream are totally ordered)
 * * `__seq_number` - monotonically increasing sequence number relative to
 * the start of the stream.
 *
 * ```json
 * {"PART":1,"VENDOR":2,"EFFECTIVE_SINCE":"2019-05-21","PRICE":"10000","__action":"insert","__stream_id":4523666124030717756,"__seq_number":1}
 * ```
 *
 * ### `Raw`
 *
 * This format is suitable for insert-only streams (no deletions).
 * Each element in the input stream contains a record without any
 * additional envelope that gets inserted in the input table.
 */
export type JsonUpdateFormat =
  | 'insert_delete'
  | 'weighted'
  | 'debezium'
  | 'snowflake'
  | 'raw'
  | 'redis'

/**
 * Kafka message header.
 */
export type KafkaHeader = {
  key: string
  value?: KafkaHeaderValue | null
}

/**
 * Kafka header value encoded as a UTF-8 string or a byte array.
 */
export type KafkaHeaderValue = Blob | File

/**
 * Configuration for reading data from Kafka topics with `InputTransport`.
 */
export type KafkaInputConfig = {
  /**
   * Maximum timeout in seconds to wait for the endpoint to join the Kafka
   * consumer group during initialization.
   */
  group_join_timeout_secs?: number
  log_level?: KafkaLogLevel | null
  /**
   * Set to 1 or more to fix the number of threads used to poll
   * `rdkafka`. Multiple threads can increase performance with small Kafka
   * messages; for large messages, one thread is enough. In either case, too
   * many threads can harm performance. If unset, the default is 3, which
   * helps with small messages but will not harm performance with large
   * messagee
   */
  poller_threads?: number | null
  start_from?: KafkaStartFromConfig
  /**
   * Topic to subscribe to.
   */
  topic: string
  /**
   * Options passed directly to `rdkafka`.
   *
   * [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
   * used to configure the Kafka consumer.
   *
   * This input connector does not use consumer groups, so options related to
   * consumer groups are rejected, including:
   *
   * * `group.id`, if present, is ignored.
   * * `auto.offset.reset` (use `start_from` instead).
   * * "enable.auto.commit", if present, must be set to "false".
   * * "enable.auto.offset.store", if present, must be set to "false".
   */
  '[key: string]': (string | number | unknown | KafkaStartFromConfig) | undefined
}

/**
 * Kafka logging levels.
 */
export type KafkaLogLevel =
  | 'emerg'
  | 'alert'
  | 'critical'
  | 'error'
  | 'warning'
  | 'notice'
  | 'info'
  | 'debug'

/**
 * Configuration for writing data to a Kafka topic with `OutputTransport`.
 */
export type KafkaOutputConfig = {
  fault_tolerance?: KafkaOutputFtConfig | null
  /**
   * Kafka headers to be added to each message produced by this connector.
   */
  headers?: Array<KafkaHeader>
  /**
   * Maximum timeout in seconds to wait for the endpoint to connect to
   * a Kafka broker.
   *
   * Defaults to 60.
   */
  initialization_timeout_secs?: number
  /**
   * If specified, this service is used to provide defaults for the Kafka options.
   */
  kafka_service?: string | null
  log_level?: KafkaLogLevel | null
  /**
   * Topic to write to.
   */
  topic: string
  /**
   * Options passed directly to `rdkafka`.
   *
   * See [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
   * used to configure the Kafka producer.
   */
  '[key: string]': (string | unknown | KafkaHeader | number) | undefined
}

/**
 * Fault tolerance configuration for Kafka output connector.
 */
export type KafkaOutputFtConfig = {
  /**
   * Options passed to `rdkafka` for consumers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for consumers, and may be empty.
   */
  consumer_options?: {
    [key: string]: string
  }
  /**
   * Options passed to `rdkafka` for producers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for producers, and may be empty.
   */
  producer_options?: {
    [key: string]: string
  }
}

/**
 * Where to begin reading a Kafka topic.
 */
export type KafkaStartFromConfig =
  | 'earliest'
  | 'latest'
  | {
      /**
       * Start from particular offsets in the topic.
       *
       * The number of offsets must match the number of partitions in the topic.
       */
      offsets: Array<number>
    }

export type LicenseInformation = {
  /**
   * Optional description of the advantages of extending the license / upgrading from a trial
   */
  description_html: string
  /**
   * Timestamp at which the license expires
   */
  expires_at: string
  /**
   * Duration until the license expires
   */
  expires_in_seconds: number
  /**
   * URL that navigates the user to extend / upgrade their license
   */
  extension_url: string
  /**
   * Whether the license is expired
   */
  is_expired: boolean
  /**
   * Whether the license is a trial
   */
  is_trial: boolean
  remind_schedule: DisplaySchedule
  /**
   * Timestamp from which the user should be reminded of the license expiring soon
   */
  remind_starting_at: string
}

/**
 * Circuit metrics output format.
 * - `prometheus`: [format](https://github.com/prometheus/docs/blob/main/content/docs/instrumenting/exposition_formats.md) expected by Prometheus
 * - `json`: JSON format
 */
export type MetricsFormat = 'prometheus' | 'json'

/**
 * Query parameters to retrieve pipeline circuit metrics.
 */
export type MetricsParameters = {
  format?: MetricsFormat
}

/**
 * Request to create a new API key.
 */
export type NewApiKeyRequest = {
  /**
   * Key name.
   */
  name: string
}

/**
 * Response to a successful API key creation.
 */
export type NewApiKeyResponse = {
  /**
   * Generated secret API key. There is no way to retrieve this
   * key again through the API, so store it securely.
   */
  api_key: string
  id: ApiKeyId
  /**
   * API key name provided by the user.
   */
  name: string
}

/**
 * Configuration for generating Nexmark input data.
 *
 * This connector must be used exactly three times in a pipeline if it is used
 * at all, once for each [`NexmarkTable`].
 */
export type NexmarkInputConfig = {
  options?: NexmarkInputOptions | null
  table: NexmarkTable
}

/**
 * Configuration for generating Nexmark input data.
 */
export type NexmarkInputOptions = {
  /**
   * Number of events to generate and submit together, per thread.
   *
   * Each thread generates this many records, which are then combined with
   * the records generated by the other threads, to form combined input
   * batches of size `threads × batch_size_per_thread`.
   */
  batch_size_per_thread?: number
  /**
   * Number of events to generate.
   */
  events?: number
  /**
   * Maximum number of events to submit in a single step, per thread.
   *
   * This should really be per worker thread, not per generator thread, but
   * the connector does not know how many worker threads there are.
   *
   * This stands in for `max_batch_size` from the connector configuration
   * because it must be a constant across all three of the nexmark tables.
   */
  max_step_size_per_thread?: number
  /**
   * Number of event generator threads.
   *
   * It's reasonable to choose the same number of generator threads as worker
   * threads.
   */
  threads?: number
}

/**
 * Table in Nexmark.
 */
export type NexmarkTable = 'bid' | 'auction' | 'person'

export type ObjectStorageConfig = {
  /**
   * URL.
   *
   * The following URL schemes are supported:
   *
   * * S3:
   * - `s3://<bucket>/<path>`
   * - `s3a://<bucket>/<path>`
   * - `https://s3.<region>.amazonaws.com/<bucket>`
   * - `https://<bucket>.s3.<region>.amazonaws.com`
   * - `https://ACCOUNT_ID.r2.cloudflarestorage.com/bucket`
   * * Google Cloud Storage:
   * - `gs://<bucket>/<path>`
   * * Microsoft Azure Blob Storage:
   * - `abfs[s]://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))
   * - `abfs[s]://<file_system>@<account_name>.dfs.core.windows.net/<path>`
   * - `abfs[s]://<file_system>@<account_name>.dfs.fabric.microsoft.com/<path>`
   * - `az://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))
   * - `adl://<container>/<path>` (according to [fsspec](https://github.com/fsspec/adlfs))
   * - `azure://<container>/<path>` (custom)
   * - `https://<account>.dfs.core.windows.net`
   * - `https://<account>.blob.core.windows.net`
   * - `https://<account>.blob.core.windows.net/<container>`
   * - `https://<account>.dfs.fabric.microsoft.com`
   * - `https://<account>.dfs.fabric.microsoft.com/<container>`
   * - `https://<account>.blob.fabric.microsoft.com`
   * - `https://<account>.blob.fabric.microsoft.com/<container>`
   *
   * Settings derived from the URL will override other settings.
   */
  url: string
  /**
   * Additional options as key-value pairs.
   *
   * The following keys are supported:
   *
   * * S3:
   * - `access_key_id`: AWS Access Key.
   * - `secret_access_key`: AWS Secret Access Key.
   * - `region`: Region.
   * - `default_region`: Default region.
   * - `endpoint`: Custom endpoint for communicating with S3,
   * e.g. `https://localhost:4566` for testing against a localstack
   * instance.
   * - `token`: Token to use for requests (passed to underlying provider).
   * - [Other keys](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html#variants).
   * * Google Cloud Storage:
   * - `service_account`: Path to the service account file.
   * - `service_account_key`: The serialized service account key.
   * - `google_application_credentials`: Application credentials path.
   * - [Other keys](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html).
   * * Microsoft Azure Blob Storage:
   * - `access_key`: Azure Access Key.
   * - `container_name`: Azure Container Name.
   * - `account`: Azure Account.
   * - `bearer_token_authorization`: Static bearer token for authorizing requests.
   * - `client_id`: Client ID for use in client secret or Kubernetes federated credential flow.
   * - `client_secret`: Client secret for use in client secret flow.
   * - `tenant_id`: Tenant ID for use in client secret or Kubernetes federated credential flow.
   * - `endpoint`: Override the endpoint for communicating with blob storage.
   * - [Other keys](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html#variants).
   *
   * Options set through the URL take precedence over those set with these
   * options.
   */
  '[key: string]': string | undefined
}

export type OutputBufferConfig = {
  /**
   * Enable output buffering.
   *
   * The output buffering mechanism allows decoupling the rate at which the pipeline
   * pushes changes to the output transport from the rate of input changes.
   *
   * By default, output updates produced by the pipeline are pushed directly to
   * the output transport. Some destinations may prefer to receive updates in fewer
   * bigger batches. For instance, when writing Parquet files, producing
   * one bigger file every few minutes is usually better than creating
   * small files every few milliseconds.
   *
   * To achieve such input/output decoupling, users can enable output buffering by
   * setting the `enable_output_buffer` flag to `true`.  When buffering is enabled, output
   * updates produced by the pipeline are consolidated in an internal buffer and are
   * pushed to the output transport when one of several conditions is satisfied:
   *
   * * data has been accumulated in the buffer for more than `max_output_buffer_time_millis`
   * milliseconds.
   * * buffer size exceeds `max_output_buffer_size_records` records.
   *
   * This flag is `false` by default.
   */
  enable_output_buffer?: boolean
  /**
   * Maximum number of updates to be kept in the output buffer.
   *
   * This parameter bounds the maximal size of the buffer.
   * Note that the size of the buffer is not always equal to the
   * total number of updates output by the pipeline. Updates to the
   * same record can overwrite or cancel previous updates.
   *
   * By default, the buffer can grow indefinitely until one of
   * the other output conditions is satisfied.
   *
   * NOTE: this configuration option requires the `enable_output_buffer` flag
   * to be set.
   */
  max_output_buffer_size_records?: number
  /**
   * Maximum time in milliseconds data is kept in the output buffer.
   *
   * By default, data is kept in the buffer indefinitely until one of
   * the other output conditions is satisfied.  When this option is
   * set the buffer will be flushed at most every
   * `max_output_buffer_time_millis` milliseconds.
   *
   * NOTE: this configuration option requires the `enable_output_buffer` flag
   * to be set.
   */
  max_output_buffer_time_millis?: number
}

/**
 * Describes an output connector configuration
 */
export type OutputEndpointConfig = ConnectorConfig & {
  /**
   * The name of the output stream of the circuit that this endpoint is
   * connected to.
   */
  stream: string
}

/**
 * Program information is the result of the SQL compilation.
 */
export type PartialProgramInfo = {
  /**
   * Input connectors derived from the schema.
   */
  input_connectors: {
    [key: string]: InputEndpointConfig
  }
  /**
   * Output connectors derived from the schema.
   */
  output_connectors: {
    [key: string]: OutputEndpointConfig
  }
  schema: ProgramSchema
  /**
   * Generated user defined function (UDF) stubs Rust code: stubs.rs
   */
  udf_stubs: string
}

/**
 * Partially update the pipeline (PATCH).
 *
 * Note that the patching only applies to the main fields, not subfields.
 * For instance, it is not possible to update only the number of workers;
 * it is required to again pass the whole runtime configuration with the
 * change.
 */
export type PatchPipeline = {
  description?: string | null
  name?: string | null
  program_code?: string | null
  program_config?: ProgramConfig | null
  runtime_config?: RuntimeConfig | null
  udf_rust?: string | null
  udf_toml?: string | null
}

/**
 * Pipeline deployment configuration.
 * It represents configuration entries directly provided by the user
 * (e.g., runtime configuration) and entries derived from the schema
 * of the compiled program (e.g., connectors). Storage configuration,
 * if applicable, is set by the runner.
 */
export type PipelineConfig = {
  /**
   * Real-time clock resolution in microseconds.
   *
   * This parameter controls the execution of queries that use the `NOW()` function.  The output of such
   * queries depends on the real-time clock and can change over time without any external
   * inputs.  The pipeline will update the clock value and trigger incremental recomputation
   * at most each `clock_resolution_usecs` microseconds.
   *
   * It is set to 100 milliseconds (100,000 microseconds) by default.
   *
   * Set to `null` to disable periodic clock updates.
   */
  clock_resolution_usecs?: number | null
  /**
   * Enable CPU profiler.
   *
   * The default value is `true`.
   */
  cpu_profiler?: boolean
  fault_tolerance?: FtConfig
  /**
   * Maximal delay in microseconds to wait for `min_batch_size_records` to
   * get buffered by the controller, defaults to 0.
   */
  max_buffering_delay_usecs?: number
  /**
   * The maximum number of connectors initialized in parallel during pipeline
   * startup.
   *
   * At startup, the pipeline must initialize all of its input and output connectors.
   * Depending on the number and types of connectors, this can take a long time.
   * To accelerate the process, multiple connectors are initialized concurrently.
   * This option controls the maximum number of connectors that can be intitialized
   * in parallel.
   *
   * The default is 10.
   */
  max_parallel_connector_init?: number | null
  /**
   * Minimal input batch size.
   *
   * The controller delays pushing input records to the circuit until at
   * least `min_batch_size_records` records have been received (total
   * across all endpoints) or `max_buffering_delay_usecs` microseconds
   * have passed since at least one input records has been buffered.
   * Defaults to 0.
   */
  min_batch_size_records?: number
  /**
   * Optionally, a list of CPU numbers for CPUs to which the pipeline may pin
   * its worker threads.  Specify at least twice as many CPU numbers as
   * workers.  CPUs are generally numbered starting from 0.  The pipeline
   * might not be able to honor CPU pinning requests.
   *
   * CPU pinning can make pipelines run faster and perform more consistently,
   * as long as different pipelines running on the same machine are pinned to
   * different CPUs.
   */
  pin_cpus?: Array<number>
  /**
   * Timeout in seconds for the `Provisioning` phase of the pipeline.
   * Setting this value will override the default of the runner.
   */
  provisioning_timeout_secs?: number | null
  resources?: ResourceConfig
  storage?: StorageOptions | null
  /**
   * Enable pipeline tracing.
   */
  tracing?: boolean
  /**
   * Jaeger tracing endpoint to send tracing information to.
   */
  tracing_endpoint_jaeger?: string
  /**
   * Number of DBSP worker threads.
   *
   * Each DBSP "foreground" worker thread is paired with a "background"
   * thread for LSM merging, making the total number of threads twice the
   * specified number.
   */
  workers?: number
} & {
  /**
   * Input endpoint configuration.
   */
  inputs: {
    [key: string]: InputEndpointConfig
  }
  /**
   * Pipeline name.
   */
  name?: string | null
  /**
   * Output endpoint configuration.
   */
  outputs?: {
    [key: string]: OutputEndpointConfig
  }
  storage_config?: StorageConfig | null
}

export type PipelineDesiredStatus = 'Shutdown' | 'Paused' | 'Running'

export type PipelineFieldSelector = 'all' | 'status'

/**
 * Pipeline identifier.
 */
export type PipelineId = string

/**
 * Pipeline information.
 * It both includes fields which are user-provided and system-generated.
 */
export type PipelineInfo = {
  created_at: string
  deployment_desired_status: PipelineDesiredStatus
  deployment_error?: ErrorResponse | null
  deployment_status: PipelineStatus
  deployment_status_since: string
  description: string
  id: PipelineId
  name: string
  platform_version: string
  program_code: string
  program_config: ProgramConfig
  program_error: ProgramError
  program_info?: PartialProgramInfo | null
  program_status: ProgramStatus
  program_status_since: string
  program_version: Version
  refresh_version: Version
  runtime_config: RuntimeConfig
  udf_rust: string
  udf_toml: string
  version: Version
}

/**
 * Pipeline information which has a selected subset of optional fields.
 * It both includes fields which are user-provided and system-generated.
 * If an optional field is not selected (i.e., is `None`), it will not be serialized.
 */
export type PipelineSelectedInfo = {
  created_at: string
  deployment_desired_status: PipelineDesiredStatus
  deployment_error?: ErrorResponse | null
  deployment_status: PipelineStatus
  deployment_status_since: string
  description: string
  id: PipelineId
  name: string
  platform_version: string
  program_code?: string | null
  program_config?: ProgramConfig | null
  program_error?: ProgramError | null
  program_info?: PartialProgramInfo | null
  program_status: ProgramStatus
  program_status_since: string
  program_version: Version
  refresh_version: Version
  runtime_config?: RuntimeConfig | null
  udf_rust?: string | null
  udf_toml?: string | null
  version: Version
}

/**
 * Pipeline status.
 *
 * This type represents the state of the pipeline tracked by the pipeline
 * runner and observed by the API client via the `GET /v0/pipelines/{name}` endpoint.
 *
 * ### The lifecycle of a pipeline
 *
 * The following automaton captures the lifecycle of the pipeline.
 * Individual states and transitions of the automaton are described below.
 *
 * * States labeled with the hourglass symbol (⌛) are **timed** states. The
 * automaton stays in timed state until the corresponding operation completes
 * or until it transitions to become failed after the pre-defined timeout
 * period expires.
 *
 * * State transitions labeled with API endpoint names (`/start`, `/pause`,
 * `/shutdown`) are triggered by invoking corresponding endpoint,
 * e.g., `POST /v0/pipelines/{name}/start`. Note that these only express
 * desired state, and are applied asynchronously by the automata.
 *
 * ```text
 * Shutdown◄────────────────────┐
 * │                        │
 * /start or /pause│                    ShuttingDown ◄────── Failed
 * │                        ▲                  ▲
 * ▼              /shutdown │                  │
 * ⌛Provisioning ──────────────────┤        Shutdown, Provisioning,
 * │                        │        Initializing, Paused,
 * │                        │         Running, Unavailable
 * ▼                        │    (all states except ShuttingDown
 * ⌛Initializing ──────────────────┤      can transition to Failed)
 * │                        │
 * ┌─────────┼────────────────────────┴─┐
 * │         ▼                          │
 * │       Paused  ◄──────► Unavailable │
 * │       │    ▲                ▲      │
 * │ /start│    │/pause          │      │
 * │       ▼    │                │      │
 * │      Running ◄──────────────┘      │
 * └────────────────────────────────────┘
 * ```
 *
 * ### Desired and actual status
 *
 * We use the desired state model to manage the lifecycle of a pipeline.
 * In this model, the pipeline has two status attributes associated with
 * it at runtime: the **desired** status, which represents what the user
 * would like the pipeline to do, and the **current** status, which
 * represents the actual state of the pipeline.  The pipeline runner
 * service continuously monitors both fields and steers the pipeline
 * towards the desired state specified by the user.
 * Only three of the states in the pipeline automaton above can be
 * used as desired statuses: `Paused`, `Running`, and `Shutdown`.
 * These statuses are selected by invoking REST endpoints shown
 * in the diagram.
 *
 * The user can monitor the current state of the pipeline via the
 * `GET /v0/pipelines/{name}` endpoint. In a typical scenario,
 * the user first sets the desired state, e.g., by invoking the
 * `/start` endpoint, and then polls the `GET /v0/pipelines/{name}`
 * endpoint to monitor the actual status of the pipeline until its
 * `deployment_status` attribute changes to `Running` indicating
 * that the pipeline has been successfully initialized and is
 * processing data, or `Failed`, indicating an error.
 */
export type PipelineStatus =
  | 'Shutdown'
  | 'Provisioning'
  | 'Initializing'
  | 'Paused'
  | 'Running'
  | 'Unavailable'
  | 'Failed'
  | 'ShuttingDown'

/**
 * Create a new pipeline (POST), or fully update an existing pipeline (PUT).
 * Fields which are optional and not provided will be set to their empty type value
 * (for strings: an empty string `""`, for objects: an empty dictionary `{}`).
 */
export type PostPutPipeline = {
  description?: string | null
  name: string
  program_code: string
  program_config?: ProgramConfig | null
  runtime_config?: RuntimeConfig | null
  udf_rust?: string | null
  udf_toml?: string | null
}

/**
 * Postgres input connector configuration.
 */
export type PostgresReaderConfig = {
  /**
   * Query that specifies what data to fetch from postgres.
   */
  query: string
  /**
   * Postgres URI.
   * See: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>
   */
  uri: string
}

/**
 * Postgres output connector configuration.
 */
export type PostgresWriterConfig = {
  /**
   * The table to write the output to.
   */
  table: string
  /**
   * Postgres URI.
   * See: <https://docs.rs/tokio-postgres/0.7.12/tokio_postgres/config/struct.Config.html>
   */
  uri: string
}

/**
 * Program configuration.
 */
export type ProgramConfig = {
  /**
   * If `true` (default), when a prior compilation with the same checksum
   * already exists, the output of that (i.e., binary) is used.
   * Set `false` to always trigger a new compilation, which might take longer
   * and as well can result in overriding an existing binary.
   */
  cache?: boolean
  profile?: CompilationProfile | null
}

/**
 * Log, warning and error information about the program compilation.
 */
export type ProgramError = {
  rust_compilation?: RustCompilationInfo | null
  sql_compilation?: SqlCompilationInfo | null
  /**
   * System error that occurred.
   * - Set `Some(...)` upon transition to `SystemError`
   * - Set `None` upon transition to `Pending`
   */
  system_error?: string | null
}

/**
 * Program information is the output of the SQL compiler.
 *
 * It includes information needed for Rust compilation (e.g., generated Rust code)
 * as well as only for runtime (e.g., schema, input/output connectors).
 */
export type ProgramInfo = {
  /**
   * Dataflow graph of the program.
   */
  dataflow?: unknown
  /**
   * Input connectors derived from the schema.
   */
  input_connectors: {
    [key: string]: InputEndpointConfig
  }
  /**
   * Generated main program Rust code: main.rs
   */
  main_rust?: string
  /**
   * Output connectors derived from the schema.
   */
  output_connectors: {
    [key: string]: OutputEndpointConfig
  }
  schema: ProgramSchema
  /**
   * Generated user defined function (UDF) stubs Rust code: stubs.rs
   */
  udf_stubs?: string
}

/**
 * A struct containing the tables (inputs) and views for a program.
 *
 * Parse from the JSON data-type of the DDL generated by the SQL compiler.
 */
export type ProgramSchema = {
  inputs: Array<Relation>
  outputs: Array<Relation>
}

/**
 * Program compilation status.
 */
export type ProgramStatus =
  | 'Pending'
  | 'CompilingSql'
  | 'SqlCompiled'
  | 'CompilingRust'
  | 'Success'
  | 'SqlError'
  | 'RustError'
  | 'SystemError'

export type PropertyValue = {
  key_position: SourcePosition
  value: string
  value_position: SourcePosition
}

export type ProviderAwsCognito = {
  jwk_uri: string
  login_url: string
  logout_url: string
}

export type ProviderGoogleIdentity = {
  client_id: string
  jwk_uri: string
}

/**
 * Google Pub/Sub input connector configuration.
 */
export type PubSubInputConfig = {
  /**
   * gRPC connection timeout.
   */
  connect_timeout_seconds?: number | null
  /**
   * The content of a Google Cloud credentials JSON file.
   *
   * When this option is specified, the connector will use the provided credentials for
   * authentication.  Otherwise, it will use Application Default Credentials (ADC) configured
   * in the environment where the Feldera service is running.  See
   * [Google Cloud documentation](https://cloud.google.com/docs/authentication/provide-credentials-adc)
   * for information on configuring application default credentials.
   *
   * When running Feldera in an environment where ADC are not configured,
   * e.g., a Docker container, use this option to ship Google Cloud credentials from another environment.
   * For example, if you use the
   * [`gcloud auth application-default login`](https://cloud.google.com/pubsub/docs/authentication#client-libs)
   * command for authentication in your local development environment, ADC are stored in the
   * `.config/gcloud/application_default_credentials.json` file in your home directory.
   */
  credentials?: string | null
  /**
   * Set in order to use a Pub/Sub [emulator](https://cloud.google.com/pubsub/docs/emulator)
   * instead of the production service, e.g., 'localhost:8681'.
   */
  emulator?: string | null
  /**
   * Override the default service endpoint 'pubsub.googleapis.com'
   */
  endpoint?: string | null
  /**
   * gRPC channel pool size.
   */
  pool_size?: number | null
  /**
   * Google Cloud project_id.
   *
   * When not specified, the connector will use the project id associated
   * with the authenticated account.
   */
  project_id?: string | null
  /**
   * Reset subscription's backlog to a given snapshot on startup,
   * using the Pub/Sub `Seek` API.
   *
   * This option is mutually exclusive with the `timestamp` option.
   */
  snapshot?: string | null
  /**
   * Subscription name.
   */
  subscription: string
  /**
   * gRPC request timeout.
   */
  timeout_seconds?: number | null
  /**
   * Reset subscription's backlog to a given timestamp on startup,
   * using the Pub/Sub `Seek` API.
   *
   * The value of this option is an ISO 8601-encoded UTC time, e.g., "2024-08-17T16:39:57-08:00".
   *
   * This option is mutually exclusive with the `snapshot` option.
   */
  timestamp?: string | null
}

/**
 * Redis output connector configuration.
 */
export type RedisOutputConfig = {
  /**
   * The URL format: `redis://[<username>][:<password>@]<hostname>[:port][/[<db>][?protocol=<protocol>]]`
   * This is parsed by the [redis](https://docs.rs/redis/latest/redis/#connection-parameters) crate.
   */
  connection_string: string
  /**
   * Separator used to join multiple components into a single key.
   * ":" by default.
   */
  key_separator?: string
}

/**
 * A SQL table or view. It has a name and a list of fields.
 *
 * Matches the Calcite JSON format.
 */
export type Relation = SqlIdentifier & {
  fields: Array<Field>
  materialized?: boolean
  properties?: {
    [key: string]: PropertyValue
  }
}

export type ResourceConfig = {
  /**
   * The maximum number of CPU cores to reserve
   * for an instance of this pipeline
   */
  cpu_cores_max?: number | null
  /**
   * The minimum number of CPU cores to reserve
   * for an instance of this pipeline
   */
  cpu_cores_min?: number | null
  /**
   * The maximum memory in Megabytes to reserve
   * for an instance of this pipeline
   */
  memory_mb_max?: number | null
  /**
   * The minimum memory in Megabytes to reserve
   * for an instance of this pipeline
   */
  memory_mb_min?: number | null
  /**
   * Storage class to use for an instance of this pipeline.
   * The class determines storage performance such as IOPS and throughput.
   */
  storage_class?: string | null
  /**
   * The total storage in Megabytes to reserve
   * for an instance of this pipeline
   */
  storage_mb_max?: number | null
}

/**
 * Iceberg REST catalog config.
 */
export type RestCatalogConfig = {
  /**
   * Logical name of target resource or service.
   */
  'rest.audience'?: string | null
  /**
   * Credential to use for OAuth2 credential flow when initializing the catalog.
   *
   * A key and secret pair separated by ":" (key is optional).
   */
  'rest.credential'?: string | null
  /**
   * Additional HTTP request headers added to each catalog REST API call.
   */
  'rest.headers'?: Array<Array<string>> | null
  /**
   * Authentication URL to use for client credentials authentication (default: uri + 'v1/oauth/tokens')
   */
  'rest.oauth2-server-uri'?: string | null
  /**
   * Customize table storage paths.
   *
   * When combined with the `warehouse` property, the prefix determines
   * how table data is organized within the storage.
   */
  'rest.prefix'?: string | null
  /**
   * URI for the target resource or service.
   */
  'rest.resource'?: string | null
  'rest.scope'?: string | null
  /**
   * Bearer token value to use for `Authorization` header.
   */
  'rest.token'?: string | null
  /**
   * URI identifying the REST catalog server.
   */
  'rest.uri'?: string | null
  /**
   * The default location for managed tables created by the catalog.
   */
  'rest.warehouse'?: string | null
}

/**
 * Configuration for generating random data for a field of a table.
 */
export type RngFieldSettings = {
  /**
   * The frequency rank exponent for the Zipf distribution.
   *
   * - This value is only used if the strategy is set to `Zipf`.
   * - The default value is 1.0.
   */
  e?: number
  /**
   * Specifies the values that the generator should produce in case the field is a struct type.
   */
  fields?: {
    [key: string]: RngFieldSettings
  } | null
  key?: RngFieldSettings | null
  /**
   * Percentage of records where this field should be set to NULL.
   *
   * If not set, the generator will produce only records with non-NULL values.
   * If set to `1..=100`, the generator will produce records with NULL values with the specified percentage.
   */
  null_percentage?: number | null
  /**
   * An optional, exclusive range [a, b) to limit the range of values the generator should produce.
   *
   * - For integer/floating point types specifies min/max values as an integer.
   * If not set, the generator will produce values for the entire range of the type for number types.
   * - For string/binary types specifies min/max length as an integer, values are required to be >=0.
   * If not set, a range of [0, 25) is used by default.
   * - For timestamp types specifies the min/max as two strings in the RFC 3339 format
   * (e.g., ["2021-01-01T00:00:00Z", "2022-01-02T00:00:00Z"]).
   * Alternatively, the range values can be specified as a number of non-leap
   * milliseconds since January 1, 1970 0:00:00.000 UTC (aka “UNIX timestamp”).
   * If not set, a range of ["1970-01-01T00:00:00Z", "2100-01-01T00:00:00Z") or [0, 4102444800000)
   * is used by default.
   * - For time types specifies the min/max as two strings in the "HH:MM:SS" format.
   * Alternatively, the range values can be specified in milliseconds as two positive integers.
   * If not set, the range is 24h.
   * - For date types, the min/max range is specified as two strings in the "YYYY-MM-DD" format.
   * Alternatively, two integers that represent number of days since January 1, 1970 can be used.
   * If not set, a range of ["1970-01-01", "2100-01-01") or [0, 54787) is used by default.
   * - For array types specifies the min/max number of elements as an integer.
   * If not set, a range of [0, 5) is used by default. Range values are required to be >=0.
   * - For map types specifies the min/max number of key-value pairs as an integer.
   * If not set, a range of [0, 5) is used by default.
   * - For struct/boolean/null types `range` is ignored.
   */
  range?: {
    [key: string]: unknown
  }
  /**
   * A scale factor to apply a multiplier to the generated value.
   *
   * - For integer/floating point types, the value is multiplied by the scale factor.
   * - For timestamp types, the generated value (milliseconds) is multiplied by the scale factor.
   * - For time types, the generated value (milliseconds) is multiplied by the scale factor.
   * - For date types, the generated value (days) is multiplied by the scale factor.
   * - For string/binary/array/map/struct/boolean/null types, the scale factor is ignored.
   *
   * - If `values` is specified, the scale factor is ignored.
   * - If `range` is specified and the range is required to be positive (struct, map, array etc.)
   * the scale factor is required to be positive too.
   *
   * The default scale factor is 1.
   */
  scale?: number
  strategy?: DatagenStrategy
  value?: RngFieldSettings | null
  /**
   * An optional set of values the generator will pick from.
   *
   * If set, the generator will pick values from the specified set.
   * If not set, the generator will produce values according to the specified range.
   * If set to an empty set, the generator will produce NULL values.
   * If set to a single value, the generator will produce only that value.
   *
   * Note that `range` is ignored if `values` is set.
   */
  values?: Array<{
    [key: string]: unknown
  }> | null
}

/**
 * Global pipeline configuration settings. This is the publicly
 * exposed type for users to configure pipelines.
 */
export type RuntimeConfig = {
  /**
   * Real-time clock resolution in microseconds.
   *
   * This parameter controls the execution of queries that use the `NOW()` function.  The output of such
   * queries depends on the real-time clock and can change over time without any external
   * inputs.  The pipeline will update the clock value and trigger incremental recomputation
   * at most each `clock_resolution_usecs` microseconds.
   *
   * It is set to 100 milliseconds (100,000 microseconds) by default.
   *
   * Set to `null` to disable periodic clock updates.
   */
  clock_resolution_usecs?: number | null
  /**
   * Enable CPU profiler.
   *
   * The default value is `true`.
   */
  cpu_profiler?: boolean
  fault_tolerance?: FtConfig
  /**
   * Maximal delay in microseconds to wait for `min_batch_size_records` to
   * get buffered by the controller, defaults to 0.
   */
  max_buffering_delay_usecs?: number
  /**
   * The maximum number of connectors initialized in parallel during pipeline
   * startup.
   *
   * At startup, the pipeline must initialize all of its input and output connectors.
   * Depending on the number and types of connectors, this can take a long time.
   * To accelerate the process, multiple connectors are initialized concurrently.
   * This option controls the maximum number of connectors that can be intitialized
   * in parallel.
   *
   * The default is 10.
   */
  max_parallel_connector_init?: number | null
  /**
   * Minimal input batch size.
   *
   * The controller delays pushing input records to the circuit until at
   * least `min_batch_size_records` records have been received (total
   * across all endpoints) or `max_buffering_delay_usecs` microseconds
   * have passed since at least one input records has been buffered.
   * Defaults to 0.
   */
  min_batch_size_records?: number
  /**
   * Optionally, a list of CPU numbers for CPUs to which the pipeline may pin
   * its worker threads.  Specify at least twice as many CPU numbers as
   * workers.  CPUs are generally numbered starting from 0.  The pipeline
   * might not be able to honor CPU pinning requests.
   *
   * CPU pinning can make pipelines run faster and perform more consistently,
   * as long as different pipelines running on the same machine are pinned to
   * different CPUs.
   */
  pin_cpus?: Array<number>
  /**
   * Timeout in seconds for the `Provisioning` phase of the pipeline.
   * Setting this value will override the default of the runner.
   */
  provisioning_timeout_secs?: number | null
  resources?: ResourceConfig
  storage?: StorageOptions | null
  /**
   * Enable pipeline tracing.
   */
  tracing?: boolean
  /**
   * Jaeger tracing endpoint to send tracing information to.
   */
  tracing_endpoint_jaeger?: string
  /**
   * Number of DBSP worker threads.
   *
   * Each DBSP "foreground" worker thread is paired with a "background"
   * thread for LSM merging, making the total number of threads twice the
   * specified number.
   */
  workers?: number
}

/**
 * Rust compilation information.
 */
export type RustCompilationInfo = {
  /**
   * Exit code of the `cargo` compilation command.
   */
  exit_code: number
  /**
   * Output printed to stderr by the `cargo` compilation command.
   */
  stderr: string
  /**
   * Output printed to stdout by the `cargo` compilation command.
   */
  stdout: string
}

/**
 * Configuration for reading data from AWS S3.
 */
export type S3InputConfig = {
  /**
   * AWS Access Key id. This property must be specified unless `no_sign_request` is set to `true`.
   */
  aws_access_key_id?: string | null
  /**
   * Secret Access Key. This property must be specified unless `no_sign_request` is set to `true`.
   */
  aws_secret_access_key?: string | null
  /**
   * S3 bucket name to access.
   */
  bucket_name: string
  /**
   * The endpoint URL used to communicate with this service. Can be used to make this connector
   * talk to non-AWS services with an S3 API.
   */
  endpoint_url?: string | null
  /**
   * Read a single object specified by a key.
   */
  key?: string | null
  /**
   * Do not sign requests. This is equivalent to the `--no-sign-request` flag in the AWS CLI.
   */
  no_sign_request?: boolean
  /**
   * Read all objects whose keys match a prefix. Set to an empty string to read all objects in the bucket.
   */
  prefix?: string | null
  /**
   * AWS region.
   */
  region: string
}

export type SourcePosition = {
  end_column: number
  end_line_number: number
  start_column: number
  start_line_number: number
}

/**
 * SQL compilation information.
 */
export type SqlCompilationInfo = {
  /**
   * Exit code of the SQL compiler.
   */
  exit_code: number
  /**
   * Messages (warnings and errors) generated by the SQL compiler.
   */
  messages: Array<SqlCompilerMessage>
}

/**
 * A SQL compiler error.
 *
 * The SQL compiler returns a list of errors in the following JSON format if
 * it's invoked with the `-je` option.
 *
 * ```ignore
 * [ {
 * "start_line_number" : 2,
 * "start_column" : 4,
 * "end_line_number" : 2,
 * "end_column" : 8,
 * "warning" : false,
 * "error_type" : "PRIMARY KEY cannot be nullable",
 * "message" : "PRIMARY KEY column 'C' has type INTEGER, which is nullable",
 * "snippet" : "    2|   c INT PRIMARY KEY\n         ^^^^^\n    3|);\n"
 * } ]
 * ```
 */
export type SqlCompilerMessage = {
  end_column: number
  end_line_number: number
  error_type: string
  message: string
  snippet?: string | null
  start_column: number
  start_line_number: number
  warning: boolean
}

/**
 * An SQL identifier.
 *
 * This struct is used to represent SQL identifiers in a canonical form.
 * We store table names or field names as identifiers in the schema.
 */
export type SqlIdentifier = {
  case_sensitive: boolean
  name: string
}

/**
 * The available SQL types as specified in `CREATE` statements.
 */
export type SqlType =
  | 'Boolean'
  | 'TinyInt'
  | 'SmallInt'
  | 'Int'
  | 'BigInt'
  | 'Real'
  | 'Double'
  | 'Decimal'
  | 'Char'
  | 'Varchar'
  | 'Binary'
  | 'Varbinary'
  | 'Time'
  | 'Date'
  | 'Timestamp'
  | {
      Interval: IntervalUnit
    }
  | 'Array'
  | 'Struct'
  | 'Map'
  | 'Null'
  | 'Uuid'
  | 'Variant'

/**
 * Backend storage configuration.
 */
export type StorageBackendConfig =
  | {
      name: 'default'
    }
  | {
      config: ObjectStorageConfig
      name: 'object'
    }

export type name = 'default'

/**
 * How to cache access to storage within a Feldera pipeline.
 */
export type StorageCacheConfig = 'page_cache' | 'feldera_cache'

/**
 * Storage compression algorithm.
 */
export type StorageCompression = 'default' | 'none' | 'snappy'

/**
 * Configuration for persistent storage in a [`PipelineConfig`].
 */
export type StorageConfig = {
  cache?: StorageCacheConfig
  /**
   * A directory to keep pipeline state, as a path on the filesystem of the
   * machine or container where the pipeline will run.
   *
   * When storage is enabled, this directory stores the data for
   * [StorageBackendConfig::Default].
   *
   * When fault tolerance is enabled, this directory stores checkpoints and
   * the log.
   */
  path: string
}

/**
 * Storage configuration for a pipeline.
 */
export type StorageOptions = {
  backend?: StorageBackendConfig
  /**
   * The maximum size of the in-memory storage cache, in MiB.
   *
   * If set, the specified cache size is spread across all the foreground and
   * background threads. If unset, each foreground or background thread cache
   * is limited to 256 MiB.
   */
  cache_mib?: number | null
  compression?: StorageCompression
  /**
   * For a batch of data passed through the pipeline during a single step,
   * the minimum estimated number of bytes to write it to storage.
   *
   * This is provided for debugging and fine-tuning and should ordinarily be
   * left unset.  If it is set, it should ordinarily be greater than or equal
   * to `min_storage_bytes`.
   *
   * A value of 0 will write even empty batches to storage, and nonzero
   * values provide a threshold.  `usize::MAX` would effectively disable
   * storage for such batches.  The default is 10,485,760 (10 MiB).
   */
  min_step_storage_bytes?: number | null
  /**
   * For a batch of data maintained as a persistent index during a pipeline
   * run, the minimum estimated number of bytes to write it to storage.
   *
   * This is provided for debugging and fine-tuning and should ordinarily be
   * left unset.
   *
   * A value of 0 will write even empty batches to storage, and nonzero
   * values provide a threshold.  `usize::MAX` would effectively disable
   * storage for such batches.  The default is 1,048,576 (1 MiB).
   */
  min_storage_bytes?: number | null
}

/**
 * Transport-specific endpoint configuration passed to
 * `crate::OutputTransport::new_endpoint`
 * and `crate::InputTransport::new_endpoint`.
 */
export type TransportConfig =
  | {
      config: FileInputConfig
      name: 'file_input'
    }
  | {
      config: FileOutputConfig
      name: 'file_output'
    }
  | {
      config: KafkaInputConfig
      name: 'kafka_input'
    }
  | {
      config: KafkaOutputConfig
      name: 'kafka_output'
    }
  | {
      config: PubSubInputConfig
      name: 'pub_sub_input'
    }
  | {
      config: UrlInputConfig
      name: 'url_input'
    }
  | {
      config: S3InputConfig
      name: 's3_input'
    }
  | {
      config: DeltaTableReaderConfig
      name: 'delta_table_input'
    }
  | {
      config: DeltaTableWriterConfig
      name: 'delta_table_output'
    }
  | {
      config: RedisOutputConfig
      name: 'redis_output'
    }
  | {
      config: IcebergReaderConfig
      name: 'iceberg_input'
    }
  | {
      config: PostgresReaderConfig
      name: 'postgres_input'
    }
  | {
      config: PostgresWriterConfig
      name: 'postgres_output'
    }
  | {
      config: DatagenInputConfig
      name: 'datagen'
    }
  | {
      config: NexmarkInputConfig
      name: 'nexmark'
    }
  | {
      config: HttpInputConfig
      name: 'http_input'
    }
  | {
      name: 'http_output'
    }
  | {
      config: AdHocInputConfig
      name: 'ad_hoc_input'
    }

export type name2 = 'file_input'

export type UpdateInformation = {
  /**
   * URL that navigates the user to instructions on how to update their deployment's version
   */
  instructions_url: string
  /**
   * Whether the current version matches the latest version
   */
  is_latest_version: boolean
  /**
   * Latest version corresponding to the edition
   */
  latest_version: string
  remind_schedule: DisplaySchedule
}

/**
 * Configuration for reading data from an HTTP or HTTPS URL with
 * `UrlInputTransport`.
 */
export type UrlInputConfig = {
  /**
   * URL.
   */
  path: string
  /**
   * Timeout before disconnection when paused, in seconds.
   *
   * If the pipeline is paused, or if the input adapter reads data faster
   * than the pipeline can process it, then the controller will pause the
   * input adapter. If the input adapter stays paused longer than this
   * timeout, it will drop the network connection to the server. It will
   * automatically reconnect when the input adapter starts running again.
   */
  pause_timeout?: number
}

/**
 * Version number.
 */
export type Version = number

export type GetConfigAuthenticationResponse = AuthProvider

export type GetConfigAuthenticationError = ErrorResponse

export type ListApiKeysResponse = Array<ApiKeyDescr>

export type ListApiKeysError = ErrorResponse

export type PostApiKeyData = {
  body: NewApiKeyRequest
}

export type PostApiKeyResponse = NewApiKeyResponse

export type PostApiKeyError = ErrorResponse

export type GetApiKeyData = {
  path: {
    /**
     * Unique API key name
     */
    api_key_name: string
  }
}

export type GetApiKeyResponse = ApiKeyDescr

export type GetApiKeyError = ErrorResponse

export type DeleteApiKeyData = {
  path: {
    /**
     * Unique API key name
     */
    api_key_name: string
  }
}

export type DeleteApiKeyResponse = unknown

export type DeleteApiKeyError = ErrorResponse

export type GetConfigResponse = Configuration

export type GetConfigError = ErrorResponse

export type GetConfigDemosResponse = Array<Demo>

export type GetConfigDemosError = ErrorResponse

export type GetMetricsResponse = Blob | File

export type GetMetricsError = unknown

export type ListPipelinesData = {
  query?: {
    /**
     * The `selector` parameter limits which fields are returned for a pipeline.
     * Limiting which fields is particularly handy for instance when frequently
     * monitoring over low bandwidth connections while being only interested
     * in pipeline status.
     */
    selector?: PipelineFieldSelector
  }
}

export type ListPipelinesResponse = Array<PipelineSelectedInfo>

export type ListPipelinesError = ErrorResponse

export type PostPipelineData = {
  body: PostPutPipeline
}

export type PostPipelineResponse = PipelineInfo

export type PostPipelineError = ErrorResponse

export type GetPipelineData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query?: {
    /**
     * The `selector` parameter limits which fields are returned for a pipeline.
     * Limiting which fields is particularly handy for instance when frequently
     * monitoring over low bandwidth connections while being only interested
     * in pipeline status.
     */
    selector?: PipelineFieldSelector
  }
}

export type GetPipelineResponse = PipelineSelectedInfo

export type GetPipelineError = ErrorResponse

export type PutPipelineData = {
  body: PostPutPipeline
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PutPipelineResponse = PipelineInfo

export type PutPipelineError = ErrorResponse

export type DeletePipelineData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type DeletePipelineResponse = unknown

export type DeletePipelineError = ErrorResponse

export type PatchPipelineData = {
  body: PatchPipeline
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PatchPipelineResponse = PipelineInfo

export type PatchPipelineError = ErrorResponse

export type CheckpointPipelineData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type CheckpointPipelineResponse = unknown

export type CheckpointPipelineError = ErrorResponse

export type GetPipelineCircuitProfileData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineCircuitProfileResponse = {
  [key: string]: unknown
}

export type GetPipelineCircuitProfileError = ErrorResponse

export type CompletionStatusData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query: {
    /**
     * Completion token returned by the '/ingress' or '/completion_status' endpoint.
     */
    completion_token: string
  }
}

export type CompletionStatusResponse2 = CompletionStatusResponse

export type CompletionStatusError = ErrorResponse

export type HttpOutputData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * SQL table name. Unquoted SQL names have to be capitalized. Quoted SQL names have to exactly match the case from the SQL program.
     */
    table_name: string
  }
  query: {
    /**
     * Set to `true` to group updates in this stream into JSON arrays (used in conjunction with `format=json`). The default value is `false`
     */
    array?: boolean | null
    /**
     * Apply backpressure on the pipeline when the HTTP client cannot receive data fast enough.
     * When this flag is set to false (the default), the HTTP connector drops data chunks if the client is not keeping up with its output.  This prevents a slow HTTP client from slowing down the entire pipeline.
     * When the flag is set to true, the connector waits for the client to receive each chunk and blocks the pipeline if the client cannot keep up.
     */
    backpressure?: boolean | null
    /**
     * Output data format, e.g., 'csv' or 'json'.
     */
    format: string
  }
}

export type HttpOutputResponse = Chunk

export type HttpOutputError = ErrorResponse

export type GetPipelineHeapProfileData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineHeapProfileResponse = Blob | File

export type GetPipelineHeapProfileError = ErrorResponse

export type HttpInputData = {
  /**
   * Input data in the specified format
   */
  body: string
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * SQL table name. Unquoted SQL names have to be capitalized. Quoted SQL names have to exactly match the case from the SQL program.
     */
    table_name: string
  }
  query: {
    /**
     * Set to `true` if updates in this stream are packaged into JSON arrays (used in conjunction with `format=json`). The default values is `false`.
     */
    array?: boolean | null
    /**
     * When `true`, push data to the pipeline even if the pipeline is paused. The default value is `false`
     */
    force: boolean
    /**
     * Input data format, e.g., 'csv' or 'json'.
     */
    format: string
    /**
     * JSON data change event format (used in conjunction with `format=json`).  The default value is 'insert_delete'.
     */
    update_format?: JsonUpdateFormat | null
  }
}

export type HttpInputResponse = CompletionTokenResponse

export type HttpInputError = ErrorResponse

export type GetPipelineLogsData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineLogsResponse = Blob | File

export type GetPipelineLogsError = ErrorResponse

export type GetPipelineMetricsData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query?: {
    format?: MetricsFormat
  }
}

export type GetPipelineMetricsResponse = {
  [key: string]: unknown
}

export type GetPipelineMetricsError = ErrorResponse

export type GetProgramInfoData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetProgramInfoResponse = ProgramInfo

export type GetProgramInfoError = ErrorResponse

export type PipelineAdhocSqlData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
  query: {
    /**
     * Input data format, e.g., 'text', 'json' or 'parquet'
     */
    format: AdHocResultFormat
    /**
     * SQL query to execute
     */
    sql: string
  }
}

export type PipelineAdhocSqlResponse = Blob | File

export type PipelineAdhocSqlError = ErrorResponse

export type GetPipelineStatsData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineStatsResponse = {
  [key: string]: unknown
}

export type GetPipelineStatsError = ErrorResponse

export type CompletionTokenData = {
  path: {
    /**
     * Unique input connector name
     */
    connector_name: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * SQL table name. Unquoted SQL names have to be capitalized. Quoted SQL names have to exactly match the case from the SQL program.
     */
    table_name: string
  }
}

export type CompletionTokenResponse2 = CompletionTokenResponse

export type CompletionTokenError = ErrorResponse

export type GetPipelineInputConnectorStatusData = {
  path: {
    /**
     * Unique input connector name
     */
    connector_name: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * Unique table name
     */
    table_name: string
  }
}

export type GetPipelineInputConnectorStatusResponse = {
  [key: string]: unknown
}

export type GetPipelineInputConnectorStatusError = ErrorResponse

export type PostPipelineInputConnectorActionData = {
  path: {
    /**
     * Input connector action (one of: start, pause)
     */
    action: string
    /**
     * Unique input connector name
     */
    connector_name: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * Unique table name
     */
    table_name: string
  }
}

export type PostPipelineInputConnectorActionResponse = unknown

export type PostPipelineInputConnectorActionError = ErrorResponse

export type GetPipelineOutputConnectorStatusData = {
  path: {
    /**
     * Unique output connector name
     */
    connector_name: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * Unique SQL view name
     */
    view_name: string
  }
}

export type GetPipelineOutputConnectorStatusResponse = {
  [key: string]: unknown
}

export type GetPipelineOutputConnectorStatusError = ErrorResponse

export type PostPipelineActionData = {
  path: {
    /**
     * Pipeline action (one of: start, pause, shutdown)
     */
    action: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PostPipelineActionResponse = unknown

export type PostPipelineActionError = ErrorResponse

export type $OpenApiTs = {
  '/config/authentication': {
    get: {
      res: {
        /**
         * The response body contains Authentication Provider configuration, or is empty if no auth is configured.
         */
        '200': AuthProvider
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/api_keys': {
    get: {
      res: {
        /**
         * API keys retrieved successfully
         */
        '200': Array<ApiKeyDescr>
        '500': ErrorResponse
      }
    }
    post: {
      req: PostApiKeyData
      res: {
        /**
         * API key created successfully
         */
        '201': NewApiKeyResponse
        /**
         * API key with that name already exists
         */
        '409': ErrorResponse
      }
    }
  }
  '/v0/api_keys/{api_key_name}': {
    get: {
      req: GetApiKeyData
      res: {
        /**
         * API key retrieved successfully
         */
        '200': ApiKeyDescr
        /**
         * API key with that name does not exist
         */
        '404': ErrorResponse
      }
    }
    delete: {
      req: DeleteApiKeyData
      res: {
        /**
         * API key deleted successfully
         */
        '200': unknown
        /**
         * API key with that name does not exist
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/config': {
    get: {
      res: {
        /**
         * The response body contains basic configuration information about this host.
         */
        '200': Configuration
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/config/demos': {
    get: {
      res: {
        /**
         * List of demos
         */
        '200': Array<Demo>
        /**
         * Failed to read demos from the demos directories
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/metrics': {
    get: {
      res: {
        /**
         * Metrics of all running pipelines belonging to this tenant in Prometheus format
         */
        '200': Blob | File
      }
    }
  }
  '/v0/pipelines': {
    get: {
      req: ListPipelinesData
      res: {
        /**
         * List of pipelines retrieved successfully
         */
        '200': Array<PipelineSelectedInfo>
        '500': ErrorResponse
      }
    }
    post: {
      req: PostPipelineData
      res: {
        /**
         * Pipeline successfully created
         */
        '201': PipelineInfo
        '400': ErrorResponse
        /**
         * Cannot create pipeline as the name already exists
         */
        '409': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}': {
    get: {
      req: GetPipelineData
      res: {
        /**
         * Pipeline retrieved successfully
         */
        '200': PipelineSelectedInfo
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
    put: {
      req: PutPipelineData
      res: {
        /**
         * Pipeline successfully updated
         */
        '200': PipelineInfo
        '400': ErrorResponse
        /**
         * Cannot rename pipeline as the new name already exists
         */
        '409': ErrorResponse
        '500': ErrorResponse
      }
    }
    delete: {
      req: DeletePipelineData
      res: {
        /**
         * Pipeline successfully deleted
         */
        '200': unknown
        /**
         * Pipeline needs to be shutdown to be deleted
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
    patch: {
      req: PatchPipelineData
      res: {
        /**
         * Pipeline successfully updated
         */
        '200': PipelineInfo
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        /**
         * Cannot rename pipeline as the name already exists
         */
        '409': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/checkpoint': {
    post: {
      req: CheckpointPipelineData
      res: {
        /**
         * Checkpoint completed
         */
        '200': unknown
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/circuit_profile': {
    get: {
      req: GetPipelineCircuitProfileData
      res: {
        /**
         * Circuit performance profile
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/completion_status': {
    get: {
      req: CompletionStatusData
      res: {
        /**
         * The pipeline has finished processing inputs associated with the provided completion token.
         */
        '200': CompletionStatusResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        /**
         * Completion token was created by a previous incarnation of the pipeline and is not valid for the current incarnation. This indicates that the pipeline was suspended and resumed from a checkpoint or restarted after a failure.
         */
        '410': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/egress/{table_name}': {
    post: {
      req: HttpOutputData
      res: {
        /**
         * Connection to the endpoint successfully established. The body of the response contains a stream of data chunks.
         */
        '200': Chunk
        '400': ErrorResponse
        /**
         * Pipeline and/or table/view with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/heap_profile': {
    get: {
      req: GetPipelineHeapProfileData
      res: {
        /**
         * Heap usage profile as a gzipped protobuf that can be inspected by the pprof tool
         */
        '200': Blob | File
        /**
         * Getting a heap profile is not supported on this platform
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/ingress/{table_name}': {
    post: {
      req: HttpInputData
      res: {
        /**
         * Data successfully delivered to the pipeline. The body of the response contains a completion token that can be passed to the '/completion_status' endpoint to check whether the pipeline has fully processed the data.
         */
        '200': CompletionTokenResponse
        '400': ErrorResponse
        /**
         * Pipeline and/or table with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/logs': {
    get: {
      req: GetPipelineLogsData
      res: {
        /**
         * Pipeline logs retrieved successfully
         */
        '200': Blob | File
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/metrics': {
    get: {
      req: GetPipelineMetricsData
      res: {
        /**
         * Pipeline circuit metrics retrieved successfully
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/program_info': {
    get: {
      req: GetProgramInfoData
      res: {
        /**
         * Pipeline retrieved successfully
         */
        '200': ProgramInfo
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/query': {
    get: {
      req: PipelineAdhocSqlData
      res: {
        /**
         * Ad-hoc SQL query result
         */
        '200': Blob | File
        /**
         * Invalid SQL query
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/stats': {
    get: {
      req: GetPipelineStatsData
      res: {
        /**
         * Pipeline statistics retrieved successfully
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/tables/{table_name}/connectors/{connector_name}/completion_token': {
    get: {
      req: CompletionTokenData
      res: {
        /**
         * Completion token that can be passed to the '/completion_status' endpoint.
         */
        '200': CompletionTokenResponse
        /**
         * Specified pipeline, table, or connector does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/tables/{table_name}/connectors/{connector_name}/stats': {
    get: {
      req: GetPipelineInputConnectorStatusData
      res: {
        /**
         * Input connector status retrieved successfully
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline, table and/or input connector with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/tables/{table_name}/connectors/{connector_name}/{action}': {
    post: {
      req: PostPipelineInputConnectorActionData
      res: {
        /**
         * Action has been processed
         */
        '200': unknown
        /**
         * Pipeline, table and/or input connector with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/views/{view_name}/connectors/{connector_name}/stats': {
    get: {
      req: GetPipelineOutputConnectorStatusData
      res: {
        /**
         * Output connector status retrieved successfully
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Pipeline, view and/or output connector with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
        '503': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/{action}': {
    post: {
      req: PostPipelineActionData
      res: {
        /**
         * Action is accepted and is being performed
         */
        '202': unknown
        /**
         * Action could not be performed
         */
        '400': ErrorResponse
        /**
         * Pipeline with that name does not exist
         */
        '404': ErrorResponse
        '500': ErrorResponse
      }
    }
  }
}
