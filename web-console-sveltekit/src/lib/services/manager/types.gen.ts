// This file is auto-generated by @hey-api/openapi-ts

/**
 * ApiKey descriptor.
 */
export type ApiKeyDescr = {
  id: ApiKeyId
  name: string
  scopes: Array<ApiPermission>
}

/**
 * ApiKey ID.
 */
export type ApiKeyId = string

/**
 * Permission types for invoking pipeline manager APIs
 */
export type ApiPermission = 'Read' | 'Write'

/**
 * Format to add attached connectors during a config update.
 */
export type AttachedConnector = {
  /**
   * The name of the connector to attach.
   */
  connector_name: string
  /**
   * True for input connectors, false for output connectors.
   */
  is_input: boolean
  /**
   * A unique identifier for this attachement.
   */
  name: string
  /**
   * The table or view this connector is attached to. Unquoted
   * table/view names in the SQL program need to be capitalized
   * here. Quoted table/view names have to exactly match the
   * casing from the SQL program.
   */
  relation_name: string
}

/**
 * Unique attached connector id.
 */
export type AttachedConnectorId = string

export type AuthProvider =
  | {
      AwsCognito: ProviderAwsCognito
    }
  | {
      GoogleIdentity: ProviderGoogleIdentity
    }

/**
 * Configuration to authenticate against AWS
 */
export type AwsCredentials =
  | {
      type: 'NoSignRequest'
    }
  | {
      aws_access_key_id: string
      aws_secret_access_key: string
      type: 'AccessKey'
    }

export type type = 'NoSignRequest'

/**
 * A set of updates to a SQL table or view.
 *
 * The `sequence_number` field stores the offset of the chunk relative to the
 * start of the stream and can be used to implement reliable delivery.
 * The payload is stored in the `bin_data`, `text_data`, or `json_data` field
 * depending on the data format used.
 */
export type Chunk = {
  /**
   * Base64 encoded binary payload, e.g., bincode.
   */
  bin_data?: (Blob | File) | null
  /**
   * JSON payload.
   */
  json_data?: {
    [key: string]: unknown
  } | null
  sequence_number: number
  /**
   * Text payload, e.g., CSV.
   */
  text_data?: string | null
}

/**
 * A SQL column type description.
 *
 * Matches the Calcite JSON format.
 */
export type ColumnType = {
  component?: ColumnType | null
  /**
   * The fields of the type (if available).
   *
   * For example this would specify the fields of a `CREATE TYPE` construct.
   *
   * ```sql
   * CREATE TYPE person_typ AS (
   * firstname       VARCHAR(30),
   * lastname        VARCHAR(30),
   * address         ADDRESS_TYP
   * );
   * ```
   *
   * Would lead to the following `fields` value:
   *
   * ```sql
   * [
   * ColumnType { name: "firstname, ... },
   * ColumnType { name: "lastname", ... },
   * ColumnType { name: "address", fields: [ ... ] }
   * ]
   * ```
   */
  fields?: Array<Field> | null
  /**
   * Does the type accept NULL values?
   */
  nullable: boolean
  /**
   * Precision of the type.
   *
   * # Examples
   * - `VARCHAR` sets precision to `-1`.
   * - `VARCHAR(255)` sets precision to `255`.
   * - `BIGINT`, `DATE`, `FLOAT`, `DOUBLE`, `GEOMETRY`, etc. sets precision
   * to None
   * - `TIME`, `TIMESTAMP` set precision to `0`.
   */
  precision?: number | null
  /**
   * The scale of the type.
   *
   * # Example
   * - `DECIMAL(1,2)` sets scale to `2`.
   */
  scale?: number | null
  type?: SqlType
}

/**
 * Enumeration of possible compilation profiles that can be passed to the Rust compiler
 * as an argument via `cargo build --profile <>`. A compilation profile affects among
 * other things the compilation speed (how long till the program is ready to be run)
 * and runtime speed (the performance while running).
 */
export type CompilationProfile = 'dev' | 'unoptimized' | 'optimized'

/**
 * Request to queue a program for compilation.
 */
export type CompileProgramRequest = {
  version: Version
}

/**
 * A data connector's configuration
 */
export type ConnectorConfig = OutputBufferConfig & {
  format?: FormatConfig | null
  /**
   * Backpressure threshold.
   *
   * Maximal number of records queued by the endpoint before the endpoint
   * is paused by the backpressure mechanism.
   *
   * For input endpoints, this setting bounds the number of records that have
   * been received from the input transport but haven't yet been consumed by
   * the circuit since the circuit, since the circuit is still busy processing
   * previous inputs.
   *
   * For output endpoints, this setting bounds the number of records that have
   * been produced by the circuit but not yet sent via the output transport endpoint
   * nor stored in the output buffer (see `enable_output_buffer`).
   *
   * Note that this is not a hard bound: there can be a small delay between
   * the backpressure mechanism is triggered and the endpoint is paused, during
   * which more data may be queued.
   *
   * The default is 1 million.
   */
  max_queued_records?: number
  transport: TransportConfig
}

/**
 * Connector descriptor.
 */
export type ConnectorDescr = {
  config: ConnectorConfig
  connector_id: ConnectorId
  description: string
  name: string
}

/**
 * Unique connector id.
 */
export type ConnectorId = string

/**
 * Strategy to feed a fetched object into an InputConsumer.
 */
export type ConsumeStrategy =
  | {
      type: 'Fragment'
    }
  | {
      type: 'Object'
    }

export type type2 = 'Fragment'

/**
 * Request to create or replace a connector.
 */
export type CreateOrReplaceConnectorRequest = {
  config: ConnectorConfig
  /**
   * New connector description.
   */
  description: string
}

/**
 * Response to a create or replace connector request.
 */
export type CreateOrReplaceConnectorResponse = {
  connector_id: ConnectorId
}

/**
 * Request to create or replace an existing pipeline.
 */
export type CreateOrReplacePipelineRequest = {
  config: RuntimeConfig
  /**
   * Attached connectors.
   */
  connectors?: Array<AttachedConnector> | null
  /**
   * Pipeline description.
   */
  description: string
  /**
   * Name of the program to create a pipeline for.
   */
  program_name?: string | null
}

/**
 * Response to a pipeline create or replace request.
 */
export type CreateOrReplacePipelineResponse = {
  pipeline_id: PipelineId
  version: Version
}

/**
 * Request to create or replace a program.
 */
export type CreateOrReplaceProgramRequest = {
  /**
   * SQL code of the program.
   */
  code: string
  config?: ProgramConfig
  /**
   * Program description.
   */
  description: string
}

/**
 * Response to a create or replace program request.
 */
export type CreateOrReplaceProgramResponse = {
  program_id: ProgramId
  version: Version
}

/**
 * Request to create or replace a service.
 */
export type CreateOrReplaceServiceRequest = {
  config: ServiceConfig
  /**
   * Service description.
   */
  description: string
}

/**
 * Response to a create or replace service request.
 */
export type CreateOrReplaceServiceResponse = {
  service_id: ServiceId
}

/**
 * Response to a create service probe request.
 */
export type CreateServiceProbeResponse = {
  service_probe_id: ServiceProbeId
}

export type CsvEncoderConfig = {
  buffer_size_records?: number
}

export type CsvParserConfig = {
  [key: string]: unknown
}

/**
 * Delta table read mode.
 *
 * Three options are available:
 *
 * * `snapshot` - read a snapshot of the table and stop.
 *
 * * `follow` - continuously ingest changes to the table, starting from a specified version
 * or timestamp.
 *
 * * `snapshot_and_follow` - read a snapshot of the table before switching to continuous ingestion
 * mode.
 */
export type DeltaTableIngestMode = 'snapshot' | 'follow' | 'snapshot_and_follow'

/**
 * Delta table output connector configuration.
 */
export type DeltaTableReaderConfig = {
  /**
   * Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
   * "2024-12-09T16:09:53+00:00.
   *
   * When this option is set, the connector finds and opens the version of the table as of the
   * specified point in time.  In `snapshot` and `snapshot_and_follow` modes, it retrieves the
   * snapshot of this version of the table (based on the server time recorded in the transaction
   * log, not the event time encoded in the data).  In `follow` and `snapshot_and_follow` modes, it
   * follows transaction log records **after** this version.
   *
   * Note: at most one of `version` and `datetime` options can be specified.
   * When neither of the two options is specified, the latest committed version of the table
   * is used.
   */
  datetime?: string | null
  mode: DeltaTableIngestMode
  /**
   * Optional row filter.
   *
   * This option is only valid when `mode` is set to `snapshot` or `snapshot_and_follow`.
   *
   * When specified, only rows that satisfy the filter condition are included in the
   * snapshot.  The condition must be a valid SQL Boolean expression that can be used in
   * the `where` clause of the `select * from snapshot where ...` query.
   *
   * This option can be used to specify the range of event times to include in the snapshot,
   * e.g.: `ts BETWEEN '2005-01-01 00:00:00' AND '2010-12-31 23:59:59'`.
   */
  snapshot_filter?: string | null
  /**
   * Table column that serves as an event timestamp.
   *
   *
   * When this option is specified, and `mode` is one of `snapshot` or `snapshot_and_follow`,
   * the snapshot of the table is ingested in the timestamp order.  This setting is required
   * for tables declared with the
   * [`LATENESS`](https://www.feldera.com/docs/sql/streaming#lateness-expressions) attribute
   * in Feldera SQL. It impacts the performance of the connector, since data must be sorted
   * before pushing it to the pipeline; therefore it is not recommended to use this
   * settings for tables without `LATENESS`.
   */
  timestamp_column?: string | null
  /**
   * Table URI.
   *
   * Example: "s3://feldera-fraud-detection-data/demographics_train"
   */
  uri: string
  /**
   * Optional table version.
   *
   * When this option is set, the connector finds and opens the specified version of the table.
   * In `snapshot` and `snapshot_and_follow` modes, it retrieves the snapshot of this version of
   * the table.  In `follow` and `snapshot_and_follow` modes, it follows transaction log records
   * **after** this version.
   *
   * Note: at most one of `version` and `datetime` options can be specified.
   * When neither of the two options is specified, the latest committed version of the table
   * is used.
   */
  version?: number | null
  /**
   * Storage options for configuring backend object store.
   *
   * For specific options available for different storage backends, see:
   * * [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
   * * [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
   * * [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
   */
  '[key: string]': (string | DeltaTableIngestMode | number) | undefined
}

/**
 * Delta table write mode.
 *
 * Determines how the Delta table connector handles an existing table at the target location.
 */
export type DeltaTableWriteMode = 'append' | 'truncate' | 'error_if_exists'

/**
 * Delta table output connector configuration.
 */
export type DeltaTableWriterConfig = {
  mode?: DeltaTableWriteMode
  /**
   * Table URI.
   */
  uri: string
  /**
   * Storage options for configuring backend object store.
   *
   * For specific options available for different storage backends, see:
   * * [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
   * * [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
   * * [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)
   */
  '[key: string]': (string | DeltaTableWriteMode) | undefined
}

export type EgressMode = 'watch' | 'snapshot'

/**
 * Information returned by REST API endpoints on error.
 */
export type ErrorResponse = {
  /**
   * Detailed error metadata.
   * The contents of this field is determined by `error_code`.
   */
  details: {
    [key: string]: unknown
  }
  /**
   * Error code is a string that specifies this error type.
   */
  error_code: string
  /**
   * Human-readable error message.
   */
  message: string
}

/**
 * A SQL field.
 *
 * Matches the SQL compiler JSON format.
 */
export type Field = {
  case_sensitive?: boolean
  columntype: ColumnType
  name: string
}

/**
 * Configuration for reading data from a file with `FileInputTransport`
 */
export type FileInputConfig = {
  /**
   * Read buffer size.
   *
   * Default: when this parameter is not specified, a platform-specific
   * default is used.
   */
  buffer_size_bytes?: number | null
  /**
   * Enable file following.
   *
   * When `false`, the endpoint outputs an `InputConsumer::eoi`
   * message and stops upon reaching the end of file.  When `true`, the
   * endpoint will keep watching the file and outputting any new content
   * appended to it.
   */
  follow?: boolean
  /**
   * File path.
   */
  path: string
}

/**
 * Configuration for writing data to a file with `FileOutputTransport`.
 */
export type FileOutputConfig = {
  /**
   * File path.
   */
  path: string
}

/**
 * Data format specification used to parse raw data received from the
 * endpoint or to encode data sent to the endpoint.
 */
export type FormatConfig = {
  /**
   * Format-specific parser or encoder configuration.
   */
  config?: {
    [key: string]: unknown
  }
  /**
   * Format name, e.g., "csv", "json", "bincode", etc.
   */
  name: string
}

/**
 * Describes an input connector configuration
 */
export type InputEndpointConfig = ConnectorConfig & {
  /**
   * The name of the input stream of the circuit that this endpoint is
   * connected to.
   */
  stream: string
}

/**
 * The specified units for SQL Interval types.
 *
 * `INTERVAL 1 DAY`, `INTERVAL 1 DAY TO HOUR`, `INTERVAL 1 DAY TO MINUTE`,
 * would yield `Day`, `DayToHour`, `DayToMinute`, as the `IntervalUnit` respectively.
 */
export type IntervalUnit =
  | 'DAY'
  | 'DAYTOHOUR'
  | 'DAYTOMINUTE'
  | 'DAYTOSECOND'
  | 'HOUR'
  | 'HOURTOMINUTE'
  | 'HOURTOSECOND'
  | 'MINUTE'
  | 'MINUTETOSECOND'
  | 'MONTH'
  | 'SECOND'
  | 'YEAR'
  | 'YEARTOMONTH'

export type JsonEncoderConfig = {
  array?: boolean
  buffer_size_records?: number
  json_flavor?: JsonFlavor | null
  update_format?: JsonUpdateFormat
}

/**
 * Specifies JSON encoding used of table records.
 */
export type JsonFlavor =
  | 'default'
  | 'debezium_mysql'
  | 'snowflake'
  | 'kafka_connect_json_converter'
  | 'pandas'

/**
 * JSON parser configuration.
 *
 * Describes the shape of an input JSON stream.
 *
 * # Examples
 *
 * A configuration with `update_format="raw"` and `array=false`
 * is used to parse a stream of JSON objects without any envelope
 * that get inserted in the input table.
 *
 * ```json
 * {"b": false, "i": 100, "s": "foo"}
 * {"b": true, "i": 5, "s": "bar"}
 * ```
 *
 * A configuration with `update_format="insert_delete"` and
 * `array=false` is used to parse a stream of JSON data change events
 * in the insert/delete format:
 *
 * ```json
 * {"delete": {"b": false, "i": 15, "s": ""}}
 * {"insert": {"b": false, "i": 100, "s": "foo"}}
 * ```
 *
 * A configuration with `update_format="insert_delete"` and
 * `array=true` is used to parse a stream of JSON arrays
 * where each array contains multiple data change events in
 * the insert/delete format.
 *
 * ```json
 * [{"insert": {"b": true, "i": 0}}, {"delete": {"b": false, "i": 100, "s": "foo"}}]
 * ```
 */
export type JsonParserConfig = {
  /**
   * Set to `true` if updates in this stream are packaged into JSON arrays.
   *
   * # Example
   *
   * ```json
   * [{"b": true, "i": 0},{"b": false, "i": 100, "s": "foo"}]
   * ```
   */
  array?: boolean
  json_flavor?: JsonFlavor
  update_format?: JsonUpdateFormat
}

/**
 * Supported JSON data change event formats.
 *
 * Each element in a JSON-formatted input stream specifies
 * an update to one or more records in an input table.  We support
 * several different ways to represent such updates.
 */
export type JsonUpdateFormat = 'insert_delete' | 'weighted' | 'debezium' | 'snowflake' | 'raw'

/**
 * Kafka message header.
 */
export type KafkaHeader = {
  key: string
  value?: KafkaHeaderValue | null
}

/**
 * Kafka header value encoded as a UTF-8 string or a byte array.
 */
export type KafkaHeaderValue = Blob | File

/**
 * Configuration for reading data from Kafka topics with `InputTransport`.
 */
export type KafkaInputConfig = {
  fault_tolerance?: KafkaInputFtConfig | null
  /**
   * Maximum timeout in seconds to wait for the endpoint to join the Kafka
   * consumer group during initialization.
   */
  group_join_timeout_secs?: number
  /**
   * If specified, this service is used to provide defaults for the Kafka options.
   */
  kafka_service?: string | null
  log_level?: KafkaLogLevel | null
  /**
   * List of topics to subscribe to.
   */
  topics: Array<string>
  /**
   * Options passed directly to `rdkafka`.
   *
   * [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
   * used to configure the Kafka consumer.  Not all options are valid with
   * this Kafka adapter:
   *
   * * "enable.auto.commit", if present, must be set to "false",
   * * "enable.auto.offset.store", if present, must be set to "false"
   */
  '[key: string]': (string | unknown | number) | undefined
}

/**
 * Fault tolerance configuration for Kafka input connector.
 */
export type KafkaInputFtConfig = {
  /**
   * Options passed to `rdkafka` for consumers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for consumers, and may be empty.
   */
  consumer_options?: {
    [key: string]: string
  }
  /**
   * If this is true or unset, then the connector will create missing index
   * topics as needed.  If this is false, then a missing index topic is a
   * fatal error.
   */
  create_missing_index?: boolean | null
  /**
   * Suffix to append to each data topic name, to give the name of a topic
   * that the connector uses for recording the division of the corresponding
   * data topic into steps.  Defaults to `_input-index`.
   *
   * An index topic must have the same number of partitions as its
   * corresponding data topic.
   *
   * If two or more fault-tolerant Kafka endpoints read from overlapping sets
   * of topics, they must specify different `index_suffix` values.
   */
  index_suffix?: string | null
  /**
   * Maximum number of bytes in a step.  Any individual message bigger than
   * this will be given a step of its own.
   */
  max_step_bytes?: number | null
  /**
   * Maximum number of messages in a step.
   */
  max_step_messages?: number | null
  /**
   * Options passed to `rdkafka` for producers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for producers, and may be empty.
   */
  producer_options?: {
    [key: string]: string
  }
}

/**
 * Kafka logging levels.
 */
export type KafkaLogLevel =
  | 'emerg'
  | 'alert'
  | 'critical'
  | 'error'
  | 'warning'
  | 'notice'
  | 'info'
  | 'debug'

/**
 * Configuration for writing data to a Kafka topic with `OutputTransport`.
 */
export type KafkaOutputConfig = {
  fault_tolerance?: KafkaOutputFtConfig | null
  /**
   * Kafka headers to be added to each message produced by this connector.
   */
  headers?: Array<KafkaHeader>
  /**
   * Maximum timeout in seconds to wait for the endpoint to connect to
   * a Kafka broker.
   *
   * Defaults to 60.
   */
  initialization_timeout_secs?: number
  /**
   * If specified, this service is used to provide defaults for the Kafka options.
   */
  kafka_service?: string | null
  log_level?: KafkaLogLevel | null
  /**
   * Maximum number of unacknowledged messages buffered by the Kafka
   * producer.
   *
   * Kafka producer buffers outgoing messages until it receives an
   * acknowledgement from the broker.  This configuration parameter
   * bounds the number of unacknowledged messages.  When the number of
   * unacknowledged messages reaches this limit, sending of a new message
   * blocks until additional acknowledgements arrive from the broker.
   *
   * Defaults to 1000.
   */
  max_inflight_messages?: number
  /**
   * Topic to write to.
   */
  topic: string
  /**
   * Options passed directly to `rdkafka`.
   *
   * See [`librdkafka` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
   * used to configure the Kafka producer.
   */
  '[key: string]': (string | unknown | KafkaHeader | number) | undefined
}

/**
 * Fault tolerance configuration for Kafka output connector.
 */
export type KafkaOutputFtConfig = {
  /**
   * Options passed to `rdkafka` for consumers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for consumers, and may be empty.
   */
  consumer_options?: {
    [key: string]: string
  }
  /**
   * Options passed to `rdkafka` for producers only, as documented at
   * [`librdkafka`
   * options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
   *
   * These options override `kafka_options` for producers, and may be empty.
   */
  producer_options?: {
    [key: string]: string
  }
}

/**
 * Configuration for accessing a Kafka service.
 */
export type KafkaService = {
  /**
   * List of bootstrap servers, each formatted as hostname:port (e.g.,
   * "example.com:1234"). It will be used to set the `bootstrap.servers`
   * Kafka option.
   */
  bootstrap_servers: Array<string>
  /**
   * Additional Kafka options.
   *
   * Should not contain the bootstrap.servers key
   * as it is passed explicitly via its field.
   *
   * These options will likely encompass things
   * like SSL and authentication configuration.
   */
  options: {
    [key: string]: string
  }
}

/**
 * A request to output a specific neighborhood of a table or view.
 * The neighborhood is defined in terms of its central point (`anchor`)
 * and the number of rows preceding and following the anchor to output.
 */
export type NeighborhoodQuery = {
  after: number
  anchor?: {
    [key: string]: unknown
  } | null
  before: number
}

/**
 * Request to create a new API key.
 */
export type NewApiKeyRequest = {
  /**
   * Key name.
   */
  name: string
}

/**
 * Response to a successful API key creation.
 */
export type NewApiKeyResponse = {
  /**
   * Generated API key. There is no way to
   * retrieve this key again from the
   * pipeline-manager, so store it securely.
   */
  api_key: string
  api_key_id: ApiKeyId
  /**
   * API key name
   */
  name: string
}

/**
 * Request to create a new connector.
 */
export type NewConnectorRequest = {
  config: ConnectorConfig
  /**
   * Connector description.
   */
  description: string
  /**
   * Connector name.
   */
  name: string
}

/**
 * Response to a connector creation request.
 */
export type NewConnectorResponse = {
  connector_id: ConnectorId
}

/**
 * Request to create a new pipeline.
 */
export type NewPipelineRequest = {
  config: RuntimeConfig
  /**
   * Attached connectors.
   */
  connectors?: Array<AttachedConnector> | null
  /**
   * Pipeline description.
   */
  description: string
  /**
   * Unique pipeline name.
   */
  name: string
  /**
   * Name of the program to create a pipeline for.
   */
  program_name?: string | null
}

/**
 * Response to a pipeline creation request.
 */
export type NewPipelineResponse = {
  pipeline_id: PipelineId
  version: Version
}

/**
 * Request to create a new program.
 */
export type NewProgramRequest = {
  /**
   * SQL code of the program.
   */
  code: string
  config?: ProgramConfig
  /**
   * Program description.
   */
  description: string
  /**
   * Program name.
   */
  name: string
}

/**
 * Response to a new program request.
 */
export type NewProgramResponse = {
  program_id: ProgramId
  version: Version
}

/**
 * Request to create a new service.
 */
export type NewServiceRequest = {
  config: ServiceConfig
  /**
   * Service description.
   */
  description: string
  /**
   * Service name.
   */
  name: string
}

/**
 * Response to a service creation request.
 */
export type NewServiceResponse = {
  service_id: ServiceId
}

export type OutputBufferConfig = {
  /**
   * Enable output buffering.
   *
   * The output buffering mechanism allows decoupling the rate at which the pipeline
   * pushes changes to the output transport from the rate of input changes.
   *
   * By default, output updates produced by the pipeline are pushed directly to
   * the output transport. Some destinations may prefer to receive updates in fewer
   * bigger batches. For instance, when writing Parquet files, producing
   * one bigger file every few minutes is usually better than creating
   * small files every few milliseconds.
   *
   * To achieve such input/output decoupling, users can enable output buffering by
   * setting the `enable_output_buffer` flag to `true`.  When buffering is enabled, output
   * updates produced by the pipeline are consolidated in an internal buffer and are
   * pushed to the output transport when one of several conditions is satisfied:
   *
   * * data has been accumulated in the buffer for more than `max_output_buffer_time_millis`
   * milliseconds.
   * * buffer size exceeds `max_output_buffer_size_records` records.
   *
   * This flag is `false` by default.
   */
  enable_output_buffer?: boolean
  /**
   * Maximum number of updates to be kept in the output buffer.
   *
   * This parameter bounds the maximal size of the buffer.
   * Note that the size of the buffer is not always equal to the
   * total number of updates output by the pipeline. Updates to the
   * same record can overwrite or cancel previous updates.
   *
   * By default, the buffer can grow indefinitely until one of
   * the other output conditions is satisfied.
   *
   * NOTE: this configuration option requires the `enable_output_buffer` flag
   * to be set.
   */
  max_output_buffer_size_records?: number
  /**
   * Maximum time in milliseconds data is kept in the output buffer.
   *
   * By default, data is kept in the buffer indefinitely until one of
   * the other output conditions is satisfied.  When this option is
   * set the buffer will be flushed at most every
   * `max_output_buffer_time_millis` milliseconds.
   *
   * NOTE: this configuration option requires the `enable_output_buffer` flag
   * to be set.
   */
  max_output_buffer_time_millis?: number
}

/**
 * Describes an output connector configuration
 */
export type OutputEndpointConfig = ConnectorConfig & {
  /**
   * The name of the output stream of the circuit that this endpoint is
   * connected to.
   */
  stream: string
}

/**
 * A query over an output stream.
 *
 * We currently do not support ad hoc queries.  Instead the client can use
 * three pre-defined queries to inspect the contents of a table or view.
 */
export type OutputQuery = 'table' | 'neighborhood' | 'quantiles'

export type ParquetEncoderConfig = {
  /**
   * Number of records before a new parquet file is written.
   *
   * The default is 100_000.
   */
  buffer_size_records?: number
}

/**
 * Configuration for the parquet parser.
 */
export type ParquetParserConfig = {
  [key: string]: unknown
}

/**
 * State of a pipeline, including static configuration
 * and runtime status.
 */
export type Pipeline = {
  descriptor: PipelineDescr
  state: PipelineRuntimeState
}

/**
 * Pipeline configuration specified by the user when creating
 * a new pipeline instance.
 *
 * This is the shape of the overall pipeline configuration. It encapsulates a
 * [`RuntimeConfig`], which is the publicly exposed way for users to configure
 * pipelines.
 */
export type PipelineConfig = {
  /**
   * Enable CPU profiler.
   */
  cpu_profiler?: boolean
  /**
   * Maximal delay in microseconds to wait for `min_batch_size_records` to
   * get buffered by the controller, defaults to 0.
   */
  max_buffering_delay_usecs?: number
  /**
   * Minimal input batch size.
   *
   * The controller delays pushing input records to the circuit until at
   * least `min_batch_size_records` records have been received (total
   * across all endpoints) or `max_buffering_delay_usecs` microseconds
   * have passed since at least one input records has been buffered.
   * Defaults to 0.
   */
  min_batch_size_records?: number
  /**
   * The minimum estimated number of rows in a batch to write it to storage.
   * This is provided for debugging and fine-tuning and should ordinarily be
   * left unset. It only has an effect when `storage` is set to true.
   *
   * A value of 0 will write even empty batches to storage, and nonzero
   * values provide a threshold.  `usize::MAX` would effectively disable
   * storage.
   */
  min_storage_rows?: number | null
  resources?: ResourceConfig
  /**
   * Should persistent storage be enabled for this pipeline?
   *
   * - If `false` (default), the pipeline's state is kept in in-memory data-structures.
   * This is useful if the pipeline is ephemeral and does not need to be recovered
   * after a restart. The pipeline will most likely run faster since it does not
   * need to read from, or write to disk
   *
   * - If `true`, the pipeline state is stored in the specified location,
   * is persisted across restarts, and can be checkpointed and recovered.
   * This feature is currently experimental.
   */
  storage?: boolean
  /**
   * Enable the TCP metrics exporter.
   *
   * This is used for development purposes only.
   * If enabled, the `metrics-observer` CLI tool
   * can be used to inspect metrics from the pipeline.
   *
   * Because of how Rust metrics work, this is only honored for the first
   * pipeline to be instantiated within a given process.
   */
  tcp_metrics_exporter?: boolean
  /**
   * Number of DBSP worker threads.
   */
  workers?: number
} & {
  /**
   * Input endpoint configuration.
   */
  inputs: {
    [key: string]: InputEndpointConfig
  }
  /**
   * Pipeline name.
   */
  name?: string | null
  /**
   * Output endpoint configuration.
   */
  outputs?: {
    [key: string]: OutputEndpointConfig
  }
  storage_config?: StorageConfig | null
}

/**
 * Pipeline descriptor.
 */
export type PipelineDescr = {
  attached_connectors: Array<AttachedConnector>
  config: RuntimeConfig
  description: string
  name: string
  pipeline_id: PipelineId
  program_name?: string | null
  version: Version
}

/**
 * Unique pipeline id.
 */
export type PipelineId = string

/**
 * A pipeline revision is a versioned, immutable configuration struct that
 * contains all information necessary to run a pipeline.
 */
export type PipelineRevision = {
  config: PipelineConfig
  /**
   * The versioned connectors.
   */
  connectors: Array<ConnectorDescr>
  pipeline: PipelineDescr
  program: ProgramDescr
  revision: Revision
  /**
   * The versioned services for each connector.
   */
  services_for_connectors: Array<Array<ServiceDescr>>
}

/**
 * Runtime state of the pipeine.
 */
export type PipelineRuntimeState = {
  /**
   * Time when the pipeline started executing.
   */
  created: string
  current_status: PipelineStatus
  desired_status: PipelineStatus
  error?: ErrorResponse | null
  /**
   * Location where the pipeline can be reached at runtime.
   * e.g., a TCP port number or a URI.
   */
  location: string
  pipeline_id: PipelineId
  /**
   * Time when the pipeline was assigned its current status
   * of the pipeline.
   */
  status_since: string
}

/**
 * Pipeline status.
 *
 * This type represents the state of the pipeline tracked by the pipeline
 * runner and observed by the API client via the `GET /pipeline` endpoint.
 *
 * ### The lifecycle of a pipeline
 *
 * The following automaton captures the lifecycle of the pipeline.  Individual
 * states and transitions of the automaton are described below.
 *
 * * In addition to the transitions shown in the diagram, all states have an
 * implicit "forced shutdown" transition to the `Shutdown` state.  This
 * transition is triggered when the pipeline runner is unable to communicate
 * with the pipeline and thereby forces a shutdown.
 *
 * * States labeled with the hourglass symbol (⌛) are **timed** states.  The
 * automaton stays in timed state until the corresponding operation completes
 * or until the runner performs a forced shutdown of the pipeline after a
 * pre-defined timeout perioud.
 *
 * * State transitions labeled with API endpoint names (`/deploy`, `/start`,
 * `/pause`, `/shutdown`) are triggered by invoking corresponding endpoint,
 * e.g., `POST /v0/pipelines/{pipeline_id}/start`.
 *
 * ```text
 * Shutdown◄────┐
 * │         │
 * /deploy│         │
 * │   ⌛ShuttingDown
 * ▼         ▲
 * ⌛Provisioning    │
 * │         │
 * Provisioned        │         │
 * ▼         │/shutdown
 * ⌛Initializing    │
 * │         │
 * ┌────────┴─────────┴─┐
 * │        ▼           │
 * │      Paused        │
 * │      │    ▲        │
 * │/start│    │/pause  │
 * │      ▼    │        │
 * │     Running        │
 * └──────────┬─────────┘
 * │
 * ▼
 * Failed
 * ```
 *
 * ### Desired and actual status
 *
 * We use the desired state model to manage the lifecycle of a pipeline.
 * In this model, the pipeline has two status attributes associated with
 * it at runtime: the **desired** status, which represents what the user
 * would like the pipeline to do, and the **current** status, which
 * represents the actual state of the pipeline.  The pipeline runner
 * service continuously monitors both fields and steers the pipeline
 * towards the desired state specified by the user.
 * Only three of the states in the pipeline automaton above can be
 * used as desired statuses: `Paused`, `Running`, and `Shutdown`.
 * These statuses are selected by invoking REST endpoints shown
 * in the diagram.
 *
 * The user can monitor the current state of the pipeline via the
 * `/status` endpoint, which returns an object of type `Pipeline`.
 * In a typical scenario, the user first sets
 * the desired state, e.g., by invoking the `/deploy` endpoint, and
 * then polls the `GET /pipeline` endpoint to monitor the actual status
 * of the pipeline until its `state.current_status` attribute changes
 * to "paused" indicating that the pipeline has been successfully
 * initialized, or "failed", indicating an error.
 */
export type PipelineStatus =
  | 'Shutdown'
  | 'Provisioning'
  | 'Initializing'
  | 'Paused'
  | 'Running'
  | 'ShuttingDown'
  | 'Failed'

/**
 * Program configuration.
 */
export type ProgramConfig = {
  profile?: CompilationProfile | null
}

/**
 * Program descriptor.
 */
export type ProgramDescr = {
  /**
   * SQL code
   */
  code?: string | null
  config: ProgramConfig
  /**
   * Program description.
   */
  description: string
  /**
   * Program name (doesn't have to be unique).
   */
  name: string
  program_id: ProgramId
  schema?: ProgramSchema | null
  status: ProgramStatus
  version: Version
}

/**
 * Unique program id.
 */
export type ProgramId = string

/**
 * A struct containing the tables (inputs) and views for a program.
 *
 * Parse from the JSON data-type of the DDL generated by the SQL compiler.
 */
export type ProgramSchema = {
  inputs: Array<Relation>
  outputs: Array<Relation>
}

/**
 * Program compilation status.
 */
export type ProgramStatus =
  | 'Pending'
  | 'CompilingSql'
  | 'CompilingRust'
  | 'Success'
  | {
      /**
       * SQL compiler returned an error.
       */
      SqlError: Array<SqlCompilerMessage>
    }
  | {
      /**
       * Rust compiler returned an error.
       */
      RustError: string
    }
  | {
      /**
       * System/OS returned an error when trying to invoke commands.
       */
      SystemError: string
    }

export type ProviderAwsCognito = {
  jwk_uri: string
  login_url: string
  logout_url: string
}

export type ProviderGoogleIdentity = {
  client_id: string
  jwk_uri: string
}

/**
 * Strategy that determines which objects to read from a given bucket
 */
export type ReadStrategy =
  | {
      key: string
      type: 'SingleKey'
    }
  | {
      prefix: string
      type: 'Prefix'
    }

export type type3 = 'SingleKey'

/**
 * A SQL table or view. It has a name and a list of fields.
 *
 * Matches the Calcite JSON format.
 */
export type Relation = {
  case_sensitive?: boolean
  fields: Array<Field>
  name: string
}

export type ResourceConfig = {
  /**
   * The maximum number of CPU cores to reserve
   * for an instance of this pipeline
   */
  cpu_cores_max?: number | null
  /**
   * The minimum number of CPU cores to reserve
   * for an instance of this pipeline
   */
  cpu_cores_min?: number | null
  /**
   * The maximum memory in Megabytes to reserve
   * for an instance of this pipeline
   */
  memory_mb_max?: number | null
  /**
   * The minimum memory in Megabytes to reserve
   * for an instance of this pipeline
   */
  memory_mb_min?: number | null
  /**
   * Storage class to use for an instance of this pipeline.
   * The class determines storage performance such as IOPS and throughput.
   */
  storage_class?: string | null
  /**
   * The total storage in Megabytes to reserve
   * for an instance of this pipeline
   */
  storage_mb_max?: number | null
}

/**
 * Revision number.
 */
export type Revision = string

/**
 * Global pipeline configuration settings. This is the publicly
 * exposed type for users to configure pipelines.
 */
export type RuntimeConfig = {
  /**
   * Enable CPU profiler.
   */
  cpu_profiler?: boolean
  /**
   * Maximal delay in microseconds to wait for `min_batch_size_records` to
   * get buffered by the controller, defaults to 0.
   */
  max_buffering_delay_usecs?: number
  /**
   * Minimal input batch size.
   *
   * The controller delays pushing input records to the circuit until at
   * least `min_batch_size_records` records have been received (total
   * across all endpoints) or `max_buffering_delay_usecs` microseconds
   * have passed since at least one input records has been buffered.
   * Defaults to 0.
   */
  min_batch_size_records?: number
  /**
   * The minimum estimated number of rows in a batch to write it to storage.
   * This is provided for debugging and fine-tuning and should ordinarily be
   * left unset. It only has an effect when `storage` is set to true.
   *
   * A value of 0 will write even empty batches to storage, and nonzero
   * values provide a threshold.  `usize::MAX` would effectively disable
   * storage.
   */
  min_storage_rows?: number | null
  resources?: ResourceConfig
  /**
   * Should persistent storage be enabled for this pipeline?
   *
   * - If `false` (default), the pipeline's state is kept in in-memory data-structures.
   * This is useful if the pipeline is ephemeral and does not need to be recovered
   * after a restart. The pipeline will most likely run faster since it does not
   * need to read from, or write to disk
   *
   * - If `true`, the pipeline state is stored in the specified location,
   * is persisted across restarts, and can be checkpointed and recovered.
   * This feature is currently experimental.
   */
  storage?: boolean
  /**
   * Enable the TCP metrics exporter.
   *
   * This is used for development purposes only.
   * If enabled, the `metrics-observer` CLI tool
   * can be used to inspect metrics from the pipeline.
   *
   * Because of how Rust metrics work, this is only honored for the first
   * pipeline to be instantiated within a given process.
   */
  tcp_metrics_exporter?: boolean
  /**
   * Number of DBSP worker threads.
   */
  workers?: number
}

/**
 * Configuration for reading data from AWS S3.
 */
export type S3InputConfig = {
  /**
   * S3 bucket name to access
   */
  bucket_name: string
  consume_strategy?: ConsumeStrategy
  credentials: AwsCredentials
  read_strategy: ReadStrategy
  /**
   * AWS region
   */
  region: string
}

/**
 * Configuration for a Service, which typically includes how to establish a
 * connection (e.g., hostname, port) and authenticate (e.g., credentials).
 *
 * This configuration can be used to easily derive connectors for the service
 * as well as probe it for information.
 */
export type ServiceConfig = {
  kafka: KafkaService
}

/**
 * Service descriptor.
 */
export type ServiceDescr = {
  config: ServiceConfig
  config_type: string
  description: string
  name: string
  service_id: ServiceId
}

/**
 * Unique service id.
 */
export type ServiceId = string

/**
 * Service probe descriptor.
 */
export type ServiceProbeDescr = {
  created_at: string
  finished_at?: string | null
  probe_type: ServiceProbeType
  request: ServiceProbeRequest
  response?: ServiceProbeResponse | null
  service_probe_id: ServiceProbeId
  started_at?: string | null
  status: ServiceProbeStatus
}

/**
 * Range of possible errors that can occur during a service probe.
 * These are shared across all services.
 */
export type ServiceProbeError =
  | 'timeout_exceeded'
  | {
      unsupported_request: {
        probe_type: string
        service_type: string
      }
    }
  | {
      other: string
    }

/**
 * Unique service probe id.
 */
export type ServiceProbeId = string

/**
 * Enumeration of all possible service probe requests.
 */
export type ServiceProbeRequest = 'test_connectivity' | 'kafka_get_topics'

/**
 * Response being either success or error.
 */
export type ServiceProbeResponse =
  | {
      success: ServiceProbeResult
    }
  | {
      error: ServiceProbeError
    }

/**
 * Enumeration of all possible service probe success responses.
 */
export type ServiceProbeResult =
  | 'connected'
  | {
      /**
       * The names of all Kafka topics of the service.
       */
      kafka_topics: Array<string>
    }

/**
 * Service probe status.
 *
 * State transition diagram:
 * ```text
 * Pending
 * │
 * │ (Prober server picks up the probe)
 * │
 * ▼
 * ⌛Running ───► Failure
 * │
 * ▼
 * Success
 * ```
 */
export type ServiceProbeStatus = 'pending' | 'running' | 'success' | 'failure'

/**
 * Enumeration of all possible service probe types.
 * Each type maps to exactly one request variant.
 */
export type ServiceProbeType = 'test_connectivity' | 'kafka_get_topics'

/**
 * A SQL compiler error.
 *
 * The SQL compiler returns a list of errors in the following JSON format if
 * it's invoked with the `-je` option.
 *
 * ```ignore
 * [ {
 * "startLineNumber" : 14,
 * "startColumn" : 13,
 * "endLineNumber" : 14,
 * "endColumn" : 13,
 * "warning" : false,
 * "errorType" : "Error parsing SQL",
 * "message" : "Encountered \"<EOF>\" at line 14, column 13."
 * } ]
 * ```
 */
export type SqlCompilerMessage = {
  endColumn: number
  endLineNumber: number
  errorType: string
  message: string
  startColumn: number
  startLineNumber: number
  warning: boolean
}

/**
 * The available SQL types as specified in `CREATE` statements.
 */
export type SqlType =
  | 'BOOLEAN'
  | 'TINYINT'
  | 'SMALLINT'
  | 'INTEGER'
  | 'BIGINT'
  | 'REAL'
  | 'DOUBLE'
  | 'DECIMAL'
  | 'CHAR'
  | 'VARCHAR'
  | 'BINARY'
  | 'VARBINARY'
  | 'TIME'
  | 'DATE'
  | 'TIMESTAMP'
  | {
      Interval: IntervalUnit
    }
  | 'ARRAY'
  | 'STRUCT'
  | 'NULL'

/**
 * How to cache access to storage within a Feldera pipeline.
 */
export type StorageCacheConfig = 'page_cache' | 'feldera_cache'

/**
 * Configuration for persistent storage in a [`PipelineConfig`].
 */
export type StorageConfig = {
  cache: StorageCacheConfig
  /**
   * The location where the pipeline state is stored or will be stored.
   *
   * It should point to a path on the file-system of the machine/container
   * where the pipeline will run. If that path doesn't exist yet, or if it
   * does not contain any checkpoints, then the pipeline creates it and
   * starts from an initial state in which no data has yet been received. If
   * it does exist, then the pipeline starts from the most recent checkpoint
   * that already exists there. In either case, (further) checkpoints will be
   * written there.
   */
  path: string
}

export type TenantId = string

/**
 * Transport-specific endpoint configuration passed to
 * `crate::OutputTransport::new_endpoint`
 * and `crate::InputTransport::new_endpoint`.
 */
export type TransportConfig =
  | {
      config: FileInputConfig
      name: 'file_input'
    }
  | {
      config: FileOutputConfig
      name: 'file_output'
    }
  | {
      config: KafkaInputConfig
      name: 'kafka_input'
    }
  | {
      config: KafkaOutputConfig
      name: 'kafka_output'
    }
  | {
      config: UrlInputConfig
      name: 'url_input'
    }
  | {
      config: S3InputConfig
      name: 's3_input'
    }
  | {
      config: DeltaTableReaderConfig
      name: 'delta_table_input'
    }
  | {
      config: DeltaTableWriterConfig
      name: 'delta_table_output'
    }
  | {
      name: 'http_input'
    }
  | {
      name: 'http_output'
    }

export type name = 'file_input'

/**
 * Request to update an existing connector.
 */
export type UpdateConnectorRequest = {
  config?: ConnectorConfig | null
  /**
   * New connector description. If absent, existing name will be kept
   * unmodified.
   */
  description?: string | null
  /**
   * New connector name. If absent, existing name will be kept unmodified.
   */
  name?: string | null
}

/**
 * Response to a connector update request.
 */
export type UpdateConnectorResponse = {
  [key: string]: unknown
}

/**
 * Request to update an existing pipeline.
 */
export type UpdatePipelineRequest = {
  config?: RuntimeConfig | null
  /**
   * Attached connectors.
   *
   * - If absent, existing connectors will be kept unmodified.
   *
   * - If present all existing connectors will be replaced with the new
   * specified list.
   */
  connectors?: Array<AttachedConnector> | null
  /**
   * New pipeline description.
   */
  description: string
  /**
   * New pipeline name.
   */
  name: string
  /**
   * New program to create a pipeline for. If absent, program will be set to
   * NULL.
   */
  program_name?: string | null
}

/**
 * Response to a config update request.
 */
export type UpdatePipelineResponse = {
  version: Version
}

/**
 * Request to update an existing program.
 */
export type UpdateProgramRequest = {
  /**
   * New SQL code for the program. If absent, existing program code will be
   * kept unmodified.
   */
  code?: string | null
  config?: ProgramConfig | null
  /**
   * New program description. If absent, existing description will be kept
   * unmodified.
   */
  description?: string | null
  guard?: Version | null
  /**
   * New program name. If absent, existing name will be kept unmodified.
   */
  name?: string | null
}

/**
 * Response to a program update request.
 */
export type UpdateProgramResponse = {
  version: Version
}

/**
 * Request to update an existing service.
 */
export type UpdateServiceRequest = {
  config?: ServiceConfig | null
  /**
   * New service description. If absent, existing name will be kept
   * unmodified.
   */
  description?: string | null
  /**
   * New service name. If absent, existing name will be kept unmodified.
   */
  name?: string | null
}

/**
 * Response to a service update request.
 */
export type UpdateServiceResponse = {
  [key: string]: unknown
}

/**
 * Configuration for reading data from an HTTP or HTTPS URL with
 * `UrlInputTransport`.
 */
export type UrlInputConfig = {
  /**
   * URL.
   */
  path: string
}

/**
 * Version number.
 */
export type Version = number

export type GetAuthenticationConfigResponse = AuthProvider

export type GetAuthenticationConfigError = ErrorResponse

export type ListApiKeysData = {
  query?: {
    /**
     * API key name
     */
    name?: string | null
  }
}

export type ListApiKeysResponse = Array<ApiKeyDescr>

export type ListApiKeysError = ErrorResponse

export type CreateApiKeyData = {
  body: NewApiKeyRequest
}

export type CreateApiKeyResponse = NewApiKeyResponse

export type CreateApiKeyError = ErrorResponse

export type GetApiKeyData = {
  path: {
    /**
     * Unique API key name
     */
    api_key_name: string
  }
}

export type GetApiKeyResponse = ApiKeyDescr

export type GetApiKeyError = ErrorResponse

export type DeleteApiKeyData = {
  path: {
    /**
     * Unique API key name
     */
    api_key_name: string
  }
}

export type DeleteApiKeyResponse = unknown

export type DeleteApiKeyError = ErrorResponse

export type GetDemosResponse = Array<string>

export type GetDemosError = unknown

export type ListConnectorsData = {
  query?: {
    /**
     * Unique connector identifier.
     */
    id?: string | null
    /**
     * Unique connector name.
     */
    name?: string | null
  }
}

export type ListConnectorsResponse = Array<ConnectorDescr>

export type ListConnectorsError = ErrorResponse

export type NewConnectorData = {
  body: NewConnectorRequest
}

export type NewConnectorResponse2 = NewConnectorResponse

export type NewConnectorError = ErrorResponse

export type GetConnectorData = {
  path: {
    /**
     * Unique connector name
     */
    connector_name: string
  }
}

export type GetConnectorResponse = ConnectorDescr

export type GetConnectorError = ErrorResponse

export type CreateOrReplaceConnectorData = {
  body: CreateOrReplaceConnectorRequest
  path: {
    /**
     * Unique connector name
     */
    connector_name: string
  }
}

export type CreateOrReplaceConnectorResponse2 = CreateOrReplaceConnectorResponse

export type CreateOrReplaceConnectorError = ErrorResponse

export type DeleteConnectorData = {
  path: {
    /**
     * Unique connector name
     */
    connector_name: string
  }
}

export type DeleteConnectorResponse = unknown

export type DeleteConnectorError = ErrorResponse

export type UpdateConnectorData = {
  body: UpdateConnectorRequest
  path: {
    /**
     * Unique connector name
     */
    connector_name: string
  }
}

export type UpdateConnectorResponse2 = UpdateConnectorResponse

export type UpdateConnectorError = ErrorResponse

export type ListPipelinesData = {
  query?: {
    /**
     * Unique pipeline id.
     */
    id?: string | null
    /**
     * Unique pipeline name.
     */
    name?: string | null
  }
}

export type ListPipelinesResponse = Array<Pipeline>

export type ListPipelinesError = unknown

export type NewPipelineData = {
  body: NewPipelineRequest
}

export type NewPipelineResponse2 = NewPipelineResponse

export type NewPipelineError = ErrorResponse

export type GetPipelineData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineResponse = Pipeline

export type GetPipelineError = ErrorResponse

export type CreateOrReplacePipelineData = {
  body: CreateOrReplacePipelineRequest
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type CreateOrReplacePipelineResponse2 = CreateOrReplacePipelineResponse

export type CreateOrReplacePipelineError = ErrorResponse

export type PipelineDeleteData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PipelineDeleteResponse = unknown

export type PipelineDeleteError = ErrorResponse

export type UpdatePipelineData = {
  body: UpdatePipelineRequest
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type UpdatePipelineResponse2 = UpdatePipelineResponse

export type UpdatePipelineError = ErrorResponse

export type GetPipelineConfigData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type GetPipelineConfigResponse = PipelineConfig

export type GetPipelineConfigError = ErrorResponse

export type PipelineDeployedData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PipelineDeployedResponse = PipelineRevision | null

export type PipelineDeployedError = ErrorResponse

export type DumpProfileData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type DumpProfileResponse = {
  [key: string]: unknown
}

export type DumpProfileError = ErrorResponse

export type HttpOutputData = {
  /**
   * When the `query` parameter is set to 'neighborhood', the body of the request must contain a neighborhood specification.
   */
  body?: NeighborhoodQuery | null
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * SQL table name. Unquoted SQL names have to be capitalized. Quoted SQL names have to exactly match the case from the SQL program.
     */
    table_name: string
  }
  query: {
    /**
     * Set to `true` to group updates in this stream into JSON arrays (used in conjunction with `format=json`). The default value is `false`
     */
    array?: boolean | null
    /**
     * Apply backpressure on the pipeline when the HTTP client cannot receive data fast enough.
     * When this flag is set to false (the default), the HTTP connector drops data chunks if the client is not keeping up with its output.  This prevents a slow HTTP client from slowing down the entire pipeline.
     * When the flag is set to true, the connector waits for the client to receive each chunk and blocks the pipeline if the client cannot keep up.
     */
    backpressure?: boolean | null
    /**
     * Output data format, e.g., 'csv' or 'json'.
     */
    format: string
    /**
     * Output mode. Must be one of 'watch' or 'snapshot'. The default value is 'watch'
     */
    mode?: EgressMode | null
    /**
     * For 'quantiles' queries: the number of quantiles to output. The default value is 100.
     */
    quantiles?: number | null
    /**
     * Query to execute on the table. Must be one of 'table', 'neighborhood', or 'quantiles'. The default value is 'table'
     */
    query?: OutputQuery | null
  }
}

export type HttpOutputResponse = Chunk

export type HttpOutputError = ErrorResponse

export type HeapProfileData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type HeapProfileResponse = Blob | File

export type HeapProfileError = ErrorResponse

export type HttpInputData = {
  /**
   * Contains the new input data in CSV.
   */
  body: string
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
    /**
     * SQL table name. Unquoted SQL names have to be capitalized. Quoted SQL names have to exactly match the case from the SQL program.
     */
    table_name: string
  }
  query: {
    /**
     * Set to `true` if updates in this stream are packaged into JSON arrays (used in conjunction with `format=json`). The default values is `false`.
     */
    array?: boolean | null
    /**
     * When `true`, push data to the pipeline even if the pipeline is paused. The default value is `false`
     */
    force: boolean
    /**
     * Input data format, e.g., 'csv' or 'json'.
     */
    format: string
    /**
     * JSON data change event format (used in conjunction with `format=json`).  The default value is 'insert_delete'.
     */
    update_format?: JsonUpdateFormat | null
  }
}

export type HttpInputResponse = unknown

export type HttpInputError = ErrorResponse

export type PipelineStatsData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PipelineStatsResponse = {
  [key: string]: unknown
}

export type PipelineStatsError = ErrorResponse

export type PipelineValidateData = {
  path: {
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PipelineValidateResponse = string

export type PipelineValidateError = ErrorResponse

export type PipelineActionData = {
  path: {
    /**
     * Pipeline action [start, pause, shutdown]
     */
    action: string
    /**
     * Unique pipeline name
     */
    pipeline_name: string
  }
}

export type PipelineActionResponse = unknown

export type PipelineActionError = ErrorResponse

export type GetProgramsData = {
  query?: {
    /**
     * Unique program identifier.
     */
    id?: string | null
    /**
     * Unique program name.
     */
    name?: string | null
    /**
     * Option to include the SQL program code or not
     * in the Program objects returned by the query.
     * If false (default), the returned program object
     * will not include the code.
     */
    with_code?: boolean | null
  }
}

export type GetProgramsResponse = Array<ProgramDescr>

export type GetProgramsError = ErrorResponse

export type NewProgramData = {
  body: NewProgramRequest
}

export type NewProgramResponse2 = NewProgramResponse

export type NewProgramError = ErrorResponse

export type GetProgramData = {
  path: {
    /**
     * Unique program name
     */
    program_name: string
  }
  query?: {
    /**
     * Option to include the SQL program code or not
     * in the Program objects returned by the query.
     * If false (default), the returned program object
     * will not include the code.
     */
    with_code?: boolean | null
  }
}

export type GetProgramResponse = ProgramDescr

export type GetProgramError = ErrorResponse

export type CreateOrReplaceProgramData = {
  body: CreateOrReplaceProgramRequest
  path: {
    /**
     * Unique program name
     */
    program_name: string
  }
}

export type CreateOrReplaceProgramResponse2 = CreateOrReplaceProgramResponse

export type CreateOrReplaceProgramError = ErrorResponse

export type DeleteProgramData = {
  path: {
    /**
     * Unique program name
     */
    program_name: string
  }
}

export type DeleteProgramResponse = unknown

export type DeleteProgramError = ErrorResponse

export type UpdateProgramData = {
  body: UpdateProgramRequest
  path: {
    /**
     * Unique program name
     */
    program_name: string
  }
}

export type UpdateProgramResponse2 = UpdateProgramResponse

export type UpdateProgramError = ErrorResponse

export type CompileProgramData = {
  body: CompileProgramRequest
  path: {
    /**
     * Unique program name
     */
    program_name: string
  }
}

export type CompileProgramResponse = unknown

export type CompileProgramError = ErrorResponse

export type ListServicesData = {
  query?: {
    /**
     * If provided, will filter based on exact match of the configuration type.
     */
    config_type?: string | null
    /**
     * If provided, will filter based on exact match of the service identifier.
     */
    id?: string | null
    /**
     * If provided, will filter based on exact match of the service name.
     */
    name?: string | null
  }
}

export type ListServicesResponse = Array<ServiceDescr>

export type ListServicesError = ErrorResponse

export type NewServiceData = {
  body: NewServiceRequest
}

export type NewServiceResponse2 = NewServiceResponse

export type NewServiceError = ErrorResponse

export type GetServiceData = {
  path: {
    /**
     * Unique service name
     */
    service_name: string
  }
}

export type GetServiceResponse = ServiceDescr

export type GetServiceError = ErrorResponse

export type DeleteServiceData = {
  path: {
    /**
     * Unique service name
     */
    service_name: string
  }
}

export type DeleteServiceResponse = unknown

export type DeleteServiceError = ErrorResponse

export type UpdateServiceData = {
  body: UpdateServiceRequest
  path: {
    /**
     * Unique service name
     */
    service_name: string
  }
}

export type UpdateServiceResponse2 = UpdateServiceResponse

export type UpdateServiceError = ErrorResponse

export type ListServiceProbesData = {
  path: {
    /**
     * Unique service name
     */
    service_name: string
  }
  query?: {
    /**
     * If provided, will filter based on exact match of the service probe
     * identifier.
     */
    id?: string | null
    /**
     * If provided, will limit the amount of probes to the N most recent.
     */
    limit?: number | null
    /**
     * If provided, will only have probes of that particular type.
     */
    type?: ServiceProbeType | null
  }
}

export type ListServiceProbesResponse = Array<ServiceProbeDescr>

export type ListServiceProbesError = ErrorResponse

export type NewServiceProbeData = {
  body: ServiceProbeRequest
  path: {
    /**
     * Unique service name
     */
    service_name: string
  }
}

export type NewServiceProbeResponse = CreateServiceProbeResponse

export type NewServiceProbeError = ErrorResponse

export type $OpenApiTs = {
  '/config/authentication': {
    get: {
      res: {
        /**
         * The response body contains Authentication Provider configuration, or is empty if no auth is configured.
         */
        '200': AuthProvider
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/api_keys': {
    get: {
      req: ListApiKeysData
      res: {
        /**
         * API keys retrieved successfully
         */
        '200': Array<ApiKeyDescr>
        /**
         * Specified API key name does not exist.
         */
        '404': ErrorResponse
      }
    }
    post: {
      req: CreateApiKeyData
      res: {
        /**
         * API key created successfully.
         */
        '200': NewApiKeyResponse
        /**
         * An api key with this name already exists.
         */
        '409': ErrorResponse
      }
    }
  }
  '/v0/api_keys/{api_key_name}': {
    get: {
      req: GetApiKeyData
      res: {
        /**
         * API key retrieved successfully
         */
        '200': ApiKeyDescr
        /**
         * Specified API key name does not exist.
         */
        '404': ErrorResponse
      }
    }
    delete: {
      req: DeleteApiKeyData
      res: {
        /**
         * API key deleted successfully
         */
        '200': unknown
        /**
         * Specified API key name does not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/config/demos': {
    get: {
      res: {
        /**
         * URLs to JSON objects that describe a set of demos
         */
        '200': Array<string>
      }
    }
  }
  '/v0/connectors': {
    get: {
      req: ListConnectorsData
      res: {
        /**
         * List of connectors retrieved successfully
         */
        '200': Array<ConnectorDescr>
        /**
         * Specified connector name or ID does not exist
         */
        '404': ErrorResponse
      }
    }
    post: {
      req: NewConnectorData
      res: {
        /**
         * Connector successfully created
         */
        '201': NewConnectorResponse
        /**
         * A connector with this name already exists in the database
         */
        '409': ErrorResponse
      }
    }
  }
  '/v0/connectors/{connector_name}': {
    get: {
      req: GetConnectorData
      res: {
        /**
         * Connector retrieved successfully
         */
        '200': ConnectorDescr
        /**
         * Specified connector name does not exist
         */
        '404': ErrorResponse
      }
    }
    put: {
      req: CreateOrReplaceConnectorData
      res: {
        /**
         * Connector updated successfully
         */
        '200': CreateOrReplaceConnectorResponse
        /**
         * A connector with this name already exists in the database
         */
        '409': ErrorResponse
      }
    }
    delete: {
      req: DeleteConnectorData
      res: {
        /**
         * Connector successfully deleted
         */
        '200': unknown
        /**
         * Specified connector name does not exist
         */
        '404': ErrorResponse
      }
    }
    patch: {
      req: UpdateConnectorData
      res: {
        /**
         * Connector successfully updated
         */
        '200': UpdateConnectorResponse
        /**
         * Specified connector name does not exist
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines': {
    get: {
      req: ListPipelinesData
      res: {
        /**
         * Pipeline list retrieved successfully.
         */
        '200': Array<Pipeline>
      }
    }
    post: {
      req: NewPipelineData
      res: {
        /**
         * Pipeline successfully created.
         */
        '200': NewPipelineResponse
        /**
         * Specified program id or connector ids do not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}': {
    get: {
      req: GetPipelineData
      res: {
        /**
         * Pipeline descriptor retrieved successfully.
         */
        '200': Pipeline
        /**
         * Specified pipeline ID does not exist.
         */
        '404': ErrorResponse
      }
    }
    put: {
      req: CreateOrReplacePipelineData
      res: {
        /**
         * Pipeline updated successfully
         */
        '200': CreateOrReplacePipelineResponse
        /**
         * A pipeline with this name already exists in the database.
         */
        '409': ErrorResponse
      }
    }
    delete: {
      req: PipelineDeleteData
      res: {
        /**
         * Pipeline successfully deleted.
         */
        '200': unknown
        /**
         * Pipeline ID is invalid or pipeline is already running.
         */
        '400': ErrorResponse
        /**
         * Specified pipeline id does not exist.
         */
        '404': ErrorResponse
      }
    }
    patch: {
      req: UpdatePipelineData
      res: {
        /**
         * Pipeline successfully updated.
         */
        '200': UpdatePipelineResponse
        /**
         * Specified pipeline or connector does not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/config': {
    get: {
      req: GetPipelineConfigData
      res: {
        /**
         * Expanded pipeline configuration retrieved successfully.
         */
        '200': PipelineConfig
        /**
         * Specified pipeline ID does not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/deployed': {
    get: {
      req: PipelineDeployedData
      res: {
        /**
         * Last deployed version of the pipeline retrieved successfully (returns null if pipeline was never deployed yet).
         */
        '200': PipelineRevision | null
        /**
         * Specified `pipeline_id` does not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/dump_profile': {
    get: {
      req: DumpProfileData
      res: {
        /**
         * Profile dump initiated.
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Specified pipeline id is not a valid uuid.
         */
        '400': ErrorResponse
        /**
         * Specified pipeline id does not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/egress/{table_name}': {
    post: {
      req: HttpOutputData
      res: {
        /**
         * Connection to the endpoint successfully established. The body of the response contains a stream of data chunks.
         */
        '200': Chunk
        /**
         * Unknown data format specified in the '?format=' argument.
         */
        '400': ErrorResponse
        /**
         * Specified table or view does not exist.
         */
        '404': ErrorResponse
        /**
         * Pipeline is not currently running because it has been shutdown or not yet started.
         */
        '410': ErrorResponse
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/heap_profile': {
    get: {
      req: HeapProfileData
      res: {
        /**
         * Pipeline's heap usage profile as a gzipped protobuf that can be inspected by the pprof tool.
         */
        '200': Blob | File
        /**
         * Specified pipeline id is not a valid uuid.
         */
        '400': ErrorResponse
        /**
         * Specified pipeline id does not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/ingress/{table_name}': {
    post: {
      req: HttpInputData
      res: {
        /**
         * Data successfully delivered to the pipeline.
         */
        '200': unknown
        /**
         * Error parsing input data.
         */
        '400': ErrorResponse
        /**
         * Pipeline is not currently running because it has been shutdown or not yet started.
         */
        '404': ErrorResponse
        /**
         * Request failed.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/stats': {
    get: {
      req: PipelineStatsData
      res: {
        /**
         * Pipeline metrics retrieved successfully.
         */
        '200': {
          [key: string]: unknown
        }
        /**
         * Specified pipeline id is not a valid uuid.
         */
        '400': ErrorResponse
        /**
         * Specified pipeline id does not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/validate': {
    get: {
      req: PipelineValidateData
      res: {
        /**
         * Validate a Pipeline config.
         */
        '200': string
        /**
         * Invalid pipeline.
         */
        '400': ErrorResponse
        /**
         * Specified pipeline id does not exist.
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/pipelines/{pipeline_name}/{action}': {
    post: {
      req: PipelineActionData
      res: {
        /**
         * Request accepted.
         */
        '202': unknown
        /**
         * Pipeline desired state is not valid.
         */
        '400': ErrorResponse
        /**
         * Specified pipeline id does not exist.
         */
        '404': ErrorResponse
        /**
         * Timeout waiting for the pipeline to initialize.
         */
        '500': ErrorResponse
      }
    }
  }
  '/v0/programs': {
    get: {
      req: GetProgramsData
      res: {
        /**
         * List of programs retrieved successfully
         */
        '200': Array<ProgramDescr>
        /**
         * Specified program name or ID does not exist
         */
        '404': ErrorResponse
      }
    }
    post: {
      req: NewProgramData
      res: {
        /**
         * Program created successfully
         */
        '201': NewProgramResponse
        /**
         * A program with this name already exists in the database
         */
        '409': ErrorResponse
      }
    }
  }
  '/v0/programs/{program_name}': {
    get: {
      req: GetProgramData
      res: {
        /**
         * Program retrieved successfully
         */
        '200': ProgramDescr
        /**
         * Specified program name does not exist
         */
        '404': ErrorResponse
      }
    }
    put: {
      req: CreateOrReplaceProgramData
      res: {
        /**
         * Program updated successfully
         */
        '200': CreateOrReplaceProgramResponse
        /**
         * A program with this name already exists in the database
         */
        '409': ErrorResponse
      }
    }
    delete: {
      req: DeleteProgramData
      res: {
        /**
         * Program successfully deleted
         */
        '200': unknown
        /**
         * Specified program is referenced by a pipeline
         */
        '400': ErrorResponse
        /**
         * Specified program name does not exist
         */
        '404': ErrorResponse
      }
    }
    patch: {
      req: UpdateProgramData
      res: {
        /**
         * Program updated successfully
         */
        '200': UpdateProgramResponse
        /**
         * Specified program name does not exist
         */
        '404': ErrorResponse
        /**
         * A program with this name already exists in the database
         */
        '409': ErrorResponse
      }
    }
  }
  '/v0/programs/{program_name}/compile': {
    post: {
      req: CompileProgramData
      res: {
        /**
         * Compilation request submitted
         */
        '202': unknown
        /**
         * Specified program name does not exist
         */
        '404': ErrorResponse
        /**
         * Program version specified in the request doesn't match the latest program version in the database
         */
        '409': ErrorResponse
      }
    }
  }
  '/v0/services': {
    get: {
      req: ListServicesData
      res: {
        /**
         * List of services retrieved successfully
         */
        '200': Array<ServiceDescr>
        /**
         * Specified service name or ID does not exist
         */
        '404': ErrorResponse
      }
    }
    post: {
      req: NewServiceData
      res: {
        /**
         * Service successfully created
         */
        '201': NewServiceResponse
        /**
         * A service with this name already exists in the database
         */
        '409': ErrorResponse
      }
    }
  }
  '/v0/services/{service_name}': {
    get: {
      req: GetServiceData
      res: {
        /**
         * Service retrieved successfully
         */
        '200': ServiceDescr
        /**
         * Specified service name does not exist
         */
        '404': ErrorResponse
      }
    }
    delete: {
      req: DeleteServiceData
      res: {
        /**
         * Service successfully deleted
         */
        '200': unknown
        /**
         * Specified service name does not exist
         */
        '404': ErrorResponse
      }
    }
    patch: {
      req: UpdateServiceData
      res: {
        /**
         * Service successfully updated
         */
        '200': UpdateServiceResponse
        /**
         * Specified service name does not exist
         */
        '404': ErrorResponse
      }
    }
  }
  '/v0/services/{service_name}/probes': {
    get: {
      req: ListServiceProbesData
      res: {
        /**
         * Service probes retrieved successfully.
         */
        '200': Array<ServiceProbeDescr>
        /**
         * Specified service name does not exist
         */
        '404': ErrorResponse
      }
    }
    post: {
      req: NewServiceProbeData
      res: {
        /**
         * Service probe created successfully
         */
        '201': CreateServiceProbeResponse
        /**
         * Specified service name does not exist
         */
        '404': ErrorResponse
      }
    }
  }
}
