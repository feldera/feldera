// This file is auto-generated by @hey-api/openapi-ts

export const $ApiKeyDescr = {
  type: 'object',
  description: 'ApiKey descriptor.',
  required: ['id', 'name', 'scopes'],
  properties: {
    id: {
      $ref: '#/components/schemas/ApiKeyId'
    },
    name: {
      type: 'string'
    },
    scopes: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/ApiPermission'
      }
    }
  }
} as const

export const $ApiKeyId = {
  type: 'string',
  format: 'uuid',
  description: 'ApiKey ID.'
} as const

export const $ApiPermission = {
  type: 'string',
  description: 'Permission types for invoking pipeline manager APIs',
  enum: ['Read', 'Write']
} as const

export const $AttachedConnector = {
  type: 'object',
  description: 'Format to add attached connectors during a config update.',
  required: ['name', 'is_input', 'connector_name', 'relation_name'],
  properties: {
    connector_name: {
      type: 'string',
      description: 'The name of the connector to attach.'
    },
    is_input: {
      type: 'boolean',
      description: 'True for input connectors, false for output connectors.'
    },
    name: {
      type: 'string',
      description: 'A unique identifier for this attachement.'
    },
    relation_name: {
      type: 'string',
      description: `The table or view this connector is attached to. Unquoted
table/view names in the SQL program need to be capitalized
here. Quoted table/view names have to exactly match the
casing from the SQL program.`
    }
  }
} as const

export const $AttachedConnectorId = {
  type: 'string',
  format: 'uuid',
  description: 'Unique attached connector id.'
} as const

export const $AuthProvider = {
  oneOf: [
    {
      type: 'object',
      required: ['AwsCognito'],
      properties: {
        AwsCognito: {
          $ref: '#/components/schemas/ProviderAwsCognito'
        }
      }
    },
    {
      type: 'object',
      required: ['GoogleIdentity'],
      properties: {
        GoogleIdentity: {
          $ref: '#/components/schemas/ProviderGoogleIdentity'
        }
      }
    }
  ]
} as const

export const $AwsCredentials = {
  oneOf: [
    {
      type: 'object',
      required: ['type'],
      properties: {
        type: {
          type: 'string',
          enum: ['NoSignRequest']
        }
      }
    },
    {
      type: 'object',
      description: 'Authenticate using a long-lived AWS access key and secret',
      required: ['aws_access_key_id', 'aws_secret_access_key', 'type'],
      properties: {
        aws_access_key_id: {
          type: 'string'
        },
        aws_secret_access_key: {
          type: 'string'
        },
        type: {
          type: 'string',
          enum: ['AccessKey']
        }
      }
    }
  ],
  description: 'Configuration to authenticate against AWS',
  discriminator: {
    propertyName: 'type'
  }
} as const

export const $Chunk = {
  type: 'object',
  description: `A set of updates to a SQL table or view.

The \`sequence_number\` field stores the offset of the chunk relative to the
start of the stream and can be used to implement reliable delivery.
The payload is stored in the \`bin_data\`, \`text_data\`, or \`json_data\` field
depending on the data format used.`,
  required: ['sequence_number'],
  properties: {
    bin_data: {
      type: 'string',
      format: 'binary',
      description: 'Base64 encoded binary payload, e.g., bincode.',
      nullable: true
    },
    json_data: {
      type: 'object',
      description: 'JSON payload.',
      nullable: true
    },
    sequence_number: {
      type: 'integer',
      format: 'int64',
      minimum: 0
    },
    text_data: {
      type: 'string',
      description: 'Text payload, e.g., CSV.',
      nullable: true
    }
  }
} as const

export const $ColumnType = {
  type: 'object',
  description: `A SQL column type description.

Matches the Calcite JSON format.`,
  required: ['nullable'],
  properties: {
    component: {
      allOf: [
        {
          $ref: '#/components/schemas/ColumnType'
        }
      ],
      nullable: true
    },
    fields: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Field'
      },
      description: `The fields of the type (if available).

For example this would specify the fields of a \`CREATE TYPE\` construct.

\`\`\`sql
CREATE TYPE person_typ AS (
firstname       VARCHAR(30),
lastname        VARCHAR(30),
address         ADDRESS_TYP
);
\`\`\`

Would lead to the following \`fields\` value:

\`\`\`sql
[
ColumnType { name: "firstname, ... },
ColumnType { name: "lastname", ... },
ColumnType { name: "address", fields: [ ... ] }
]
\`\`\``,
      nullable: true
    },
    nullable: {
      type: 'boolean',
      description: 'Does the type accept NULL values?'
    },
    precision: {
      type: 'integer',
      format: 'int64',
      description: `Precision of the type.

# Examples
- \`VARCHAR\` sets precision to \`-1\`.
- \`VARCHAR(255)\` sets precision to \`255\`.
- \`BIGINT\`, \`DATE\`, \`FLOAT\`, \`DOUBLE\`, \`GEOMETRY\`, etc. sets precision
to None
- \`TIME\`, \`TIMESTAMP\` set precision to \`0\`.`,
      nullable: true
    },
    scale: {
      type: 'integer',
      format: 'int64',
      description: `The scale of the type.

# Example
- \`DECIMAL(1,2)\` sets scale to \`2\`.`,
      nullable: true
    },
    type: {
      $ref: '#/components/schemas/SqlType'
    }
  }
} as const

export const $CompilationProfile = {
  type: 'string',
  description: `Enumeration of possible compilation profiles that can be passed to the Rust compiler
as an argument via \`cargo build --profile <>\`. A compilation profile affects among
other things the compilation speed (how long till the program is ready to be run)
and runtime speed (the performance while running).`,
  enum: ['dev', 'unoptimized', 'optimized']
} as const

export const $CompileProgramRequest = {
  type: 'object',
  description: 'Request to queue a program for compilation.',
  required: ['version'],
  properties: {
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $ConnectorConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/OutputBufferConfig'
    },
    {
      type: 'object',
      required: ['transport'],
      properties: {
        format: {
          allOf: [
            {
              $ref: '#/components/schemas/FormatConfig'
            }
          ],
          nullable: true
        },
        max_queued_records: {
          type: 'integer',
          format: 'int64',
          description: `Backpressure threshold.

Maximal number of records queued by the endpoint before the endpoint
is paused by the backpressure mechanism.

For input endpoints, this setting bounds the number of records that have
been received from the input transport but haven't yet been consumed by
the circuit since the circuit, since the circuit is still busy processing
previous inputs.

For output endpoints, this setting bounds the number of records that have
been produced by the circuit but not yet sent via the output transport endpoint
nor stored in the output buffer (see \`enable_output_buffer\`).

Note that this is not a hard bound: there can be a small delay between
the backpressure mechanism is triggered and the endpoint is paused, during
which more data may be queued.

The default is 1 million.`,
          minimum: 0
        },
        transport: {
          $ref: '#/components/schemas/TransportConfig'
        }
      }
    }
  ],
  description: "A data connector's configuration"
} as const

export const $ConnectorDescr = {
  type: 'object',
  description: 'Connector descriptor.',
  required: ['connector_id', 'name', 'description', 'config'],
  properties: {
    config: {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    connector_id: {
      $ref: '#/components/schemas/ConnectorId'
    },
    description: {
      type: 'string'
    },
    name: {
      type: 'string'
    }
  }
} as const

export const $ConnectorId = {
  type: 'string',
  format: 'uuid',
  description: 'Unique connector id.'
} as const

export const $ConsumeStrategy = {
  oneOf: [
    {
      type: 'object',
      required: ['type'],
      properties: {
        type: {
          type: 'string',
          enum: ['Fragment']
        }
      }
    },
    {
      type: 'object',
      required: ['type'],
      properties: {
        type: {
          type: 'string',
          enum: ['Object']
        }
      }
    }
  ],
  description: 'Strategy to feed a fetched object into an InputConsumer.'
} as const

export const $CreateOrReplaceConnectorRequest = {
  type: 'object',
  description: 'Request to create or replace a connector.',
  required: ['description', 'config'],
  properties: {
    config: {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    description: {
      type: 'string',
      description: 'New connector description.'
    }
  }
} as const

export const $CreateOrReplaceConnectorResponse = {
  type: 'object',
  description: 'Response to a create or replace connector request.',
  required: ['connector_id'],
  properties: {
    connector_id: {
      $ref: '#/components/schemas/ConnectorId'
    }
  }
} as const

export const $CreateOrReplacePipelineRequest = {
  type: 'object',
  description: 'Request to create or replace an existing pipeline.',
  required: ['description', 'config'],
  properties: {
    config: {
      $ref: '#/components/schemas/RuntimeConfig'
    },
    connectors: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/AttachedConnector'
      },
      description: 'Attached connectors.',
      nullable: true
    },
    description: {
      type: 'string',
      description: 'Pipeline description.'
    },
    program_name: {
      type: 'string',
      description: 'Name of the program to create a pipeline for.',
      nullable: true
    }
  },
  additionalProperties: false
} as const

export const $CreateOrReplacePipelineResponse = {
  type: 'object',
  description: 'Response to a pipeline create or replace request.',
  required: ['pipeline_id', 'version'],
  properties: {
    pipeline_id: {
      $ref: '#/components/schemas/PipelineId'
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $CreateOrReplaceProgramRequest = {
  type: 'object',
  description: 'Request to create or replace a program.',
  required: ['description', 'code'],
  properties: {
    code: {
      type: 'string',
      description: 'SQL code of the program.',
      example: 'CREATE TABLE example(name VARCHAR);'
    },
    config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    description: {
      type: 'string',
      description: 'Program description.',
      example: 'Example description'
    }
  }
} as const

export const $CreateOrReplaceProgramResponse = {
  type: 'object',
  description: 'Response to a create or replace program request.',
  required: ['program_id', 'version'],
  properties: {
    program_id: {
      $ref: '#/components/schemas/ProgramId'
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $CreateOrReplaceServiceRequest = {
  type: 'object',
  description: 'Request to create or replace a service.',
  required: ['description', 'config'],
  properties: {
    config: {
      $ref: '#/components/schemas/ServiceConfig'
    },
    description: {
      type: 'string',
      description: 'Service description.'
    }
  }
} as const

export const $CreateOrReplaceServiceResponse = {
  type: 'object',
  description: 'Response to a create or replace service request.',
  required: ['service_id'],
  properties: {
    service_id: {
      $ref: '#/components/schemas/ServiceId'
    }
  }
} as const

export const $CreateServiceProbeResponse = {
  type: 'object',
  description: 'Response to a create service probe request.',
  required: ['service_probe_id'],
  properties: {
    service_probe_id: {
      $ref: '#/components/schemas/ServiceProbeId'
    }
  }
} as const

export const $CsvEncoderConfig = {
  type: 'object',
  properties: {
    buffer_size_records: {
      type: 'integer',
      minimum: 0
    }
  }
} as const

export const $CsvParserConfig = {
  type: 'object'
} as const

export const $DeltaTableIngestMode = {
  type: 'string',
  description: `Delta table read mode.

Three options are available:

* \`snapshot\` - read a snapshot of the table and stop.

* \`follow\` - continuously ingest changes to the table, starting from a specified version
or timestamp.

* \`snapshot_and_follow\` - read a snapshot of the table before switching to continuous ingestion
mode.`,
  enum: ['snapshot', 'follow', 'snapshot_and_follow']
} as const

export const $DeltaTableReaderConfig = {
  type: 'object',
  description: 'Delta table output connector configuration.',
  required: ['uri', 'mode'],
  properties: {
    datetime: {
      type: 'string',
      description: `Optional timestamp for the snapshot in the ISO-8601/RFC-3339 format, e.g.,
"2024-12-09T16:09:53+00:00.

When this option is set, the connector finds and opens the version of the table as of the
specified point in time.  In \`snapshot\` and \`snapshot_and_follow\` modes, it retrieves the
snapshot of this version of the table (based on the server time recorded in the transaction
log, not the event time encoded in the data).  In \`follow\` and \`snapshot_and_follow\` modes, it
follows transaction log records **after** this version.

Note: at most one of \`version\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
      nullable: true
    },
    mode: {
      $ref: '#/components/schemas/DeltaTableIngestMode'
    },
    snapshot_filter: {
      type: 'string',
      description: `Optional row filter.

This option is only valid when \`mode\` is set to \`snapshot\` or \`snapshot_and_follow\`.

When specified, only rows that satisfy the filter condition are included in the
snapshot.  The condition must be a valid SQL Boolean expression that can be used in
the \`where\` clause of the \`select * from snapshot where ...\` query.

This option can be used to specify the range of event times to include in the snapshot,
e.g.: \`ts BETWEEN '2005-01-01 00:00:00' AND '2010-12-31 23:59:59'\`.`,
      nullable: true
    },
    timestamp_column: {
      type: 'string',
      description: `Table column that serves as an event timestamp.


When this option is specified, and \`mode\` is one of \`snapshot\` or \`snapshot_and_follow\`,
the snapshot of the table is ingested in the timestamp order.  This setting is required
for tables declared with the
[\`LATENESS\`](https://www.feldera.com/docs/sql/streaming#lateness-expressions) attribute
in Feldera SQL. It impacts the performance of the connector, since data must be sorted
before pushing it to the pipeline; therefore it is not recommended to use this
settings for tables without \`LATENESS\`.`,
      nullable: true
    },
    uri: {
      type: 'string',
      description: `Table URI.

Example: "s3://feldera-fraud-detection-data/demographics_train"`
    },
    version: {
      type: 'integer',
      format: 'int64',
      description: `Optional table version.

When this option is set, the connector finds and opens the specified version of the table.
In \`snapshot\` and \`snapshot_and_follow\` modes, it retrieves the snapshot of this version of
the table.  In \`follow\` and \`snapshot_and_follow\` modes, it follows transaction log records
**after** this version.

Note: at most one of \`version\` and \`datetime\` options can be specified.
When neither of the two options is specified, the latest committed version of the table
is used.`,
      nullable: true
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Storage options for configuring backend object store.

For specific options available for different storage backends, see:
* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)`
  }
} as const

export const $DeltaTableWriteMode = {
  type: 'string',
  description: `Delta table write mode.

Determines how the Delta table connector handles an existing table at the target location.`,
  enum: ['append', 'truncate', 'error_if_exists']
} as const

export const $DeltaTableWriterConfig = {
  type: 'object',
  description: 'Delta table output connector configuration.',
  required: ['uri'],
  properties: {
    mode: {
      $ref: '#/components/schemas/DeltaTableWriteMode'
    },
    uri: {
      type: 'string',
      description: 'Table URI.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Storage options for configuring backend object store.

For specific options available for different storage backends, see:
* [Azure options](https://docs.rs/object_store/latest/object_store/azure/enum.AzureConfigKey.html)
* [Amazon S3 options](https://docs.rs/object_store/latest/object_store/aws/enum.AmazonS3ConfigKey.html)
* [Google Cloud Storage options](https://docs.rs/object_store/latest/object_store/gcp/enum.GoogleConfigKey.html)`
  }
} as const

export const $EgressMode = {
  type: 'string',
  enum: ['watch', 'snapshot']
} as const

export const $ErrorResponse = {
  type: 'object',
  description: 'Information returned by REST API endpoints on error.',
  required: ['message', 'error_code', 'details'],
  properties: {
    details: {
      type: 'object',
      description: `Detailed error metadata.
The contents of this field is determined by \`error_code\`.`
    },
    error_code: {
      type: 'string',
      description: 'Error code is a string that specifies this error type.',
      example: 'UnknownInputFormat'
    },
    message: {
      type: 'string',
      description: 'Human-readable error message.',
      example: "Unknown input format 'xml'."
    }
  }
} as const

export const $Field = {
  type: 'object',
  description: `A SQL field.

Matches the SQL compiler JSON format.`,
  required: ['name', 'columntype'],
  properties: {
    case_sensitive: {
      type: 'boolean'
    },
    columntype: {
      $ref: '#/components/schemas/ColumnType'
    },
    name: {
      type: 'string'
    }
  }
} as const

export const $FileInputConfig = {
  type: 'object',
  description: 'Configuration for reading data from a file with `FileInputTransport`',
  required: ['path'],
  properties: {
    buffer_size_bytes: {
      type: 'integer',
      description: `Read buffer size.

Default: when this parameter is not specified, a platform-specific
default is used.`,
      nullable: true,
      minimum: 0
    },
    follow: {
      type: 'boolean',
      description: `Enable file following.

When \`false\`, the endpoint outputs an \`InputConsumer::eoi\`
message and stops upon reaching the end of file.  When \`true\`, the
endpoint will keep watching the file and outputting any new content
appended to it.`
    },
    path: {
      type: 'string',
      description: 'File path.'
    }
  }
} as const

export const $FileOutputConfig = {
  type: 'object',
  description: 'Configuration for writing data to a file with `FileOutputTransport`.',
  required: ['path'],
  properties: {
    path: {
      type: 'string',
      description: 'File path.'
    }
  }
} as const

export const $FormatConfig = {
  type: 'object',
  description: `Data format specification used to parse raw data received from the
endpoint or to encode data sent to the endpoint.`,
  required: ['name'],
  properties: {
    config: {
      type: 'object',
      description: 'Format-specific parser or encoder configuration.'
    },
    name: {
      type: 'string',
      description: 'Format name, e.g., "csv", "json", "bincode", etc.'
    }
  }
} as const

export const $InputEndpointConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    {
      type: 'object',
      required: ['stream'],
      properties: {
        stream: {
          type: 'string',
          description: `The name of the input stream of the circuit that this endpoint is
connected to.`
        }
      }
    }
  ],
  description: 'Describes an input connector configuration'
} as const

export const $IntervalUnit = {
  type: 'string',
  description: `The specified units for SQL Interval types.

\`INTERVAL 1 DAY\`, \`INTERVAL 1 DAY TO HOUR\`, \`INTERVAL 1 DAY TO MINUTE\`,
would yield \`Day\`, \`DayToHour\`, \`DayToMinute\`, as the \`IntervalUnit\` respectively.`,
  enum: [
    'DAY',
    'DAYTOHOUR',
    'DAYTOMINUTE',
    'DAYTOSECOND',
    'HOUR',
    'HOURTOMINUTE',
    'HOURTOSECOND',
    'MINUTE',
    'MINUTETOSECOND',
    'MONTH',
    'SECOND',
    'YEAR',
    'YEARTOMONTH'
  ]
} as const

export const $JsonEncoderConfig = {
  type: 'object',
  properties: {
    array: {
      type: 'boolean'
    },
    buffer_size_records: {
      type: 'integer',
      minimum: 0
    },
    json_flavor: {
      allOf: [
        {
          $ref: '#/components/schemas/JsonFlavor'
        }
      ],
      nullable: true
    },
    update_format: {
      $ref: '#/components/schemas/JsonUpdateFormat'
    }
  }
} as const

export const $JsonFlavor = {
  type: 'string',
  description: 'Specifies JSON encoding used of table records.',
  enum: ['default', 'debezium_mysql', 'snowflake', 'kafka_connect_json_converter', 'pandas']
} as const

export const $JsonParserConfig = {
  type: 'object',
  description: `JSON parser configuration.

Describes the shape of an input JSON stream.

# Examples

A configuration with \`update_format="raw"\` and \`array=false\`
is used to parse a stream of JSON objects without any envelope
that get inserted in the input table.

\`\`\`json
{"b": false, "i": 100, "s": "foo"}
{"b": true, "i": 5, "s": "bar"}
\`\`\`

A configuration with \`update_format="insert_delete"\` and
\`array=false\` is used to parse a stream of JSON data change events
in the insert/delete format:

\`\`\`json
{"delete": {"b": false, "i": 15, "s": ""}}
{"insert": {"b": false, "i": 100, "s": "foo"}}
\`\`\`

A configuration with \`update_format="insert_delete"\` and
\`array=true\` is used to parse a stream of JSON arrays
where each array contains multiple data change events in
the insert/delete format.

\`\`\`json
[{"insert": {"b": true, "i": 0}}, {"delete": {"b": false, "i": 100, "s": "foo"}}]
\`\`\``,
  properties: {
    array: {
      type: 'boolean',
      description: `Set to \`true\` if updates in this stream are packaged into JSON arrays.

# Example

\`\`\`json
[{"b": true, "i": 0},{"b": false, "i": 100, "s": "foo"}]
\`\`\``
    },
    json_flavor: {
      $ref: '#/components/schemas/JsonFlavor'
    },
    update_format: {
      $ref: '#/components/schemas/JsonUpdateFormat'
    }
  }
} as const

export const $JsonUpdateFormat = {
  type: 'string',
  description: `Supported JSON data change event formats.

Each element in a JSON-formatted input stream specifies
an update to one or more records in an input table.  We support
several different ways to represent such updates.`,
  enum: ['insert_delete', 'weighted', 'debezium', 'snowflake', 'raw']
} as const

export const $KafkaHeader = {
  type: 'object',
  description: 'Kafka message header.',
  required: ['key'],
  properties: {
    key: {
      type: 'string'
    },
    value: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaHeaderValue'
        }
      ],
      nullable: true
    }
  }
} as const

export const $KafkaHeaderValue = {
  type: 'string',
  format: 'binary',
  description: 'Kafka header value encoded as a UTF-8 string or a byte array.'
} as const

export const $KafkaInputConfig = {
  type: 'object',
  description: 'Configuration for reading data from Kafka topics with `InputTransport`.',
  required: ['topics'],
  properties: {
    fault_tolerance: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaInputFtConfig'
        }
      ],
      nullable: true
    },
    group_join_timeout_secs: {
      type: 'integer',
      format: 'int32',
      description: `Maximum timeout in seconds to wait for the endpoint to join the Kafka
consumer group during initialization.`,
      minimum: 0
    },
    kafka_service: {
      type: 'string',
      description: 'If specified, this service is used to provide defaults for the Kafka options.',
      nullable: true
    },
    log_level: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaLogLevel'
        }
      ],
      nullable: true
    },
    topics: {
      type: 'array',
      items: {
        type: 'string'
      },
      description: 'List of topics to subscribe to.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Options passed directly to \`rdkafka\`.

[\`librdkafka\` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
used to configure the Kafka consumer.  Not all options are valid with
this Kafka adapter:

* "enable.auto.commit", if present, must be set to "false",
* "enable.auto.offset.store", if present, must be set to "false"`
  }
} as const

export const $KafkaInputFtConfig = {
  type: 'object',
  description: 'Fault tolerance configuration for Kafka input connector.',
  properties: {
    consumer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for consumers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for consumers, and may be empty.`,
      additionalProperties: {
        type: 'string'
      }
    },
    create_missing_index: {
      type: 'boolean',
      description: `If this is true or unset, then the connector will create missing index
topics as needed.  If this is false, then a missing index topic is a
fatal error.`,
      nullable: true
    },
    index_suffix: {
      type: 'string',
      description: `Suffix to append to each data topic name, to give the name of a topic
that the connector uses for recording the division of the corresponding
data topic into steps.  Defaults to \`_input-index\`.

An index topic must have the same number of partitions as its
corresponding data topic.

If two or more fault-tolerant Kafka endpoints read from overlapping sets
of topics, they must specify different \`index_suffix\` values.`,
      nullable: true
    },
    max_step_bytes: {
      type: 'integer',
      format: 'int64',
      description: `Maximum number of bytes in a step.  Any individual message bigger than
this will be given a step of its own.`,
      nullable: true,
      minimum: 0
    },
    max_step_messages: {
      type: 'integer',
      format: 'int64',
      description: 'Maximum number of messages in a step.',
      nullable: true,
      minimum: 0
    },
    producer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for producers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for producers, and may be empty.`,
      additionalProperties: {
        type: 'string'
      }
    }
  }
} as const

export const $KafkaLogLevel = {
  type: 'string',
  description: 'Kafka logging levels.',
  enum: ['emerg', 'alert', 'critical', 'error', 'warning', 'notice', 'info', 'debug']
} as const

export const $KafkaOutputConfig = {
  type: 'object',
  description: 'Configuration for writing data to a Kafka topic with `OutputTransport`.',
  required: ['topic'],
  properties: {
    fault_tolerance: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaOutputFtConfig'
        }
      ],
      nullable: true
    },
    headers: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/KafkaHeader'
      },
      description: 'Kafka headers to be added to each message produced by this connector.'
    },
    initialization_timeout_secs: {
      type: 'integer',
      format: 'int32',
      description: `Maximum timeout in seconds to wait for the endpoint to connect to
a Kafka broker.

Defaults to 60.`,
      minimum: 0
    },
    kafka_service: {
      type: 'string',
      description: 'If specified, this service is used to provide defaults for the Kafka options.',
      nullable: true
    },
    log_level: {
      allOf: [
        {
          $ref: '#/components/schemas/KafkaLogLevel'
        }
      ],
      nullable: true
    },
    max_inflight_messages: {
      type: 'integer',
      format: 'int32',
      description: `Maximum number of unacknowledged messages buffered by the Kafka
producer.

Kafka producer buffers outgoing messages until it receives an
acknowledgement from the broker.  This configuration parameter
bounds the number of unacknowledged messages.  When the number of
unacknowledged messages reaches this limit, sending of a new message
blocks until additional acknowledgements arrive from the broker.

Defaults to 1000.`,
      minimum: 0
    },
    topic: {
      type: 'string',
      description: 'Topic to write to.'
    }
  },
  additionalProperties: {
    type: 'string',
    description: `Options passed directly to \`rdkafka\`.

See [\`librdkafka\` options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md)
used to configure the Kafka producer.`
  }
} as const

export const $KafkaOutputFtConfig = {
  type: 'object',
  description: 'Fault tolerance configuration for Kafka output connector.',
  properties: {
    consumer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for consumers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for consumers, and may be empty.`,
      additionalProperties: {
        type: 'string'
      }
    },
    producer_options: {
      type: 'object',
      description: `Options passed to \`rdkafka\` for producers only, as documented at
[\`librdkafka\`
options](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).

These options override \`kafka_options\` for producers, and may be empty.`,
      additionalProperties: {
        type: 'string'
      }
    }
  }
} as const

export const $KafkaService = {
  type: 'object',
  description: 'Configuration for accessing a Kafka service.',
  required: ['bootstrap_servers', 'options'],
  properties: {
    bootstrap_servers: {
      type: 'array',
      items: {
        type: 'string'
      },
      description: `List of bootstrap servers, each formatted as hostname:port (e.g.,
"example.com:1234"). It will be used to set the \`bootstrap.servers\`
Kafka option.`
    },
    options: {
      type: 'object',
      description: `Additional Kafka options.

Should not contain the bootstrap.servers key
as it is passed explicitly via its field.

These options will likely encompass things
like SSL and authentication configuration.`,
      additionalProperties: {
        type: 'string'
      }
    }
  }
} as const

export const $NeighborhoodQuery = {
  type: 'object',
  description: `A request to output a specific neighborhood of a table or view.
The neighborhood is defined in terms of its central point (\`anchor\`)
and the number of rows preceding and following the anchor to output.`,
  required: ['before', 'after'],
  properties: {
    after: {
      type: 'integer',
      format: 'int32',
      minimum: 0
    },
    anchor: {
      type: 'object',
      nullable: true
    },
    before: {
      type: 'integer',
      format: 'int32',
      minimum: 0
    }
  }
} as const

export const $NewApiKeyRequest = {
  type: 'object',
  description: 'Request to create a new API key.',
  required: ['name'],
  properties: {
    name: {
      type: 'string',
      description: 'Key name.',
      example: 'my-api-key'
    }
  }
} as const

export const $NewApiKeyResponse = {
  type: 'object',
  description: 'Response to a successful API key creation.',
  required: ['api_key_id', 'name', 'api_key'],
  properties: {
    api_key: {
      type: 'string',
      description: `Generated API key. There is no way to
retrieve this key again from the
pipeline-manager, so store it securely.`,
      example:
        'apikey:v5y5QNtlPNVMwkmNjKwFU8bbIu5lMge3yHbyddxAOdXlEo84SEoNn32DUhQaf1KLeI9aOOfnJjhQ1pYzMrU4wQXON6pm6BS7Zgzj46U2b8pwz1280vYBEtx41hiDBRP'
    },
    api_key_id: {
      $ref: '#/components/schemas/ApiKeyId'
    },
    name: {
      type: 'string',
      description: 'API key name',
      example: 'my-api-key'
    }
  }
} as const

export const $NewConnectorRequest = {
  type: 'object',
  description: 'Request to create a new connector.',
  required: ['name', 'description', 'config'],
  properties: {
    config: {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    description: {
      type: 'string',
      description: 'Connector description.'
    },
    name: {
      type: 'string',
      description: 'Connector name.'
    }
  }
} as const

export const $NewConnectorResponse = {
  type: 'object',
  description: 'Response to a connector creation request.',
  required: ['connector_id'],
  properties: {
    connector_id: {
      $ref: '#/components/schemas/ConnectorId'
    }
  }
} as const

export const $NewPipelineRequest = {
  type: 'object',
  description: 'Request to create a new pipeline.',
  required: ['name', 'description', 'config'],
  properties: {
    config: {
      $ref: '#/components/schemas/RuntimeConfig'
    },
    connectors: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/AttachedConnector'
      },
      description: 'Attached connectors.',
      nullable: true
    },
    description: {
      type: 'string',
      description: 'Pipeline description.'
    },
    name: {
      type: 'string',
      description: 'Unique pipeline name.'
    },
    program_name: {
      type: 'string',
      description: 'Name of the program to create a pipeline for.',
      nullable: true
    }
  },
  additionalProperties: false
} as const

export const $NewPipelineResponse = {
  type: 'object',
  description: 'Response to a pipeline creation request.',
  required: ['pipeline_id', 'version'],
  properties: {
    pipeline_id: {
      $ref: '#/components/schemas/PipelineId'
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $NewProgramRequest = {
  type: 'object',
  description: 'Request to create a new program.',
  required: ['name', 'description', 'code'],
  properties: {
    code: {
      type: 'string',
      description: 'SQL code of the program.',
      example: 'CREATE TABLE example(name VARCHAR);'
    },
    config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    description: {
      type: 'string',
      description: 'Program description.',
      example: 'Example description'
    },
    name: {
      type: 'string',
      description: 'Program name.',
      example: 'example-program'
    }
  }
} as const

export const $NewProgramResponse = {
  type: 'object',
  description: 'Response to a new program request.',
  required: ['program_id', 'version'],
  properties: {
    program_id: {
      $ref: '#/components/schemas/ProgramId'
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $NewServiceRequest = {
  type: 'object',
  description: 'Request to create a new service.',
  required: ['name', 'description', 'config'],
  properties: {
    config: {
      $ref: '#/components/schemas/ServiceConfig'
    },
    description: {
      type: 'string',
      description: 'Service description.'
    },
    name: {
      type: 'string',
      description: 'Service name.'
    }
  }
} as const

export const $NewServiceResponse = {
  type: 'object',
  description: 'Response to a service creation request.',
  required: ['service_id'],
  properties: {
    service_id: {
      $ref: '#/components/schemas/ServiceId'
    }
  }
} as const

export const $OutputBufferConfig = {
  type: 'object',
  properties: {
    enable_output_buffer: {
      type: 'boolean',
      description: `Enable output buffering.

The output buffering mechanism allows decoupling the rate at which the pipeline
pushes changes to the output transport from the rate of input changes.

By default, output updates produced by the pipeline are pushed directly to
the output transport. Some destinations may prefer to receive updates in fewer
bigger batches. For instance, when writing Parquet files, producing
one bigger file every few minutes is usually better than creating
small files every few milliseconds.

To achieve such input/output decoupling, users can enable output buffering by
setting the \`enable_output_buffer\` flag to \`true\`.  When buffering is enabled, output
updates produced by the pipeline are consolidated in an internal buffer and are
pushed to the output transport when one of several conditions is satisfied:

* data has been accumulated in the buffer for more than \`max_output_buffer_time_millis\`
milliseconds.
* buffer size exceeds \`max_output_buffer_size_records\` records.

This flag is \`false\` by default.`
    },
    max_output_buffer_size_records: {
      type: 'integer',
      description: `Maximum number of updates to be kept in the output buffer.

This parameter bounds the maximal size of the buffer.
Note that the size of the buffer is not always equal to the
total number of updates output by the pipeline. Updates to the
same record can overwrite or cancel previous updates.

By default, the buffer can grow indefinitely until one of
the other output conditions is satisfied.

NOTE: this configuration option requires the \`enable_output_buffer\` flag
to be set.`,
      minimum: 0
    },
    max_output_buffer_time_millis: {
      type: 'integer',
      description: `Maximum time in milliseconds data is kept in the output buffer.

By default, data is kept in the buffer indefinitely until one of
the other output conditions is satisfied.  When this option is
set the buffer will be flushed at most every
\`max_output_buffer_time_millis\` milliseconds.

NOTE: this configuration option requires the \`enable_output_buffer\` flag
to be set.`,
      minimum: 0
    }
  }
} as const

export const $OutputEndpointConfig = {
  allOf: [
    {
      $ref: '#/components/schemas/ConnectorConfig'
    },
    {
      type: 'object',
      required: ['stream'],
      properties: {
        stream: {
          type: 'string',
          description: `The name of the output stream of the circuit that this endpoint is
connected to.`
        }
      }
    }
  ],
  description: 'Describes an output connector configuration'
} as const

export const $OutputQuery = {
  type: 'string',
  description: `A query over an output stream.

We currently do not support ad hoc queries.  Instead the client can use
three pre-defined queries to inspect the contents of a table or view.`,
  enum: ['table', 'neighborhood', 'quantiles']
} as const

export const $ParquetEncoderConfig = {
  type: 'object',
  properties: {
    buffer_size_records: {
      type: 'integer',
      description: `Number of records before a new parquet file is written.

The default is 100_000.`,
      minimum: 0
    }
  }
} as const

export const $ParquetParserConfig = {
  type: 'object',
  description: 'Configuration for the parquet parser.'
} as const

export const $Pipeline = {
  type: 'object',
  description: `State of a pipeline, including static configuration
and runtime status.`,
  required: ['descriptor', 'state'],
  properties: {
    descriptor: {
      $ref: '#/components/schemas/PipelineDescr'
    },
    state: {
      $ref: '#/components/schemas/PipelineRuntimeState'
    }
  }
} as const

export const $PipelineConfig = {
  allOf: [
    {
      type: 'object',
      description: `Global pipeline configuration settings. This is the publicly
exposed type for users to configure pipelines.`,
      properties: {
        cpu_profiler: {
          type: 'boolean',
          description: 'Enable CPU profiler.'
        },
        max_buffering_delay_usecs: {
          type: 'integer',
          format: 'int64',
          description: `Maximal delay in microseconds to wait for \`min_batch_size_records\` to
get buffered by the controller, defaults to 0.`,
          minimum: 0
        },
        min_batch_size_records: {
          type: 'integer',
          format: 'int64',
          description: `Minimal input batch size.

The controller delays pushing input records to the circuit until at
least \`min_batch_size_records\` records have been received (total
across all endpoints) or \`max_buffering_delay_usecs\` microseconds
have passed since at least one input records has been buffered.
Defaults to 0.`,
          minimum: 0
        },
        min_storage_rows: {
          type: 'integer',
          description: `The minimum estimated number of rows in a batch to write it to storage.
This is provided for debugging and fine-tuning and should ordinarily be
left unset. It only has an effect when \`storage\` is set to true.

A value of 0 will write even empty batches to storage, and nonzero
values provide a threshold.  \`usize::MAX\` would effectively disable
storage.`,
          nullable: true,
          minimum: 0
        },
        resources: {
          $ref: '#/components/schemas/ResourceConfig'
        },
        storage: {
          type: 'boolean',
          description: `Should persistent storage be enabled for this pipeline?

- If \`false\` (default), the pipeline's state is kept in in-memory data-structures.
This is useful if the pipeline is ephemeral and does not need to be recovered
after a restart. The pipeline will most likely run faster since it does not
need to read from, or write to disk

- If \`true\`, the pipeline state is stored in the specified location,
is persisted across restarts, and can be checkpointed and recovered.
This feature is currently experimental.`
        },
        tcp_metrics_exporter: {
          type: 'boolean',
          description: `Enable the TCP metrics exporter.

This is used for development purposes only.
If enabled, the \`metrics-observer\` CLI tool
can be used to inspect metrics from the pipeline.

Because of how Rust metrics work, this is only honored for the first
pipeline to be instantiated within a given process.`
        },
        workers: {
          type: 'integer',
          format: 'int32',
          description: 'Number of DBSP worker threads.',
          minimum: 0
        }
      }
    },
    {
      type: 'object',
      required: ['inputs'],
      properties: {
        inputs: {
          type: 'object',
          description: 'Input endpoint configuration.',
          additionalProperties: {
            $ref: '#/components/schemas/InputEndpointConfig'
          }
        },
        name: {
          type: 'string',
          description: 'Pipeline name.',
          nullable: true
        },
        outputs: {
          type: 'object',
          description: 'Output endpoint configuration.',
          additionalProperties: {
            $ref: '#/components/schemas/OutputEndpointConfig'
          }
        },
        storage_config: {
          allOf: [
            {
              $ref: '#/components/schemas/StorageConfig'
            }
          ],
          nullable: true
        }
      }
    }
  ],
  description: `Pipeline configuration specified by the user when creating
a new pipeline instance.

This is the shape of the overall pipeline configuration. It encapsulates a
[\`RuntimeConfig\`], which is the publicly exposed way for users to configure
pipelines.`
} as const

export const $PipelineDescr = {
  type: 'object',
  description: 'Pipeline descriptor.',
  required: ['pipeline_id', 'version', 'name', 'description', 'config', 'attached_connectors'],
  properties: {
    attached_connectors: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/AttachedConnector'
      }
    },
    config: {
      $ref: '#/components/schemas/RuntimeConfig'
    },
    description: {
      type: 'string'
    },
    name: {
      type: 'string'
    },
    pipeline_id: {
      $ref: '#/components/schemas/PipelineId'
    },
    program_name: {
      type: 'string',
      nullable: true
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $PipelineId = {
  type: 'string',
  format: 'uuid',
  description: 'Unique pipeline id.'
} as const

export const $PipelineRevision = {
  type: 'object',
  description: `A pipeline revision is a versioned, immutable configuration struct that
contains all information necessary to run a pipeline.`,
  required: ['revision', 'pipeline', 'connectors', 'services_for_connectors', 'program', 'config'],
  properties: {
    config: {
      $ref: '#/components/schemas/PipelineConfig'
    },
    connectors: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/ConnectorDescr'
      },
      description: 'The versioned connectors.'
    },
    pipeline: {
      $ref: '#/components/schemas/PipelineDescr'
    },
    program: {
      $ref: '#/components/schemas/ProgramDescr'
    },
    revision: {
      $ref: '#/components/schemas/Revision'
    },
    services_for_connectors: {
      type: 'array',
      items: {
        type: 'array',
        items: {
          $ref: '#/components/schemas/ServiceDescr'
        }
      },
      description: 'The versioned services for each connector.'
    }
  }
} as const

export const $PipelineRuntimeState = {
  type: 'object',
  description: 'Runtime state of the pipeine.',
  required: [
    'pipeline_id',
    'location',
    'desired_status',
    'current_status',
    'status_since',
    'created'
  ],
  properties: {
    created: {
      type: 'string',
      format: 'date-time',
      description: 'Time when the pipeline started executing.'
    },
    current_status: {
      $ref: '#/components/schemas/PipelineStatus'
    },
    desired_status: {
      $ref: '#/components/schemas/PipelineStatus'
    },
    error: {
      allOf: [
        {
          $ref: '#/components/schemas/ErrorResponse'
        }
      ],
      nullable: true
    },
    location: {
      type: 'string',
      description: `Location where the pipeline can be reached at runtime.
e.g., a TCP port number or a URI.`
    },
    pipeline_id: {
      $ref: '#/components/schemas/PipelineId'
    },
    status_since: {
      type: 'string',
      format: 'date-time',
      description: `Time when the pipeline was assigned its current status
of the pipeline.`
    }
  }
} as const

export const $PipelineStatus = {
  type: 'string',
  description: `Pipeline status.

This type represents the state of the pipeline tracked by the pipeline
runner and observed by the API client via the \`GET /pipeline\` endpoint.

### The lifecycle of a pipeline

The following automaton captures the lifecycle of the pipeline.  Individual
states and transitions of the automaton are described below.

* In addition to the transitions shown in the diagram, all states have an
implicit "forced shutdown" transition to the \`Shutdown\` state.  This
transition is triggered when the pipeline runner is unable to communicate
with the pipeline and thereby forces a shutdown.

* States labeled with the hourglass symbol (⌛) are **timed** states.  The
automaton stays in timed state until the corresponding operation completes
or until the runner performs a forced shutdown of the pipeline after a
pre-defined timeout perioud.

* State transitions labeled with API endpoint names (\`/deploy\`, \`/start\`,
\`/pause\`, \`/shutdown\`) are triggered by invoking corresponding endpoint,
e.g., \`POST /v0/pipelines/{pipeline_id}/start\`.

\`\`\`text
Shutdown◄────┐
│         │
/deploy│         │
│   ⌛ShuttingDown
▼         ▲
⌛Provisioning    │
│         │
Provisioned        │         │
▼         │/shutdown
⌛Initializing    │
│         │
┌────────┴─────────┴─┐
│        ▼           │
│      Paused        │
│      │    ▲        │
│/start│    │/pause  │
│      ▼    │        │
│     Running        │
└──────────┬─────────┘
│
▼
Failed
\`\`\`

### Desired and actual status

We use the desired state model to manage the lifecycle of a pipeline.
In this model, the pipeline has two status attributes associated with
it at runtime: the **desired** status, which represents what the user
would like the pipeline to do, and the **current** status, which
represents the actual state of the pipeline.  The pipeline runner
service continuously monitors both fields and steers the pipeline
towards the desired state specified by the user.
Only three of the states in the pipeline automaton above can be
used as desired statuses: \`Paused\`, \`Running\`, and \`Shutdown\`.
These statuses are selected by invoking REST endpoints shown
in the diagram.

The user can monitor the current state of the pipeline via the
\`/status\` endpoint, which returns an object of type \`Pipeline\`.
In a typical scenario, the user first sets
the desired state, e.g., by invoking the \`/deploy\` endpoint, and
then polls the \`GET /pipeline\` endpoint to monitor the actual status
of the pipeline until its \`state.current_status\` attribute changes
to "paused" indicating that the pipeline has been successfully
initialized, or "failed", indicating an error.`,
  enum: ['Shutdown', 'Provisioning', 'Initializing', 'Paused', 'Running', 'ShuttingDown', 'Failed']
} as const

export const $ProgramConfig = {
  type: 'object',
  description: 'Program configuration.',
  properties: {
    profile: {
      allOf: [
        {
          $ref: '#/components/schemas/CompilationProfile'
        }
      ],
      nullable: true
    }
  }
} as const

export const $ProgramDescr = {
  type: 'object',
  description: 'Program descriptor.',
  required: ['program_id', 'name', 'description', 'version', 'status', 'config'],
  properties: {
    code: {
      type: 'string',
      description: 'SQL code',
      nullable: true
    },
    config: {
      $ref: '#/components/schemas/ProgramConfig'
    },
    description: {
      type: 'string',
      description: 'Program description.'
    },
    name: {
      type: 'string',
      description: "Program name (doesn't have to be unique)."
    },
    program_id: {
      $ref: '#/components/schemas/ProgramId'
    },
    schema: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramSchema'
        }
      ],
      nullable: true
    },
    status: {
      $ref: '#/components/schemas/ProgramStatus'
    },
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $ProgramId = {
  type: 'string',
  format: 'uuid',
  description: 'Unique program id.'
} as const

export const $ProgramSchema = {
  type: 'object',
  description: `A struct containing the tables (inputs) and views for a program.

Parse from the JSON data-type of the DDL generated by the SQL compiler.`,
  required: ['inputs', 'outputs'],
  properties: {
    inputs: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Relation'
      }
    },
    outputs: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Relation'
      }
    }
  }
} as const

export const $ProgramStatus = {
  oneOf: [
    {
      type: 'string',
      description: `Compilation request received from the user; program has been placed
in the queue.`,
      enum: ['Pending']
    },
    {
      type: 'string',
      description: 'Compilation of SQL -> Rust in progress.',
      enum: ['CompilingSql']
    },
    {
      type: 'string',
      description: 'Compiling Rust -> executable in progress.',
      enum: ['CompilingRust']
    },
    {
      type: 'string',
      description: 'Compilation succeeded.',
      enum: ['Success']
    },
    {
      type: 'object',
      required: ['SqlError'],
      properties: {
        SqlError: {
          type: 'array',
          items: {
            $ref: '#/components/schemas/SqlCompilerMessage'
          },
          description: 'SQL compiler returned an error.'
        }
      }
    },
    {
      type: 'object',
      required: ['RustError'],
      properties: {
        RustError: {
          type: 'string',
          description: 'Rust compiler returned an error.'
        }
      }
    },
    {
      type: 'object',
      required: ['SystemError'],
      properties: {
        SystemError: {
          type: 'string',
          description: 'System/OS returned an error when trying to invoke commands.'
        }
      }
    }
  ],
  description: 'Program compilation status.'
} as const

export const $ProviderAwsCognito = {
  type: 'object',
  required: ['jwk_uri', 'login_url', 'logout_url'],
  properties: {
    jwk_uri: {
      type: 'string'
    },
    login_url: {
      type: 'string'
    },
    logout_url: {
      type: 'string'
    }
  }
} as const

export const $ProviderGoogleIdentity = {
  type: 'object',
  required: ['jwk_uri', 'client_id'],
  properties: {
    client_id: {
      type: 'string'
    },
    jwk_uri: {
      type: 'string'
    }
  }
} as const

export const $ReadStrategy = {
  oneOf: [
    {
      type: 'object',
      description: 'Read a single object specified by a key',
      required: ['key', 'type'],
      properties: {
        key: {
          type: 'string'
        },
        type: {
          type: 'string',
          enum: ['SingleKey']
        }
      }
    },
    {
      type: 'object',
      description: 'Read all objects whose keys match a prefix',
      required: ['prefix', 'type'],
      properties: {
        prefix: {
          type: 'string'
        },
        type: {
          type: 'string',
          enum: ['Prefix']
        }
      }
    }
  ],
  description: 'Strategy that determines which objects to read from a given bucket',
  discriminator: {
    propertyName: 'type'
  }
} as const

export const $Relation = {
  type: 'object',
  description: `A SQL table or view. It has a name and a list of fields.

Matches the Calcite JSON format.`,
  required: ['name', 'fields'],
  properties: {
    case_sensitive: {
      type: 'boolean'
    },
    fields: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/Field'
      }
    },
    name: {
      type: 'string'
    }
  }
} as const

export const $ResourceConfig = {
  type: 'object',
  properties: {
    cpu_cores_max: {
      type: 'integer',
      format: 'int64',
      description: `The maximum number of CPU cores to reserve
for an instance of this pipeline`,
      nullable: true,
      minimum: 0
    },
    cpu_cores_min: {
      type: 'integer',
      format: 'int64',
      description: `The minimum number of CPU cores to reserve
for an instance of this pipeline`,
      nullable: true,
      minimum: 0
    },
    memory_mb_max: {
      type: 'integer',
      format: 'int64',
      description: `The maximum memory in Megabytes to reserve
for an instance of this pipeline`,
      nullable: true,
      minimum: 0
    },
    memory_mb_min: {
      type: 'integer',
      format: 'int64',
      description: `The minimum memory in Megabytes to reserve
for an instance of this pipeline`,
      nullable: true,
      minimum: 0
    },
    storage_class: {
      type: 'string',
      description: `Storage class to use for an instance of this pipeline.
The class determines storage performance such as IOPS and throughput.`,
      nullable: true
    },
    storage_mb_max: {
      type: 'integer',
      format: 'int64',
      description: `The total storage in Megabytes to reserve
for an instance of this pipeline`,
      nullable: true,
      minimum: 0
    }
  }
} as const

export const $Revision = {
  type: 'string',
  format: 'uuid',
  description: 'Revision number.'
} as const

export const $RuntimeConfig = {
  type: 'object',
  description: `Global pipeline configuration settings. This is the publicly
exposed type for users to configure pipelines.`,
  properties: {
    cpu_profiler: {
      type: 'boolean',
      description: 'Enable CPU profiler.'
    },
    max_buffering_delay_usecs: {
      type: 'integer',
      format: 'int64',
      description: `Maximal delay in microseconds to wait for \`min_batch_size_records\` to
get buffered by the controller, defaults to 0.`,
      minimum: 0
    },
    min_batch_size_records: {
      type: 'integer',
      format: 'int64',
      description: `Minimal input batch size.

The controller delays pushing input records to the circuit until at
least \`min_batch_size_records\` records have been received (total
across all endpoints) or \`max_buffering_delay_usecs\` microseconds
have passed since at least one input records has been buffered.
Defaults to 0.`,
      minimum: 0
    },
    min_storage_rows: {
      type: 'integer',
      description: `The minimum estimated number of rows in a batch to write it to storage.
This is provided for debugging and fine-tuning and should ordinarily be
left unset. It only has an effect when \`storage\` is set to true.

A value of 0 will write even empty batches to storage, and nonzero
values provide a threshold.  \`usize::MAX\` would effectively disable
storage.`,
      nullable: true,
      minimum: 0
    },
    resources: {
      $ref: '#/components/schemas/ResourceConfig'
    },
    storage: {
      type: 'boolean',
      description: `Should persistent storage be enabled for this pipeline?

- If \`false\` (default), the pipeline's state is kept in in-memory data-structures.
This is useful if the pipeline is ephemeral and does not need to be recovered
after a restart. The pipeline will most likely run faster since it does not
need to read from, or write to disk

- If \`true\`, the pipeline state is stored in the specified location,
is persisted across restarts, and can be checkpointed and recovered.
This feature is currently experimental.`
    },
    tcp_metrics_exporter: {
      type: 'boolean',
      description: `Enable the TCP metrics exporter.

This is used for development purposes only.
If enabled, the \`metrics-observer\` CLI tool
can be used to inspect metrics from the pipeline.

Because of how Rust metrics work, this is only honored for the first
pipeline to be instantiated within a given process.`
    },
    workers: {
      type: 'integer',
      format: 'int32',
      description: 'Number of DBSP worker threads.',
      minimum: 0
    }
  }
} as const

export const $S3InputConfig = {
  type: 'object',
  description: 'Configuration for reading data from AWS S3.',
  required: ['credentials', 'region', 'bucket_name', 'read_strategy'],
  properties: {
    bucket_name: {
      type: 'string',
      description: 'S3 bucket name to access'
    },
    consume_strategy: {
      $ref: '#/components/schemas/ConsumeStrategy'
    },
    credentials: {
      $ref: '#/components/schemas/AwsCredentials'
    },
    read_strategy: {
      $ref: '#/components/schemas/ReadStrategy'
    },
    region: {
      type: 'string',
      description: 'AWS region'
    }
  }
} as const

export const $ServiceConfig = {
  oneOf: [
    {
      type: 'object',
      required: ['kafka'],
      properties: {
        kafka: {
          $ref: '#/components/schemas/KafkaService'
        }
      }
    }
  ],
  description: `Configuration for a Service, which typically includes how to establish a
connection (e.g., hostname, port) and authenticate (e.g., credentials).

This configuration can be used to easily derive connectors for the service
as well as probe it for information.`
} as const

export const $ServiceDescr = {
  type: 'object',
  description: 'Service descriptor.',
  required: ['service_id', 'name', 'description', 'config', 'config_type'],
  properties: {
    config: {
      $ref: '#/components/schemas/ServiceConfig'
    },
    config_type: {
      type: 'string'
    },
    description: {
      type: 'string'
    },
    name: {
      type: 'string'
    },
    service_id: {
      $ref: '#/components/schemas/ServiceId'
    }
  }
} as const

export const $ServiceId = {
  type: 'string',
  format: 'uuid',
  description: 'Unique service id.'
} as const

export const $ServiceProbeDescr = {
  type: 'object',
  description: 'Service probe descriptor.',
  required: ['service_probe_id', 'status', 'probe_type', 'request', 'created_at'],
  properties: {
    created_at: {
      type: 'string',
      format: 'date-time'
    },
    finished_at: {
      type: 'string',
      format: 'date-time',
      nullable: true
    },
    probe_type: {
      $ref: '#/components/schemas/ServiceProbeType'
    },
    request: {
      $ref: '#/components/schemas/ServiceProbeRequest'
    },
    response: {
      allOf: [
        {
          $ref: '#/components/schemas/ServiceProbeResponse'
        }
      ],
      nullable: true
    },
    service_probe_id: {
      $ref: '#/components/schemas/ServiceProbeId'
    },
    started_at: {
      type: 'string',
      format: 'date-time',
      nullable: true
    },
    status: {
      $ref: '#/components/schemas/ServiceProbeStatus'
    }
  }
} as const

export const $ServiceProbeError = {
  oneOf: [
    {
      type: 'string',
      enum: ['timeout_exceeded']
    },
    {
      type: 'object',
      required: ['unsupported_request'],
      properties: {
        unsupported_request: {
          type: 'object',
          required: ['service_type', 'probe_type'],
          properties: {
            probe_type: {
              type: 'string'
            },
            service_type: {
              type: 'string'
            }
          }
        }
      }
    },
    {
      type: 'object',
      required: ['other'],
      properties: {
        other: {
          type: 'string'
        }
      }
    }
  ],
  description: `Range of possible errors that can occur during a service probe.
These are shared across all services.`
} as const

export const $ServiceProbeId = {
  type: 'string',
  format: 'uuid',
  description: 'Unique service probe id.'
} as const

export const $ServiceProbeRequest = {
  type: 'string',
  description: 'Enumeration of all possible service probe requests.',
  enum: ['test_connectivity', 'kafka_get_topics']
} as const

export const $ServiceProbeResponse = {
  oneOf: [
    {
      type: 'object',
      required: ['success'],
      properties: {
        success: {
          $ref: '#/components/schemas/ServiceProbeResult'
        }
      }
    },
    {
      type: 'object',
      required: ['error'],
      properties: {
        error: {
          $ref: '#/components/schemas/ServiceProbeError'
        }
      }
    }
  ],
  description: 'Response being either success or error.'
} as const

export const $ServiceProbeResult = {
  oneOf: [
    {
      type: 'string',
      description: 'A connection to the service was established.',
      enum: ['connected']
    },
    {
      type: 'object',
      required: ['kafka_topics'],
      properties: {
        kafka_topics: {
          type: 'array',
          items: {
            type: 'string'
          },
          description: 'The names of all Kafka topics of the service.'
        }
      }
    }
  ],
  description: 'Enumeration of all possible service probe success responses.'
} as const

export const $ServiceProbeStatus = {
  type: 'string',
  description: `Service probe status.

State transition diagram:
\`\`\`text
Pending
│
│ (Prober server picks up the probe)
│
▼
⌛Running ───► Failure
│
▼
Success
\`\`\``,
  enum: ['pending', 'running', 'success', 'failure']
} as const

export const $ServiceProbeType = {
  type: 'string',
  description: `Enumeration of all possible service probe types.
Each type maps to exactly one request variant.`,
  enum: ['test_connectivity', 'kafka_get_topics']
} as const

export const $SqlCompilerMessage = {
  type: 'object',
  description: `A SQL compiler error.

The SQL compiler returns a list of errors in the following JSON format if
it's invoked with the \`-je\` option.

\`\`\`ignore
[ {
"startLineNumber" : 14,
"startColumn" : 13,
"endLineNumber" : 14,
"endColumn" : 13,
"warning" : false,
"errorType" : "Error parsing SQL",
"message" : "Encountered \"<EOF>\" at line 14, column 13."
} ]
\`\`\``,
  required: [
    'startLineNumber',
    'startColumn',
    'endLineNumber',
    'endColumn',
    'warning',
    'errorType',
    'message'
  ],
  properties: {
    endColumn: {
      type: 'integer',
      minimum: 0
    },
    endLineNumber: {
      type: 'integer',
      minimum: 0
    },
    errorType: {
      type: 'string'
    },
    message: {
      type: 'string'
    },
    startColumn: {
      type: 'integer',
      minimum: 0
    },
    startLineNumber: {
      type: 'integer',
      minimum: 0
    },
    warning: {
      type: 'boolean'
    }
  }
} as const

export const $SqlType = {
  oneOf: [
    {
      type: 'string',
      description: 'SQL `BOOLEAN` type.',
      enum: ['BOOLEAN']
    },
    {
      type: 'string',
      description: 'SQL `TINYINT` type.',
      enum: ['TINYINT']
    },
    {
      type: 'string',
      description: 'SQL `SMALLINT` or `INT2` type.',
      enum: ['SMALLINT']
    },
    {
      type: 'string',
      description: 'SQL `INTEGER`, `INT`, `SIGNED`, `INT4` type.',
      enum: ['INTEGER']
    },
    {
      type: 'string',
      description: 'SQL `BIGINT` or `INT64` type.',
      enum: ['BIGINT']
    },
    {
      type: 'string',
      description: 'SQL `REAL` or `FLOAT4` or `FLOAT32` type.',
      enum: ['REAL']
    },
    {
      type: 'string',
      description: 'SQL `DOUBLE` or `FLOAT8` or `FLOAT64` type.',
      enum: ['DOUBLE']
    },
    {
      type: 'string',
      description: 'SQL `DECIMAL` or `DEC` or `NUMERIC` type.',
      enum: ['DECIMAL']
    },
    {
      type: 'string',
      description: 'SQL `CHAR(n)` or `CHARACTER(n)` type.',
      enum: ['CHAR']
    },
    {
      type: 'string',
      description: 'SQL `VARCHAR`, `CHARACTER VARYING`, `TEXT`, or `STRING` type.',
      enum: ['VARCHAR']
    },
    {
      type: 'string',
      description: 'SQL `BINARY(n)` type.',
      enum: ['BINARY']
    },
    {
      type: 'string',
      description: 'SQL `VARBINARY` or `BYTEA` type.',
      enum: ['VARBINARY']
    },
    {
      type: 'string',
      description: 'SQL `TIME` type.',
      enum: ['TIME']
    },
    {
      type: 'string',
      description: 'SQL `DATE` type.',
      enum: ['DATE']
    },
    {
      type: 'string',
      description: 'SQL `TIMESTAMP` type.',
      enum: ['TIMESTAMP']
    },
    {
      type: 'object',
      required: ['Interval'],
      properties: {
        Interval: {
          $ref: '#/components/schemas/IntervalUnit'
        }
      }
    },
    {
      type: 'string',
      description: 'SQL `ARRAY` type.',
      enum: ['ARRAY']
    },
    {
      type: 'string',
      description: 'A complex SQL struct type (`CREATE TYPE x ...`).',
      enum: ['STRUCT']
    },
    {
      type: 'string',
      description: 'SQL `NULL` type.',
      enum: ['NULL']
    }
  ],
  description: 'The available SQL types as specified in `CREATE` statements.'
} as const

export const $StorageCacheConfig = {
  type: 'string',
  description: 'How to cache access to storage within a Feldera pipeline.',
  enum: ['page_cache', 'feldera_cache']
} as const

export const $StorageConfig = {
  type: 'object',
  description: 'Configuration for persistent storage in a [`PipelineConfig`].',
  required: ['path', 'cache'],
  properties: {
    cache: {
      $ref: '#/components/schemas/StorageCacheConfig'
    },
    path: {
      type: 'string',
      description: `The location where the pipeline state is stored or will be stored.

It should point to a path on the file-system of the machine/container
where the pipeline will run. If that path doesn't exist yet, or if it
does not contain any checkpoints, then the pipeline creates it and
starts from an initial state in which no data has yet been received. If
it does exist, then the pipeline starts from the most recent checkpoint
that already exists there. In either case, (further) checkpoints will be
written there.`
    }
  }
} as const

export const $TenantId = {
  type: 'string',
  format: 'uuid'
} as const

export const $TransportConfig = {
  oneOf: [
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileInputConfig'
        },
        name: {
          type: 'string',
          enum: ['file_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/FileOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['file_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/KafkaInputConfig'
        },
        name: {
          type: 'string',
          enum: ['kafka_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/KafkaOutputConfig'
        },
        name: {
          type: 'string',
          enum: ['kafka_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/UrlInputConfig'
        },
        name: {
          type: 'string',
          enum: ['url_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/S3InputConfig'
        },
        name: {
          type: 'string',
          enum: ['s3_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DeltaTableReaderConfig'
        },
        name: {
          type: 'string',
          enum: ['delta_table_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name', 'config'],
      properties: {
        config: {
          $ref: '#/components/schemas/DeltaTableWriterConfig'
        },
        name: {
          type: 'string',
          enum: ['delta_table_output']
        }
      }
    },
    {
      type: 'object',
      required: ['name'],
      properties: {
        name: {
          type: 'string',
          enum: ['http_input']
        }
      }
    },
    {
      type: 'object',
      required: ['name'],
      properties: {
        name: {
          type: 'string',
          enum: ['http_output']
        }
      }
    }
  ],
  description: `Transport-specific endpoint configuration passed to
\`crate::OutputTransport::new_endpoint\`
and \`crate::InputTransport::new_endpoint\`.`,
  discriminator: {
    propertyName: 'name'
  }
} as const

export const $UpdateConnectorRequest = {
  type: 'object',
  description: 'Request to update an existing connector.',
  properties: {
    config: {
      allOf: [
        {
          $ref: '#/components/schemas/ConnectorConfig'
        }
      ],
      nullable: true
    },
    description: {
      type: 'string',
      description: `New connector description. If absent, existing name will be kept
unmodified.`,
      nullable: true
    },
    name: {
      type: 'string',
      description: 'New connector name. If absent, existing name will be kept unmodified.',
      nullable: true
    }
  }
} as const

export const $UpdateConnectorResponse = {
  type: 'object',
  description: 'Response to a connector update request.'
} as const

export const $UpdatePipelineRequest = {
  type: 'object',
  description: 'Request to update an existing pipeline.',
  required: ['name', 'description'],
  properties: {
    config: {
      allOf: [
        {
          $ref: '#/components/schemas/RuntimeConfig'
        }
      ],
      nullable: true
    },
    connectors: {
      type: 'array',
      items: {
        $ref: '#/components/schemas/AttachedConnector'
      },
      description: `Attached connectors.

- If absent, existing connectors will be kept unmodified.

- If present all existing connectors will be replaced with the new
specified list.`,
      nullable: true
    },
    description: {
      type: 'string',
      description: 'New pipeline description.'
    },
    name: {
      type: 'string',
      description: 'New pipeline name.'
    },
    program_name: {
      type: 'string',
      description: `New program to create a pipeline for. If absent, program will be set to
NULL.`,
      nullable: true
    }
  },
  additionalProperties: false
} as const

export const $UpdatePipelineResponse = {
  type: 'object',
  description: 'Response to a config update request.',
  required: ['version'],
  properties: {
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $UpdateProgramRequest = {
  type: 'object',
  description: 'Request to update an existing program.',
  properties: {
    code: {
      type: 'string',
      description: `New SQL code for the program. If absent, existing program code will be
kept unmodified.`,
      nullable: true
    },
    config: {
      allOf: [
        {
          $ref: '#/components/schemas/ProgramConfig'
        }
      ],
      nullable: true
    },
    description: {
      type: 'string',
      description: `New program description. If absent, existing description will be kept
unmodified.`,
      nullable: true
    },
    guard: {
      allOf: [
        {
          $ref: '#/components/schemas/Version'
        }
      ],
      nullable: true
    },
    name: {
      type: 'string',
      description: 'New program name. If absent, existing name will be kept unmodified.',
      nullable: true
    }
  }
} as const

export const $UpdateProgramResponse = {
  type: 'object',
  description: 'Response to a program update request.',
  required: ['version'],
  properties: {
    version: {
      $ref: '#/components/schemas/Version'
    }
  }
} as const

export const $UpdateServiceRequest = {
  type: 'object',
  description: 'Request to update an existing service.',
  properties: {
    config: {
      allOf: [
        {
          $ref: '#/components/schemas/ServiceConfig'
        }
      ],
      nullable: true
    },
    description: {
      type: 'string',
      description: `New service description. If absent, existing name will be kept
unmodified.`,
      nullable: true
    },
    name: {
      type: 'string',
      description: 'New service name. If absent, existing name will be kept unmodified.',
      nullable: true
    }
  }
} as const

export const $UpdateServiceResponse = {
  type: 'object',
  description: 'Response to a service update request.'
} as const

export const $UrlInputConfig = {
  type: 'object',
  description: `Configuration for reading data from an HTTP or HTTPS URL with
\`UrlInputTransport\`.`,
  required: ['path'],
  properties: {
    path: {
      type: 'string',
      description: 'URL.'
    }
  }
} as const

export const $Version = {
  type: 'integer',
  format: 'int64',
  description: 'Version number.'
} as const
