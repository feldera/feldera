{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2b0059",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Case Study: Real-Time Credit Card Fraud Detection\n",
    "\n",
    "## Background: real-time feature engineering with DBSP\n",
    "\n",
    "**Feature engineering** is the process of transforming raw data into a set of features that can be used to improve the predictive accuracy of an ML model.  Features are often expressed as SQL queries that filter, transform, aggregate, and join raw data.  These queries are evaluated by an RDBMS, e.g., Postgres, and the results are fed to the ML model during training and inference stages.\n",
    "\n",
    "**Real-time feature engineering** arises in applications where data arrives continuously and requires immediate analysis, such as fraud detection and anomaly detection.  The main challenge in this process is extracting features from constantly changing data.  Although simple cases can be handled by streaming analytics platforms like Flink, they fall short when it comes to complex SQL queries that feature engineers commonly use (we will see examples of such queries in this case study!).  A common workaround is to precompute features through periodic batch jobs in an RDBMS such as BigQuery and inject the precomputed features into the real-time data stream.  This approach allows arbitrary feauture queries but sacrifices **feature freshness**, resulting in poor prediction accuracy in many real-time ML applications since precomputed features do not reflect the latest input data.\n",
    "\n",
    "**DBSP aims to provide the benefits of both worlds** by evaluating complex feature queries directly on streaming data, eliminating the need for batch jobs and delivering perfect feature freshness.\n",
    "\n",
    "## About this case study\n",
    "\n",
    "Our goal in this case study is two-fold:\n",
    "\n",
    "1. To illustrate how ML engineers can invoke DBSP from a Jupyter notebook to evaluate feature extraction queries on streaming data during model training, testing, and inference.\n",
    "1. To empirically prove that DBSP enhances prediction accuracy in real-time ML.  Specifically, we demonstrate that **both complex queries and data freshness are critical for achieving high accuracy in real-time ML applications**.\n",
    "\n",
    "This case study is based on the credit card fraud detection solution published by the Google Cloud blog:\n",
    "https://cloud.google.com/blog/products/data-analytics/how-to-build-a-fraud-detection-solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0696ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 22:32:13.290372: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-24 22:32:13.290595: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-24 22:32:13.292447: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-24 22:32:13.319425: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 22:32:13.852236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import gdown\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import geopy\n",
    "import geopy.distance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_curve, roc_curve\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "import urllib.request\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import threading\n",
    "import requests\n",
    "\n",
    "# API URL\n",
    "api_url = \"http://localhost:8080\"\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_url = \"https://raw.githubusercontent.com/feldera/ci_datasets/0eed3caac45415f6cd531dc5cb2e7f531d09a823/train_ci.csv\"\n",
    "test_url = \"https://raw.githubusercontent.com/feldera/ci_datasets/0eed3caac45415f6cd531dc5cb2e7f531d09a823/test_ci.csv\"\n",
    "simulation_url = \"https://raw.githubusercontent.com/feldera/ci_datasets/0eed3caac45415f6cd531dc5cb2e7f531d09a823/simulation_ci.csv\"\n",
    "demographics_url = \"https://github.com/feldera/ci_datasets/raw/e06f92fbd74f11d96ceb8b19c7692d05c7aca6d0/demographics.csv\"\n",
    "\n",
    "train_path = path.abspath(\"fraud_data/train.csv\")\n",
    "test_path = path.abspath(\"fraud_data/train.csv\")\n",
    "simulation_path = path.abspath(\"fraud_data/simulation_short2.csv\")\n",
    "demographics_path = path.abspath(\"fraud_data/demographics.csv\")\n",
    "\n",
    "def train_outpath(query: str):\n",
    "    return path.abspath(f\"fraud_data/{query}-train_output.csv\")\n",
    "    \n",
    "def test_outpath(query: str):\n",
    "    return path.abspath(f\"fraud_data/{query}-test_output.csv\")\n",
    "\n",
    "def simulation_outpath(query: str):\n",
    "    return path.abspath(f\"fraud_data/{query}-simulation_output.csv\")\n",
    "\n",
    "def create_program(program_name: str, query: str):\n",
    "    program_sql = \"\"\"\n",
    "    CREATE TABLE demographics (\n",
    "        cc_num FLOAT64 NOT NULL,\n",
    "        first STRING,\n",
    "        gender STRING,\n",
    "        street STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        zip INTEGER,\n",
    "        lat FLOAT64 NOT NULL,\n",
    "        long FLOAT64 NOT NULL,\n",
    "        city_pop INTEGER,\n",
    "        job STRING,\n",
    "        dob STRING\n",
    "        --dob DATE\n",
    "    );\n",
    "\n",
    "    CREATE TABLE transactions (\n",
    "        trans_date_trans_time TIMESTAMP NOT NULL,\n",
    "        cc_num FLOAT64 NOT NULL,\n",
    "        merchant STRING,\n",
    "        category STRING,\n",
    "        amt FLOAT64,\n",
    "        trans_num STRING,\n",
    "        unix_time INTEGER NOT NULL,\n",
    "        merch_lat FLOAT64 NOT NULL,\n",
    "        merch_long FLOAT64 NOT NULL,\n",
    "        is_fraud INTEGER\n",
    "    );\n",
    "    \"\"\" + query\n",
    "\n",
    "    # Create program\n",
    "    response = requests.put(f\"{api_url}/v0/programs/{program_name}\", json={\n",
    "        \"description\": \"\",\n",
    "        \"code\": program_sql\n",
    "    })\n",
    "    response.raise_for_status()\n",
    "    program_version = response.json()[\"version\"]\n",
    "\n",
    "    # Compile program\n",
    "    print(f\"Compiling program '{program_name}' (version: {program_version})...\")\n",
    "    requests.post(f\"{api_url}/v0/programs/{program_name}/compile\", json={\"version\": program_version}).raise_for_status()\n",
    "    while True:\n",
    "        status = requests.get(f\"{api_url}/v0/programs/{program_name}\").json()[\"status\"]\n",
    "        print(f\"Program status: {status}\")\n",
    "        if status == \"Success\":\n",
    "            break\n",
    "        elif status != \"Pending\" and status != \"CompilingRust\" and status != \"CompilingSql\":\n",
    "            raise RuntimeError(f\"Failed program compilation with status {status}\")\n",
    "        time.sleep(5)\n",
    "    print(\"done\")\n",
    "    \n",
    "def wait_for_pipeline(pipeline_name: str):\n",
    "    # Wait till the pipeline is completed\n",
    "    while not requests.get(f\"{api_url}/v0/pipelines/{pipeline_name}/stats\") \\\n",
    "            .json()[\"global_metrics\"][\"pipeline_complete\"]:\n",
    "        time.sleep(1)\n",
    "    \n",
    "    requests.post(f\"{api_url}/v0/pipelines/{pipeline_name}/shutdown\").raise_for_status()\n",
    "\n",
    "def run_query(program_name, transaction_path: str, output_file: str):\n",
    "    # Connectors\n",
    "    connectors = []\n",
    "    for (connector_name, stream, url, is_input) in [\n",
    "        (\"demographics\", 'DEMOGRAPHICS', demographics_path, True),\n",
    "        (\"transactions\", 'TRANSACTIONS', transaction_path, True),\n",
    "        (\"features\", 'FEATURES', output_file, False),\n",
    "    ]:\n",
    "        requests.put(f\"{api_url}/v0/connectors/{connector_name}\", json={\n",
    "            \"description\": \"\",\n",
    "            \"config\": {\n",
    "               \"format\": { \"name\": \"csv\", \"config\": {} },\n",
    "               \"transport\": { \"name\": \"file\", \"config\": { \"path\": url, } }\n",
    "            }\n",
    "        })\n",
    "        connectors.append({ \"connector_name\": connector_name, \"is_input\": is_input, \"name\": connector_name, \"relation_name\": stream })\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline_name = program_name + \"-pipeline\"\n",
    "    requests.put(f\"{api_url}/v0/pipelines/{pipeline_name}\", json={\n",
    "        \"description\": \"\",\n",
    "        \"config\": {\"workers\": 6},\n",
    "        \"program_name\": program_name,\n",
    "        \"connectors\": connectors,\n",
    "    }).raise_for_status()\n",
    "    \n",
    "    # Start pipeline in paused state, so we don't miss any outputs\n",
    "    print(\"(Re)starting pipeline...\")\n",
    "    requests.post(f\"{api_url}/v0/pipelines/{pipeline_name}/shutdown\").raise_for_status()\n",
    "    while requests.get(f\"{api_url}/v0/pipelines/{pipeline_name}\").json()[\"state\"][\"current_status\"] != \"Shutdown\":\n",
    "        time.sleep(1)\n",
    "    requests.post(f\"{api_url}/v0/pipelines/{pipeline_name}/pause\").raise_for_status()\n",
    "    while requests.get(f\"{api_url}/v0/pipelines/{pipeline_name}\").json()[\"state\"][\"current_status\"] != \"Paused\":\n",
    "        time.sleep(1)\n",
    "        \n",
    "    print(\"Pipeline (re)started\")\n",
    "\n",
    "    # # Start listening for the output stream\n",
    "    # features = requests.post(f\"{api_url}/v0/pipelines/{pipeline_name}/egress/features?format=csv\", stream = True)   \n",
    "\n",
    "    # Run the pipeline\n",
    "    requests.post(f\"{api_url}/v0/pipelines/{pipeline_name}/start\").raise_for_status()\n",
    "    while requests.get(f\"{api_url}/v0/pipelines/{pipeline_name}\").json()[\"state\"][\"current_status\"] != \"Running\":\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Wait for the pipeline to complete. The thread will shutdown the pipeline when it's done,\n",
    "    # allowing the iteration below to terminate.\n",
    "    thread = threading.Thread(target=wait_for_pipeline(pipeline_name))\n",
    "    thread.start()\n",
    "        \n",
    "#     full_line = \"\"\n",
    "#     print(f\"writing features to {output_file}\")\n",
    "#     with open(output_file, \"w\") as feature_file:\n",
    "#         for line in features.iter_lines():\n",
    "#             if line:\n",
    "#                 full_line += line.decode('utf-8')\n",
    "#                 # print(f\"line: {line}\")\n",
    "#                 try:\n",
    "#                     data = json.loads(full_line).get(\"text_data\")\n",
    "#                     full_line = \"\"\n",
    "#                     if data is not None:\n",
    "#                         feature_file.write(data + '\\n')\n",
    "#                 except ValueError as e:\n",
    "#                     print(\"partial json\")\n",
    "#         if full_line != \"\":\n",
    "#             print(f\"leftover: {full_line}\")\n",
    "\n",
    "def show_data(cm, print_res = 0):\n",
    "    tp = cm[1,1]\n",
    "    fn = cm[1,0]\n",
    "    fp = cm[0,1]\n",
    "    tn = cm[0,0]\n",
    "    if print_res == 1:\n",
    "        global pr, rec, f1\n",
    "        pr = tp/(tp+fp)\n",
    "        rec=  tp/(tp+fn)\n",
    "        f1 = (2*(pr*rec)/(pr+rec))\n",
    "        print('Precision =     {:.3f}'.format(pr))\n",
    "        print('Recall (TPR) =  {:.3f}'.format(rec))\n",
    "        #print('Fallout (FPR) = {:.3f}'.format(fp/(fp+tn)))\n",
    "        print('F1 = {:.3f}'.format(f1))\n",
    "    return tp/(tp+fp), tp/(tp+fn), fp/(fp+tn)\n",
    "\n",
    "def train(query_name):\n",
    "    max_depth = 12\n",
    "    n_estimators = 100\n",
    "\n",
    "    traindata     = pd.read_csv(train_outpath(query_name), float_precision='round_trip')  \n",
    "    train_dataset = shuffle(traindata)\n",
    "\n",
    "    test_dataset     = pd.read_csv(test_outpath(query_name), float_precision='round_trip')  \n",
    "\n",
    "    nb_cols = len(train_dataset.columns.tolist())\n",
    "\n",
    "    X_train = train_dataset.iloc[:, 0:nb_cols - 2].values\n",
    "    y_train = train_dataset.iloc[:, nb_cols-2].values.astype(int)        \n",
    "\n",
    "    X_test = test_dataset.iloc[:, 0:nb_cols - 2].values\n",
    "    y_test = test_dataset.iloc[:, nb_cols-2].values.astype(int)    \n",
    "\n",
    "\n",
    "    model = XGBClassifier(max_depth = max_depth,  n_estimators = n_estimators, objective = 'binary:logistic')#, scale_pos_weight= estimate) \n",
    "    setattr(model, 'verbosity', 0)\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # evaluate train data\n",
    "    y_pred = model.predict(X_train)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    cm = confusion_matrix(y_train, predictions)\n",
    "    show_data(cm, print_res = 1)\n",
    "\n",
    "    # evaluate for test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    show_data(cm, print_res = 1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"XGBoost Accuracy for {query_name}: %.2f%%\" % (accuracy * 100.0))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37021d75-be0b-44ab-acc3-887c98af9dba",
   "metadata": {},
   "source": [
    "## Prepare a feature extraction query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0036bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling program 'raw' (version: 1)...\n",
      "Program status: Success\n",
      "done\n",
      "Compiling program 'rolling_aggregates' (version: 3)...\n",
      "Program status: Pending\n",
      "Program status: CompilingRust\n",
      "Program status: CompilingRust\n",
      "Program status: CompilingRust\n",
      "Program status: Success\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    (\"raw\",\n",
    "        \"\"\"CREATE VIEW features as\n",
    "        SELECT\n",
    "            DAYOFWEEK(trans_date_trans_time) AS d,\n",
    "            -- TIMESTAMPDIFF(YEAR, trans_date_trans_time, CAST(dob as TIMESTAMP)) AS age,\n",
    "            \"ST_DISTANCE\"(\"ST_POINT\"(long,lat), \"ST_POINT\"(merch_long,merch_lat)) AS distance,\n",
    "            -- TIMESTAMPDIFF(MINUTE, trans_date_trans_time, last_txn_date) AS trans_diff,\n",
    "            category,\n",
    "            amt,\n",
    "            state,\n",
    "            job,\n",
    "            unix_time,\n",
    "            city_pop,\n",
    "            merchant,\n",
    "            is_fraud\n",
    "        FROM (\n",
    "            SELECT t1.*, t2.*\n",
    "                   -- , LAG(trans_date_trans_time, 1) OVER (PARTITION BY t1.cc_num  ORDER BY trans_date_trans_time ASC) AS last_txn_date\n",
    "            FROM  transactions AS t1\n",
    "            JOIN  demographics AS t2\n",
    "            ON t1.cc_num = t2.cc_num);\"\"\"),\n",
    "    (\"rolling_aggregates\", \"\"\"CREATE VIEW features as\n",
    "        SELECT\n",
    "            DAYOFWEEK(trans_date_trans_time) AS d,\n",
    "            -- TIMESTAMPDIFF(YEAR, trans_date_trans_time, CAST(dob as TIMESTAMP)) AS age,\n",
    "            \"ST_DISTANCE\"(\"ST_POINT\"(long,lat), \"ST_POINT\"(merch_long,merch_lat)) AS distance,\n",
    "            -- TIMESTAMPDIFF(MINUTE, trans_date_trans_time, last_txn_date) AS trans_diff,\n",
    "            AVG(amt) OVER(\n",
    "                PARTITION BY   CAST(cc_num AS NUMERIC)\n",
    "                ORDER BY unix_time\n",
    "                -- 1 week is 604800  seconds\n",
    "               RANGE BETWEEN 604800  PRECEDING AND 1 PRECEDING) AS avg_spend_pw,\n",
    "            AVG(amt) OVER(\n",
    "                PARTITION BY  CAST(cc_num AS NUMERIC)\n",
    "                ORDER BY unix_time\n",
    "                -- 1 month(30 days) is 2592000 seconds\n",
    "                RANGE BETWEEN 2592000 PRECEDING AND 1 PRECEDING) AS avg_spend_pm,\n",
    "            IFNULL(AVG(amt) OVER(\n",
    "                PARTITION BY CAST(cc_num AS NUMERIC), EXTRACT(DAY FROM trans_date_trans_time)\n",
    "                ORDER BY unix_time\n",
    "                RANGE BETWEEN 7776000 PRECEDING AND 1 PRECEDING), 0) AS avg_spend_p3m_over_d,\n",
    "            COUNT(*) OVER(\n",
    "                PARTITION BY  CAST(cc_num AS NUMERIC)\n",
    "                ORDER BY unix_time\n",
    "                -- 1 day is 86400  seconds\n",
    "                RANGE BETWEEN 86400  PRECEDING AND 1 PRECEDING ) AS trans_freq_24,\n",
    "            category,\n",
    "            amt,\n",
    "            state,\n",
    "            job,\n",
    "            unix_time,\n",
    "            city_pop,\n",
    "            merchant,\n",
    "            is_fraud\n",
    "        FROM (\n",
    "            SELECT t1.*, t2.*\n",
    "                   -- , LAG(trans_date_trans_time, 1) OVER (PARTITION BY t1.cc_num  ORDER BY trans_date_trans_time ASC) AS last_txn_date\n",
    "            FROM  transactions AS t1\n",
    "            JOIN  demographics AS t2\n",
    "            ON t1.cc_num = t2.cc_num);\"\"\")]\n",
    "\n",
    "for (name, query) in queries:\n",
    "    create_program(name, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceee2c1-90af-4823-8521-733287328119",
   "metadata": {},
   "source": [
    "## Compute features on training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbfdd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Re)starting pipeline...\n",
      "Pipeline (re)started\n",
      "Training pipeline for 'raw' finished\n",
      "(Re)starting pipeline...\n",
      "Pipeline (re)started\n",
      "Test pipeline for 'raw' finished\n",
      "(Re)starting pipeline...\n",
      "Pipeline (re)started\n",
      "Training pipeline for 'rolling_aggregates' finished\n",
      "(Re)starting pipeline...\n",
      "Pipeline (re)started\n",
      "Test pipeline for 'rolling_aggregates' finished\n"
     ]
    }
   ],
   "source": [
    "for (name, query) in queries:\n",
    "    run_query(name, train_path, train_outpath(name))\n",
    "    print(f\"Training pipeline for '{name}' finished\")\n",
    "    run_query(name, test_path, test_outpath(name))\n",
    "    print(f\"Test pipeline for '{name}' finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26c169",
   "metadata": {},
   "source": [
    "## Train XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74331f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision =     1.000\n",
      "Recall (TPR) =  0.986\n",
      "F1 = 0.993\n",
      "Precision =     1.000\n",
      "Recall (TPR) =  0.986\n",
      "F1 = 0.993\n",
      "XGBoost Accuracy for raw: 99.98%\n",
      "Precision =     1.000\n",
      "Recall (TPR) =  1.000\n",
      "F1 = 1.000\n",
      "Precision =     0.998\n",
      "Recall (TPR) =  0.989\n",
      "F1 = 0.994\n",
      "XGBoost Accuracy for rolling_aggregates: 99.99%\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "for (name, query) in queries:\n",
    "    models[name] = train(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90ef39",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f9ca954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory: logs/xgboost_fraud_detection\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'logs/xgboost_fraud_detection'\n",
    "os.makedirs('logs/xgboost_fraud_detection', exist_ok=True)\n",
    "print(f\"Log directory: {log_dir}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3177eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 22:36:42.257392: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:134] retrieving CUDA diagnostic information for host: dbsp\n",
      "2024-03-24 22:36:42.257408: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:141] hostname: dbsp\n",
      "2024-03-24 22:36:42.257461: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:165] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2024-03-24 22:36:42.257485: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:169] kernel reported version is: 525.147.5\n"
     ]
    }
   ],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "692c286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Re)starting pipeline...\n",
      "Pipeline (re)started\n",
      "Simulation pipeline for 'raw' finished\n",
      "(Re)starting pipeline...\n",
      "Pipeline (re)started\n",
      "Simulation pipeline for 'rolling_aggregates' finished\n"
     ]
    }
   ],
   "source": [
    "for (name, query) in queries:\n",
    "    run_query(name, simulation_path, simulation_outpath(name))\n",
    "    print(f\"Simulation pipeline for '{name}' finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f993f7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from /home/leonid/projects/feldera/demo/demo_notebooks/fraud_data/raw-simulation_output.csv\n",
      "XGBoost Accuracy: 83.05%\n",
      "XGBoost Accuracy: 83.80%\n",
      "XGBoost Accuracy: 82.53%\n",
      "XGBoost Accuracy: 84.12%\n",
      "XGBoost Accuracy: 83.81%\n",
      "XGBoost Accuracy: 83.11%\n",
      "XGBoost Accuracy: 82.92%\n",
      "XGBoost Accuracy: 84.45%\n",
      "XGBoost Accuracy: 84.53%\n",
      "XGBoost Accuracy: 84.35%\n",
      "XGBoost Accuracy: 84.08%\n",
      "XGBoost Accuracy: 83.42%\n",
      "XGBoost Accuracy: 84.10%\n",
      "XGBoost Accuracy: 83.89%\n",
      "XGBoost Accuracy: 85.12%\n",
      "XGBoost Accuracy: 84.61%\n",
      "XGBoost Accuracy: 84.68%\n",
      "XGBoost Accuracy: 85.33%\n",
      "XGBoost Accuracy: 91.21%\n",
      "XGBoost Accuracy: 94.75%\n",
      "XGBoost Accuracy: 94.67%\n",
      "XGBoost Accuracy: 94.27%\n",
      "XGBoost Accuracy: 94.79%\n",
      "XGBoost Accuracy: 93.17%\n",
      "XGBoost Accuracy: 92.45%\n",
      "XGBoost Accuracy: 92.30%\n",
      "XGBoost Accuracy: 92.83%\n",
      "XGBoost Accuracy: 99.08%\n",
      "XGBoost Accuracy: 99.63%\n",
      "XGBoost Accuracy: 99.31%\n",
      "XGBoost Accuracy: 99.26%\n",
      "XGBoost Accuracy: 99.43%\n",
      "XGBoost Accuracy: 99.35%\n",
      "XGBoost Accuracy: 99.46%\n",
      "XGBoost Accuracy: 99.62%\n",
      "XGBoost Accuracy: 99.49%\n",
      "XGBoost Accuracy: 98.40%\n",
      "XGBoost Accuracy: 81.52%\n",
      "XGBoost Accuracy: 80.55%\n",
      "XGBoost Accuracy: 81.42%\n",
      "XGBoost Accuracy: 81.55%\n",
      "XGBoost Accuracy: 81.68%\n",
      "XGBoost Accuracy: 80.74%\n",
      "XGBoost Accuracy: 81.28%\n",
      "XGBoost Accuracy: 81.54%\n",
      "XGBoost Accuracy: 81.20%\n",
      "XGBoost Accuracy: 81.38%\n",
      "XGBoost Accuracy: 81.49%\n",
      "XGBoost Accuracy: 80.98%\n",
      "XGBoost Accuracy: 80.59%\n",
      "XGBoost Accuracy: 81.10%\n",
      "XGBoost Accuracy: 81.44%\n",
      "XGBoost Accuracy: 82.53%\n",
      "XGBoost Accuracy: 81.65%\n",
      "XGBoost Accuracy: 89.01%\n",
      "XGBoost Accuracy: 92.49%\n",
      "XGBoost Accuracy: 91.86%\n",
      "XGBoost Accuracy: 91.81%\n",
      "XGBoost Accuracy: 92.37%\n",
      "XGBoost Accuracy: 92.41%\n",
      "XGBoost Accuracy: 92.72%\n",
      "XGBoost Accuracy: 92.49%\n",
      "XGBoost Accuracy: 93.07%\n",
      "XGBoost Accuracy: 92.60%\n",
      "XGBoost Accuracy: 92.76%\n",
      "XGBoost Accuracy: 92.46%\n",
      "XGBoost Accuracy: 93.35%\n",
      "XGBoost Accuracy: 93.05%\n",
      "XGBoost Accuracy: 93.56%\n",
      "XGBoost Accuracy: 93.37%\n",
      "XGBoost Accuracy: 94.61%\n",
      "XGBoost Accuracy: 94.04%\n",
      "XGBoost Accuracy: 94.02%\n",
      "XGBoost Accuracy: 93.98%\n",
      "XGBoost Accuracy: 93.23%\n",
      "XGBoost Accuracy: 93.28%\n",
      "XGBoost Accuracy: 93.45%\n",
      "XGBoost Accuracy: 93.12%\n",
      "XGBoost Accuracy: 92.46%\n",
      "XGBoost Accuracy: 93.03%\n",
      "XGBoost Accuracy: 91.65%\n",
      "XGBoost Accuracy: 92.77%\n",
      "XGBoost Accuracy: 93.28%\n",
      "XGBoost Accuracy: 93.06%\n",
      "XGBoost Accuracy: 93.20%\n",
      "XGBoost Accuracy: 92.43%\n",
      "XGBoost Accuracy: 93.16%\n",
      "XGBoost Accuracy: 92.10%\n",
      "XGBoost Accuracy: 91.81%\n",
      "XGBoost Accuracy: 92.66%\n",
      "XGBoost Accuracy: 92.28%\n",
      "XGBoost Accuracy: 91.70%\n",
      "XGBoost Accuracy: 91.50%\n",
      "XGBoost Accuracy: 91.98%\n",
      "XGBoost Accuracy: 93.27%\n",
      "XGBoost Accuracy: 92.38%\n",
      "XGBoost Accuracy: 92.69%\n",
      "XGBoost Accuracy: 92.23%\n",
      "reading data from /home/leonid/projects/feldera/demo/demo_notebooks/fraud_data/rolling_aggregates-simulation_output.csv\n",
      "XGBoost Accuracy: 95.13%\n",
      "XGBoost Accuracy: 95.13%\n",
      "XGBoost Accuracy: 95.92%\n",
      "XGBoost Accuracy: 96.01%\n",
      "XGBoost Accuracy: 95.74%\n",
      "XGBoost Accuracy: 95.54%\n",
      "XGBoost Accuracy: 95.44%\n",
      "XGBoost Accuracy: 95.31%\n",
      "XGBoost Accuracy: 96.42%\n",
      "XGBoost Accuracy: 95.63%\n",
      "XGBoost Accuracy: 95.74%\n",
      "XGBoost Accuracy: 95.14%\n",
      "XGBoost Accuracy: 99.40%\n",
      "XGBoost Accuracy: 99.36%\n",
      "XGBoost Accuracy: 99.26%\n",
      "XGBoost Accuracy: 99.23%\n",
      "XGBoost Accuracy: 99.02%\n",
      "XGBoost Accuracy: 99.35%\n",
      "XGBoost Accuracy: 99.04%\n",
      "XGBoost Accuracy: 99.30%\n",
      "XGBoost Accuracy: 99.19%\n",
      "XGBoost Accuracy: 99.11%\n",
      "XGBoost Accuracy: 99.21%\n",
      "XGBoost Accuracy: 99.25%\n",
      "XGBoost Accuracy: 98.99%\n",
      "XGBoost Accuracy: 98.91%\n",
      "XGBoost Accuracy: 98.97%\n",
      "XGBoost Accuracy: 98.74%\n",
      "XGBoost Accuracy: 98.56%\n",
      "XGBoost Accuracy: 99.08%\n",
      "XGBoost Accuracy: 98.80%\n",
      "XGBoost Accuracy: 98.80%\n",
      "XGBoost Accuracy: 98.68%\n",
      "XGBoost Accuracy: 98.61%\n",
      "XGBoost Accuracy: 98.97%\n",
      "XGBoost Accuracy: 98.94%\n",
      "XGBoost Accuracy: 98.96%\n",
      "XGBoost Accuracy: 99.06%\n",
      "XGBoost Accuracy: 98.99%\n",
      "XGBoost Accuracy: 99.07%\n",
      "XGBoost Accuracy: 98.73%\n",
      "XGBoost Accuracy: 98.79%\n",
      "XGBoost Accuracy: 98.85%\n",
      "XGBoost Accuracy: 98.73%\n",
      "XGBoost Accuracy: 98.67%\n",
      "XGBoost Accuracy: 98.99%\n",
      "XGBoost Accuracy: 98.96%\n",
      "XGBoost Accuracy: 98.75%\n",
      "XGBoost Accuracy: 98.75%\n",
      "XGBoost Accuracy: 98.96%\n",
      "XGBoost Accuracy: 98.79%\n",
      "XGBoost Accuracy: 98.94%\n",
      "XGBoost Accuracy: 99.06%\n",
      "XGBoost Accuracy: 99.05%\n",
      "XGBoost Accuracy: 99.15%\n",
      "XGBoost Accuracy: 99.04%\n",
      "XGBoost Accuracy: 99.13%\n",
      "XGBoost Accuracy: 99.30%\n",
      "XGBoost Accuracy: 99.09%\n",
      "XGBoost Accuracy: 99.00%\n",
      "XGBoost Accuracy: 99.06%\n",
      "XGBoost Accuracy: 99.19%\n",
      "XGBoost Accuracy: 99.30%\n",
      "XGBoost Accuracy: 99.37%\n",
      "XGBoost Accuracy: 99.10%\n",
      "XGBoost Accuracy: 99.08%\n",
      "XGBoost Accuracy: 99.16%\n",
      "XGBoost Accuracy: 99.45%\n",
      "XGBoost Accuracy: 99.22%\n",
      "XGBoost Accuracy: 99.18%\n",
      "XGBoost Accuracy: 99.39%\n",
      "XGBoost Accuracy: 99.22%\n",
      "XGBoost Accuracy: 99.26%\n",
      "XGBoost Accuracy: 99.12%\n",
      "XGBoost Accuracy: 98.41%\n",
      "XGBoost Accuracy: 98.26%\n",
      "XGBoost Accuracy: 98.22%\n",
      "XGBoost Accuracy: 98.31%\n",
      "XGBoost Accuracy: 98.17%\n",
      "XGBoost Accuracy: 97.90%\n",
      "XGBoost Accuracy: 97.44%\n",
      "XGBoost Accuracy: 97.78%\n",
      "XGBoost Accuracy: 98.12%\n",
      "XGBoost Accuracy: 98.21%\n",
      "XGBoost Accuracy: 97.82%\n",
      "XGBoost Accuracy: 97.51%\n",
      "XGBoost Accuracy: 98.19%\n",
      "XGBoost Accuracy: 97.67%\n",
      "XGBoost Accuracy: 97.52%\n",
      "XGBoost Accuracy: 97.78%\n",
      "XGBoost Accuracy: 97.95%\n",
      "XGBoost Accuracy: 98.26%\n",
      "XGBoost Accuracy: 97.61%\n",
      "XGBoost Accuracy: 97.51%\n",
      "XGBoost Accuracy: 98.36%\n",
      "XGBoost Accuracy: 97.97%\n",
      "XGBoost Accuracy: 98.30%\n",
      "XGBoost Accuracy: 98.04%\n"
     ]
    }
   ],
   "source": [
    "for (name, query) in queries:\n",
    "    try:\n",
    "        chunksize = 1024*10\n",
    "        print(f\"reading data from {simulation_outpath(name)}\")\n",
    "        simulation = pd.read_csv(simulation_outpath(name), iterator=True, chunksize=chunksize)\n",
    "        for simulation_batch in simulation:\n",
    "            nb_cols = len(simulation_batch.columns.tolist())        \n",
    "            X_simulation = simulation_batch.iloc[:, 0:nb_cols - 2].values\n",
    "            y_simulation = simulation_batch.iloc[:, nb_cols-2].values.astype(int)     \n",
    "\n",
    "            y_pred = models[name].predict(X_simulation)    \n",
    "            predictions = [round(value) for value in y_pred]\n",
    "            cm = confusion_matrix(y_simulation, predictions)\n",
    "            #show_data(cm, print_res = 1)    \n",
    "            accuracy = accuracy_score(y_simulation, predictions)\n",
    "            print(\"XGBoost Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"Error: cannot read from the specified source {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d10d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "with summary_writer.as_default():\n",
    "    tf.summary.scalar('accuracy', accuracy, step= step)\n",
    "    tf.summary.scalar('precision', pr, step= step)\n",
    "    tf.summary.scalar('recall', rec, step= step)\n",
    "    tf.summary.scalar('f1_score', f1, step= step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e0e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_summary_writer = tf.summary.create_file_writer(os.path.join(log_dir, 'train'))\n",
    "# test_summary_writer = tf.summary.create_file_writer(os.path.join(log_dir, 'test'))\n",
    "# simulation_summary_writer = tf.summary.create_file_writer(os.path.join(log_dir, 'simulation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55282b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-14ee10229596c29b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-14ee10229596c29b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/xgboost_fraud_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4233ee9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Printing AUC\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_curve, auc\n\u001b[0;32m----> 4\u001b[0m y_PredTest\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_proba(X_simulation)\n\u001b[1;32m      6\u001b[0m fp, tp,_ \u001b[38;5;241m=\u001b[39m roc_curve(y_simulation, y_pred)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(fp, tp, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AUC TEST =\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(auc(fp, tp)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Printing AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "y_PredTest= model.predict_proba(X_simulation)\n",
    "\n",
    "fp, tp,_ = roc_curve(y_simulation, y_pred)\n",
    "\n",
    "\n",
    "plt.plot(fp, tp, label=\" AUC TEST =\"+str(auc(fp, tp)))\n",
    "plt.plot([0,1],[0,1],'g--')\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC(ROC curve)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b42843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importance = model.feature_importances_\n",
    "\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48415314",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944da398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
